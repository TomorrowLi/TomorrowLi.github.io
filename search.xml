<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>豆瓣输入验证码登陆</title>
      <link href="/2018/03/16/douban/"/>
      <content type="html"><![CDATA[<h2 id="首先要登陆豆瓣要必须获取验证码"><a href="#首先要登陆豆瓣要必须获取验证码" class="headerlink" title="首先要登陆豆瓣要必须获取验证码"></a>首先要登陆豆瓣要必须获取验证码</h2><h3 id="1、获取验证码的图片链接下载到本地"><a href="#1、获取验证码的图片链接下载到本地" class="headerlink" title="1、获取验证码的图片链接下载到本地"></a>1、获取验证码的图片链接下载到本地</h3><pre><code>req=r&apos;&lt;img id=&quot;captcha_image&quot; src=&quot;(.*?)&quot; alt=&quot;captcha&quot; class=&quot;captcha_image&quot;/&gt;&apos;yanzhenma=re.findall(req,html.text,re.S)</code></pre><h3 id="2、手动输入验证码添加到data中"><a href="#2、手动输入验证码添加到data中" class="headerlink" title="2、手动输入验证码添加到data中"></a>2、手动输入验证码添加到data中</h3><pre><code>x=input(&apos;请输入验证码:&apos;)data[&apos;captcha-solution&apos;]=xdata[&apos;captcha-id&apos;]=captcha_id[0]</code></pre><h3 id="3、代码演示"><a href="#3、代码演示" class="headerlink" title="3、代码演示"></a>3、代码演示</h3><pre><code>#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2018/3/15 20:20# @Author  : tomorrowliimport requestsimport reimport osimport timeimport randomfrom hashlib import md5def geturl():    time.sleep(random.randint(2,5))    url=&apos;https://www.douban.com/accounts/login&apos;    headers={        &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;    }    data={        &apos;source&apos;:&apos;index_nav&apos;,        &apos;form_email&apos;:&apos;用户名&apos;,        &apos;form_password&apos;:&apos;密码&apos;,    }    html=requests.get(url,headers=headers)    #print(html.text)    req=r&apos;&lt;img id=&quot;captcha_image&quot; src=&quot;(.*?)&quot; alt=&quot;captcha&quot; class=&quot;captcha_image&quot;/&gt;&apos;    captcha = r&apos;&lt;input type=&quot;hidden&quot; name=&quot;captcha-id&quot; value=&quot;(.*?)&quot;/&gt;&apos;    yanzhenma=re.findall(req,html.text,re.S)    if yanzhenma:        captcha_id = re.findall(captcha,html.text,re.S)        html=requests.get(yanzhenma[0])        #可以采用MD5算法给图片随机取一个名字        #md5(html.content)        file_path = &apos;{0}/{1}.{2}&apos;.format(os.getcwd(), md5(html.content).hexdigest(), &apos;jpg&apos;)        if not os.path.exists(file_path):            with open(file_path, &apos;wb&apos;) as f:                f.write(html.content)                f.close()        x=input(&apos;请输入验证码:&apos;)        data[&apos;captcha-solution&apos;]=x        data[&apos;captcha-id&apos;]=captcha_id[0]        html = requests.post(url , data=data , headers=headers)        print(html.status_code)        print(&apos;登陆成功&apos;)    else:        html=requests.post(url,data=data,headers=headers)        print(&apos;登陆成功&apos;)def main():    geturl()if __name__ == &apos;__main__&apos;:    main()</code></pre><h3 id="4、之后可以从豆瓣上获取信息存到数据库中"><a href="#4、之后可以从豆瓣上获取信息存到数据库中" class="headerlink" title="4、之后可以从豆瓣上获取信息存到数据库中"></a>4、之后可以从豆瓣上获取信息存到数据库中</h3>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
          <category> 反爬方式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 反爬 </tag>
            
            <tag> 登陆 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>推荐五款常用的Google插件</title>
      <link href="/2018/03/14/googlechajian/"/>
      <content type="html"><![CDATA[<h2 id="首先你要学会科学上网"><a href="#首先你要学会科学上网" class="headerlink" title="首先你要学会科学上网"></a>首先你要学会科学上网</h2><h3 id="1、AdBlock"><a href="#1、AdBlock" class="headerlink" title="1、AdBlock"></a>1、AdBlock</h3><h4 id="最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐"><a href="#最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐" class="headerlink" title="最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐"></a>最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐</h4><h3 id="2、Infinity新标签页"><a href="#2、Infinity新标签页" class="headerlink" title="2、Infinity新标签页"></a>2、Infinity新标签页</h3><h4 id="Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。"><a href="#Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。" class="headerlink" title="Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。"></a>Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。</h4><h3 id="3、Imagus"><a href="#3、Imagus" class="headerlink" title="3、Imagus"></a>3、Imagus</h3><h4 id="鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5-视频-音频和内容专辑"><a href="#鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5-视频-音频和内容专辑" class="headerlink" title="鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5 视频/音频和内容专辑"></a>鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5 视频/音频和内容专辑</h4><h3 id="4、Tampermonkey（油猴）"><a href="#4、Tampermonkey（油猴）" class="headerlink" title="4、Tampermonkey（油猴）"></a>4、Tampermonkey（油猴）</h3><h4 id="Tampermonkey-是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让-Chrome-支持更多-UserScript-的-Chrome-插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的-Chrome-本身不支持的用户脚本功能它适用于基于-Blink-和-WebKit-的浏览器-像是-Chrome-Opera-Next-Safari-和-Firefox-。"><a href="#Tampermonkey-是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让-Chrome-支持更多-UserScript-的-Chrome-插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的-Chrome-本身不支持的用户脚本功能它适用于基于-Blink-和-WebKit-的浏览器-像是-Chrome-Opera-Next-Safari-和-Firefox-。" class="headerlink" title="Tampermonkey 是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让 Chrome 支持更多 UserScript 的 Chrome 插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的 Chrome 本身不支持的用户脚本功能它适用于基于 Blink 和 WebKit 的浏览器,像是 Chrome, Opera Next, Safari 和 Firefox 。"></a>Tampermonkey 是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让 Chrome 支持更多 UserScript 的 Chrome 插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的 Chrome 本身不支持的用户脚本功能它适用于基于 Blink 和 WebKit 的浏览器,像是 Chrome, Opera Next, Safari 和 Firefox 。</h4><h3 id="5、Chrono下载管理器"><a href="#5、Chrono下载管理器" class="headerlink" title="5、Chrono下载管理器"></a>5、Chrono下载管理器</h3><h4 id="Chrono下载管理器是Chrome浏览器下第一款-也是唯一一款-功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。"><a href="#Chrono下载管理器是Chrome浏览器下第一款-也是唯一一款-功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。" class="headerlink" title="Chrono下载管理器是Chrome浏览器下第一款(也是唯一一款)功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。"></a>Chrono下载管理器是Chrome浏览器下第一款(也是唯一一款)功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。</h4>]]></content>
      
      <categories>
          
          <category> google插件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google插件 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用Google插件看各大视频网的VIP视频</title>
      <link href="/2018/03/14/googlevip/"/>
      <content type="html"><![CDATA[<h3 id="首先你必须科学上网"><a href="#首先你必须科学上网" class="headerlink" title="首先你必须科学上网"></a>首先你必须科学上网</h3><p>   1、你可以用蓝灯</p><p>   2、可以看这个链接 <a href="http://mp.weixin.qq.com/s?__biz=MzIzMzE4NTk3OA==&amp;mid=2651257286&amp;idx=1&amp;sn=3a65472546a66e87486af29e6f75e668&amp;chksm=f37b2e13c40ca70569a8a1635ffc21da5a2872aafc8dda6ab64cb9cb7007202e29082a061b17&amp;scene=27#wechat_redirect" target="_blank" rel="external">点击</a> 出自这里</p>]]></content>
      
      <categories>
          
          <category> google插件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google插件 </tag>
            
            <tag> vip视频观看 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>通过知乎抓取粉丝</title>
      <link href="/2018/03/14/zhihu/"/>
      <content type="html"><![CDATA[<h3 id="首先以此-网站-为目标路径"><a href="#首先以此-网站-为目标路径" class="headerlink" title="首先以此 网站 为目标路径"></a>首先以此 <a href="https://zhuanlan.zhihu.com/qinlibo-ml/followers" target="_blank" rel="external">网站</a> 为目标路径</h3><h2 id="1、所需要的库"><a href="#1、所需要的库" class="headerlink" title="1、所需要的库"></a>1、所需要的库</h2><p>   requests</p><h2 id="2、对以瀑布流的json数据进行解析"><a href="#2、对以瀑布流的json数据进行解析" class="headerlink" title="2、对以瀑布流的json数据进行解析"></a>2、对以瀑布流的json数据进行解析</h2><pre><code>data={        &apos;limit&apos;:20,        &apos;offset&apos;:20    }</code></pre><h2 id="3、代码演示"><a href="#3、代码演示" class="headerlink" title="3、代码演示"></a>3、代码演示</h2><pre><code>#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2018/3/13 15:40# @Author  : tomorrowliimport requestsclass Zhuhu():#初始化参数def __init__(self):    #赋值网址    self.url = &apos;https://zhuanlan.zhihu.com/api/columns/wajuejiprince/followers&apos;    self.headers={        &apos;user-agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;    }def getMesg(self,param):    self.html=requests.get(self.url,params=param,headers=self.headers)    for m in range(20):        &apos;&apos;&apos;        with open(&apos;zhihu.txt&apos;,&apos;a&apos;) as f:            print(self.html.json()[m][&apos;hash&apos;])            f.write(self.html.json()[m][&apos;hash&apos;]+&apos;\n&apos;)        &apos;&apos;&apos;        data={            &apos;bio&apos;:self.html.json()[m][&apos;bio&apos;],            &apos;name&apos;:self.html.json()[m][&apos;name&apos;],            &apos;profileUrl&apos;:self.html.json()[m][&apos;profileUrl&apos;],            &apos;hash&apos;:self.html.json()[m][&apos;hash&apos;]        }        print(data)def main():    for n in range(0,60,20):        data={            &apos;limit&apos;:20,            &apos;offset&apos;:n        }        zhihu=Zhuhu()        zhihu.getMesg(data)if __name__ == &apos;__main__&apos;:    main()</code></pre><h2 id="4、可以为每个粉丝进行发私信"><a href="#4、可以为每个粉丝进行发私信" class="headerlink" title="4、可以为每个粉丝进行发私信"></a>4、可以为每个粉丝进行发私信</h2><p>  需要获取每个粉丝的hash和自己cookie值</p>]]></content>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用selenium自动爬取天眼查上的公司信息</title>
      <link href="/2018/03/13/tianyancha/"/>
      <content type="html"><![CDATA[<h2 id="Quict-Start"><a href="#Quict-Start" class="headerlink" title="Quict Start"></a>Quict Start</h2><h3 id="1、需要的库"><a href="#1、需要的库" class="headerlink" title="1、需要的库"></a>1、需要的库</h3><p>re、selenium、pymysql</p><h3 id="2、程序前的准备"><a href="#2、程序前的准备" class="headerlink" title="2、程序前的准备"></a>2、程序前的准备</h3><p>首先在自己的python路径下放入 <a href="http://npm.taobao.org/mirrors/chromedriver/" target="_blank" rel="external">chromedriver.exe</a> 选择自己浏览器合适的版本下载</p><h3 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h3><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport reimport MySQLdbhost=&apos;localhost&apos;#主机名user=&apos;root&apos;#数据库的用户名passwd=&apos;&apos;#数据库的密码db=&apos;test&apos;#数据库的名字conn=MySQLdb.connect(host,user,passwd,db,charset=&apos;utf8&apos;)cursor = conn.cursor()boser=webdriver.Chrome()boser.maximize_window()wait=WebDriverWait(boser , 10)def getlogin():    boser.get(&apos;https://www.tianyancha.com/login&apos;)    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb30.position-rel &gt; input&quot;))    )    password=wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &apos;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb40.position-rel &gt; input&apos;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR , &apos;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.c-white.b-c9.pt8.f18.text-center.login_btn&apos;))    )    user.send_keys(&apos;用户名&apos;)    password.send_keys(&apos;密码&apos;)    submit.click()    getSearch()def getSearch():    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR , &quot;#home-main-search&quot;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR ,                                    &apos;#web-content &gt; div &gt; div.mainV3_tab1_enter.position-rel &gt; div.mainv2_tab1.position-rel &gt; div &gt; div.main-tab-outer &gt; div:nth-child(2) &gt; div &gt; div:nth-child(1) &gt; div.input-group.inputV2 &gt; div &gt; span&apos;))    )    key=&apos;佛山市顺德区&apos;    user.send_keys(key)    submit.click()    html=boser.page_source    geturl(html)    getUrlNext()def getUrlNext():    for i in range(2,5):        boser.get(r&apos;https://www.tianyancha.com/search/p%s?key=佛山市顺德区&apos;% i)        html=boser.page_source        geturl(html)def geturl(html):    try:        reg=r&apos;&lt;a href=&quot;.*?&quot; target=&quot;_blank&quot; tyc-event-click=&quot;&quot; tyc-event-ch=&quot;CompanySearch.Company&quot; style=&quot;word-break: break-all;font-weight: 600;&quot; class=&quot;query_name sv-search-company f18 in-block vertical-middle&quot;&gt;&lt;span&gt;(.*?)&lt;/span&gt;&lt;/a&gt;&apos;        names=re.findall(reg,html,re.S)        reg=r&apos;&lt;a title=&quot;.*?&quot; class=&quot;legalPersonName hover_underline&quot; target=&quot;_blank&quot; href=&quot;.*?&quot;&gt;(.*?)&lt;/a&gt;&apos;        daibiao=re.findall(reg,html,re.S)        reg=r&apos;&lt;div class=&quot;title overflow-width&quot; style=&quot;border-right: none&quot;&gt;.*?&lt;span title=&quot;.*?&quot;&gt;(.*?)&lt;/span&gt;&lt;/div&gt;&apos;        days = re.findall(reg,html,re.S)        reg=r&apos;&lt;span class=&quot;sec-c3&quot;&gt;联系电话：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&apos;        phones = re.findall(reg , html , re.S)        reg=r&apos;&lt;div class=&quot;add&quot;&gt;&lt;span class=&quot;sec-c3&quot;&gt;.*?&lt;/span&gt;&lt;span&gt;：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&apos;        location=re.findall(reg,html,re.S)        for i in range(20):            data={                &apos;name&apos;:names[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;daibiao&apos;:daibiao[i],                &apos;day&apos;:days[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;phone&apos;:phones[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;location&apos;: location[i].replace(&apos;&lt;em&gt;&apos;, &apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;)            }            print(data)            cursor.execute(&quot;insert into tianyancha(name,daibiao,day,phone,location) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(data[&apos;name&apos;],data[&apos;daibiao&apos;],data[&apos;day&apos;],data[&apos;phone&apos;],data[&apos;location&apos;]))            print(data[&apos;daibiao&apos;],&apos;成功存入数据库&apos;)            conn.commit()    except:        return Nonedef main():    getlogin()if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 信息 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬取网站的种子链接并下载视频到本地</title>
      <link href="/2018/02/28/zhongzi/"/>
      <content type="html"><![CDATA[<p>首先读取网站的链接存到txt文件中</p><pre><code>import reimport urllib.requestfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.support.ui import WebDriverWaitfrom multiprocessing import Poolbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def search(url,next_title):    try:        browser.get(url)        html = browser.page_source        reg = r&apos;&lt;span style=&quot;&quot; id=&quot;streamurj&quot;&gt;(.*?)&lt;/span&gt;&apos;        wang = re.findall(reg, html)        for i in wang:            zhongzi=&apos;https://openload.co/stream/&apos; +i            print(zhongzi)            with open(&apos;lianjie.txt&apos;, &apos;a+&apos;) as f:                f.write(str(zhongzi) + &apos;\n&apos;)                f.close()            #load(product[&apos;url&apos;],product[&apos;title&apos;])    except TimeoutException:        return search(url,next_title)def load(url,title):    print(&apos;正在下载&apos;,url)    urllib.request.urlretrieve(url,&apos;mp4/%s.mp4&apos;% title)    print(&apos;下载成功&apos;,title)def index_page(url):    try:        browser.get(url)        html = browser.page_source        reg = r&apos;&lt;td class=&quot;mytd-padding&quot;&gt;&lt;a href=&quot;(.*?)&quot;&gt;&apos;        video = re.findall(reg, html)        for age in video:            next_url = &apos;http://ourjav.com/&apos; + age.split(&apos;[&apos;)[0]            next_title=age.split(&apos;[&apos;)[-1]            next_page(next_url,next_title)    except TimeoutException:        index_page(url)def next_page(url,next_title):    try:        browser.get(url)        html = browser.page_source        age=r&apos;&lt;iframe id=&quot;flash_game_object&quot; name=&quot;flash_game_object&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot; width=&quot;650&quot; height=&quot;500&quot; src=&quot;(.*?)&quot; allowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;&apos;        video=re.findall(age,html,re.S)        for image in video:            search(image,next_title)    except TimeoutException:        next_page(url, next_title)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        respon=browser.get(line3)        count=count+1        if count==5:            break    print(count)def main(offset):    url = &apos;http://ourjav.com/&apos;+&apos;index.php?&amp;page=%s&apos; % offset    index_page(url)if __name__ == &apos;__main__&apos;:    group = [x for x in range(1, 11)]    pool = Pool()    pool.map(main, group)</code></pre><p>其次从txt文件中读取url进行下载</p><pre><code>from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        browser.get(line3)        count=count+1        if count==5:            break    print(count)def main():    read_load()if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 种子 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用正则表达式来爬取某论坛的邮箱地址</title>
      <link href="/2018/02/25/youxian/"/>
      <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport requestsdef fand_email(url,counts):    data=requests.get(url)    content=data.text    pattern = r&apos;[0-9a-zA-Z._]+@[0-9a-zA-Z._]+\.[0-9a-zA-Z._]+&apos;    p = re.compile(pattern)    m = p.findall(content)    with open(&apos;emal.txt&apos;,&apos;a+&apos;) as f:        for i in m:            f.write(i+&apos;\n&apos;)            print(i)            counts= counts+1    return countsdef main():    counts=0    numbers=0    for i in range(1,32):        url=&apos;http://tieba.baidu.com/p/2314539885?pn=%s&apos;% i        number=fand_email(url,counts)        numbers=numbers+number    print(numbers)if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 邮箱地址爬取 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬取淘宝美食信息并存储到数据库中</title>
      <link href="/2018/02/21/selenium-taobao/"/>
      <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport MySQLdbfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom pyquery import PyQuery as pqbroser=webdriver.Chrome()wait=WebDriverWait(broser, 10)def search(table):    try:        broser.get(&apos;https://www.taobao.com&apos;)        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#q&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;#J_TSearchForm &gt; div.search-button &gt; button&apos;))        )        inputs.send_keys(&apos;美食&apos;)        submit.click()        total = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR,&apos;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total&apos;))        )        get_product(table)        return total.text    except TimeoutException:        return search(table)def next_page(page_number,table):    try:        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit&quot;))        )        inputs.clear()        inputs.send_keys(page_number)        submit.click()        wait.until(            EC.text_to_be_present_in_element((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span&quot;),str(page_number))        )        get_product(table)    except TimeoutException:        next_page(page_number,table)def get_product(table):    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,&apos;#mainsrp-itemlist .items .item&apos;)))    html=broser.page_source    doc=pq(html)    items=doc(&apos;#mainsrp-itemlist .items .item&apos;).items()    for item in items:        product={            &apos;image&apos;:item.find(&apos;.pic .img&apos;).attr(&apos;src&apos;),            &apos;price&apos;:item.find(&apos;.price&apos;).text().replace(&apos;\n&apos;,&apos;&apos;),            &apos;deal&apos;:item.find(&apos;.deal-cnt&apos;).text()[:-3],            &apos;title&apos;:item.find(&apos;.title&apos;).text().replace(&apos;\n&apos;,&apos;&apos;),            &apos;shop&apos;:item.find(&apos;.shop&apos;).text(),            &apos;location&apos;:item.find(&apos;.location&apos;).text()        }        print(product)        inserttable(table, product[&apos;image&apos;], product[&apos;price&apos;], product[&apos;deal&apos;], product[&apos;title&apos;],product[&apos;shop&apos;],product[&apos;location&apos;])#连接数据库 mysqldef connectDB():        host=&quot;localhost&quot;        dbName=&quot;test&quot;        user=&quot;root&quot;        password=&quot;&quot;        #此处添加charset=&apos;utf8&apos;是为了在数据库中显示中文，此编码必须与数据库的编码一致        db=MySQLdb.connect(host,user,password,dbName,charset=&apos;utf8&apos;)        return db        cursorDB=db.cursor()        return cursorDB#创建表，SQL语言。CREATE TABLE IF NOT EXISTS 表示：表createTableName不存在时就创建def creatTable(createTableName):    createTableSql=&quot;CREATE TABLE IF NOT EXISTS &quot;+ createTableName+&quot;(image VARCHAR(255),price VARCHAR(255),deal  VARCHAR(255),title VARCHAR(255),shop VARCHAR(255),location VARCHAR(255))&quot;    DB_create=connectDB()    print(&apos;链接数据库成功&apos;)    cursor_create=DB_create.cursor()    cursor_create.execute(createTableSql)    DB_create.close()    print(&apos;creat table &apos;+createTableName+&apos; successfully&apos;)    return createTableName#数据插入表中def inserttable(insertTable,insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction):    insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(image,price,deal,title,shop,location)VALUES(%s,%s,%s,%s,%s,%s)&quot;#         insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(time,title,text,clicks)VALUES(&quot;+insertTime+&quot; , &quot;+insertTitle+&quot; , &quot;+insertText+&quot; , &quot;+insertClicks+&quot;)&quot;    DB_insert=connectDB()    cursor_insert=DB_insert.cursor()    cursor_insert.execute(insertContentSql,(insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction))    DB_insert.commit()    DB_insert.close()    print (&apos;inert contents to  &apos;+insertTable+&apos; successfully&apos;)def main():    table = creatTable(&apos;yh1&apos;)    print(&apos;创建表成功&apos;)    total=search(table)    total=int(re.compile(&apos;(\d+)&apos;).search(total).group(1))    for i in range(2,total+1):        next_page(i,table)if __name__==&apos;__main__&apos;:    main()</code></pre>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美食信息 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬取美女图片并按照文件进行存储</title>
      <link href="/2018/02/17/file-tupian/"/>
      <content type="html"><![CDATA[<p>欢迎来到<a href="https://www.python.org/" target="_blank" rel="external">python</a>!学习。 </p><p>代码演示：</p><pre><code>import reimport requestsfrom bs4 import BeautifulSoupdef fand_email(url,counts):    data=requests.get(url)    content=data.text    pattern = r&apos;[0-9a-zA-Z._]+@[0-9a-zA-Z._]+\.[0-9a-zA-Z._]+&apos;    p = re.compile(pattern)    m = p.findall(content)    with open(&apos;emal.txt&apos;,&apos;a+&apos;) as f:        for i in m:            f.write(i+&apos;\n&apos;)            print(i)            counts= counts+1return countsdef main():    counts=0    numbers=0    for i in range(1,32):        url=&apos;http://tieba.baidu.com/p/2314539885?pn=%s&apos;% i        number=fand_email(url,counts)        numbers=numbers+number    print(numbers)if __name__ == &apos;__main__&apos;:    main()</code></pre><p>文件 text.py</p><pre><code>import requestsfrom bs4 import BeautifulSoupdef fand_load_image(url):    wb_date = requests.get(url)    #wb_date.encoding = &apos;gbk&apos;    soup = BeautifulSoup(wb_date.text, &apos;lxml&apos;)    print(soup)    images = soup.select(&apos;div.image-item-inner &gt; a&apos;)    print(images)    #image=images[0].get(&apos;href&apos;)    #print(image)url=&apos;https://www.toutiao.com/a6520385683419300359/&apos;fand_load_image(url)</code></pre>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美女图片 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>今日头条上的美女图片进行爬取</title>
      <link href="/2018/02/15/python-student/"/>
      <content type="html"><![CDATA[<p>欢迎来到<a href="https://www.python.org/" target="_blank" rel="external">python</a>!学习。 这是我人生旅途的第一课，非常重要。 我从这个案例中学到的许多关于python爬虫的知识，这是我python爬虫的启蒙。</p><p>#所需要的库<br>requests：用于网页请求</p><p>BeautifulSoup：选择所要的元素</p><p>json：用来解析json数据的库</p><p>re：用于正则表达式的筛选</p><p>代码演示：</p><pre><code>import jsonimport refrom hashlib import md5import osimport requestsfrom urllib.parse import urlencodefrom bs4 import BeautifulSoupfrom config import *from multiprocessing import Pool#对要爬取页面的解析def get_page_index(offset, keyword):    data = {        &apos;offset&apos;: offset,        &apos;format&apos;: &apos;json&apos;,        &apos;keyword&apos;: keyword,        &apos;autoload&apos;: &apos;true&apos;,        &apos;count&apos;: &apos;20&apos;,        &apos;cur_tab&apos;: 3,        &apos;from&apos;:&apos;gallery&apos;    }    url = &apos;https://www.toutiao.com/search_content/?&apos; + urlencode(data)    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&apos;请求索引失败&apos;)        return None#获取所要的数据在json中def prase_get_index(html):    data = json.loads(html)    if data and &apos;data&apos; in data.keys():        for item in data.get(&apos;data&apos;):            yield item.get(&apos;article_url&apos;)#用正则对json数据进行解析def prase_page_urlli(html):    soup = BeautifulSoup(html, &apos;lxml&apos;)    title = soup.select(&apos;title&apos;)[0].get_text()    print(title)    images_pattern= re.compile(&apos;gallery: JSON.parse\((.*?)\)&apos;,re.S)    result=re.search(images_pattern,html)    if result:        data=json.loads(result.group(1))        data=eval(data)        if data and &apos;sub_images&apos;in data.keys():            sub_images=data.get(&apos;sub_images&apos;)            image=[item.get(&apos;url&apos;).replace(&apos;\\&apos;,&apos;&apos;) for item in sub_images]            for images_page in image:                dowload_image(images_page)            return{                &apos;title&apos;:title,                &apos;image&apos;:image            }#对所要下载的图片链接进行访问def get_page_urlli(url):    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&apos;请求失败&apos;,url)        return None#下载图片def dowload_image(url):    print(&quot;正在下载&quot;,url)    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            save_image(response.content)        return None    except:        print(&apos;请求图片出错&apos;,url)        return None#把图片保存到本地路径下def save_image(content):    file_path=&apos;{0}/{1}.{2}&apos;.format(os.getcwd(),md5(content).hexdigest(),&apos;jpg&apos;)    if not os.path.exists(file_path):        with open(file_path,&apos;wb&apos;) as f:            f.write(content)            f.close()def main(offset):    html = get_page_index(offset, keyword)    for url in prase_get_index(html):        html=get_page_urlli(url)        if html:            result=prase_page_urlli(html)            print(result)if __name__ == &apos;__main__&apos;:    group=[x*20 for x in range(stat,end+1)]    pool=Pool()    pool.map(main,group)</code></pre><p>详细的解释。</p>]]></content>
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美女图片 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2017/11/09/hello-world/"/>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
      
      
    </entry>
    
  
  
</search>
