<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>利用线程池解析大文件</title>
      <link href="/2022/05/31/%E5%88%A9%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%96%87%E4%BB%B6/"/>
      <url>/2022/05/31/%E5%88%A9%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<ol><li><p><strong>ReadFileThread</strong></p><pre><code>  public class ReadFileThread extends Thread &#123; private ReaderFileListener processPoiDataListeners; private String filePath; private long start; private long end; public ReadFileThread(ReaderFileListener processPoiDataListeners,long start,long end,String file) &#123;     this.setName(this.getName()+&quot;-ReadFileThread&quot;);     System.out.println(this.getName());     this.start = start;     this.end = end;     this.filePath = file;     this.processPoiDataListeners = processPoiDataListeners; &#125; @Override public void run() &#123;     ReadFile readFile = new ReadFile();     readFile.setReaderListener(processPoiDataListeners);     readFile.setEncode(processPoiDataListeners.getEncode()); //  readFile.addObserver();     try &#123;         readFile.readFileByLine(filePath, start, end + 1);     &#125; catch (Exception e) &#123;         e.printStackTrace();     &#125; &#125;</code></pre><p>}</p></li></ol><span id="more"></span><ol><li><p><strong>ReadFile</strong></p><pre><code>   import java.io.*; import java.nio.ByteBuffer; import java.nio.channels.FileChannel; import java.util.Observable; /**  * @author: LiMing  * @since: 2022/5/27 17:43  **/ public class ReadFile extends Observable &#123; private int bufSize = 1024; // 换行符 private byte key = &quot;\n&quot;.getBytes()[0]; // 当前行数 private long lineNum = 0; // 文件编码,默认为gb2312 private String encode = &quot;gb2312&quot;; // 具体业务逻辑监听器 private ReaderFileListener readerListener; public void setEncode(String encode) &#123;     this.encode = encode; &#125; public void setReaderListener(ReaderFileListener readerListener) &#123;     this.readerListener = readerListener; &#125; /**  * 获取准确开始位置  * @param file  * @param position  * @return  * @throws Exception  */ public long getStartNum(File file, long position) throws Exception &#123;     long startNum = position;     FileChannel fcin = new RandomAccessFile(file, &quot;r&quot;).getChannel();     fcin.position(position);     try &#123;         int cache = 1024;         ByteBuffer rBuffer = ByteBuffer.allocate(cache);         // 每次读取的内容         byte[] bs = new byte[cache];         // 缓存         byte[] tempBs = new byte[0];         String line = &quot;&quot;;         while (fcin.read(rBuffer) != -1) &#123;             int rSize = rBuffer.position();             rBuffer.rewind();             rBuffer.get(bs);             rBuffer.clear();             byte[] newStrByte = bs;             // 如果发现有上次未读完的缓存,则将它加到当前读取的内容前面             if (null != tempBs) &#123;                 int tL = tempBs.length;                 newStrByte = new byte[rSize + tL];                 System.arraycopy(tempBs, 0, newStrByte, 0, tL);                 System.arraycopy(bs, 0, newStrByte, tL, rSize);             &#125;             // 获取开始位置之后的第一个换行符             int endIndex = indexOf(newStrByte, 0);             if (endIndex != -1) &#123;                 return startNum + endIndex;             &#125;             tempBs = substring(newStrByte, 0, newStrByte.length);             startNum += 1024;         &#125;     &#125; catch (Exception e) &#123;         e.printStackTrace();     &#125; finally &#123;         fcin.close();     &#125;     return position; &#125; /**  * 从设置的开始位置读取文件，一直到结束为止。如果 end设置为负数,刚读取到文件末尾  * @param fullPath  * @param start  * @param end  * @throws Exception  */ public void readFileByLine(String fullPath, long start, long end) throws Exception &#123;     File fin = new File(fullPath);     if (fin.exists()) &#123;         FileChannel fcin = new RandomAccessFile(fin, &quot;r&quot;).getChannel();         fcin.position(start);         try &#123;             ByteBuffer rBuffer = ByteBuffer.allocate(bufSize);             // 每次读取的内容             byte[] bs = new byte[bufSize];             // 缓存             byte[] tempBs = new byte[0];             String line = &quot;&quot;;             // 当前读取文件位置             long nowCur = start;             while (fcin.read(rBuffer) != -1) &#123;                 nowCur += bufSize;                 int rSize = rBuffer.position();                 rBuffer.rewind();                 rBuffer.get(bs);                 rBuffer.clear();                 byte[] newStrByte = bs;                 // 如果发现有上次未读完的缓存,则将它加到当前读取的内容前面                 if (null != tempBs) &#123;                     int tL = tempBs.length;                     newStrByte = new byte[rSize + tL];                     System.arraycopy(tempBs, 0, newStrByte, 0, tL);                     System.arraycopy(bs, 0, newStrByte, tL, rSize);                 &#125;                 // 是否已经读到最后一位                 boolean isEnd = false;                 // 如果当前读取的位数已经比设置的结束位置大的时候，将读取的内容截取到设置的结束位置                 if (end &gt; 0 &amp;&amp; nowCur &gt; end) &#123;                     // 缓存长度 - 当前已经读取位数 - 最后位数                     int l = newStrByte.length - (int) (nowCur - end);                     newStrByte = substring(newStrByte, 0, l);                     isEnd = true;                 &#125;                 int fromIndex = 0;                 int endIndex = 0;                 // 每次读一行内容，以 key（默认为\n） 作为结束符                 while ((endIndex = indexOf(newStrByte, fromIndex)) != -1) &#123;                     byte[] bLine = substring(newStrByte, fromIndex, endIndex);                     line = new String(bLine, 0, bLine.length, encode);                     lineNum++;                     // 输出一行内容，处理方式由调用方提供                     readerListener.outLine(line.trim(), lineNum, false);                     fromIndex = endIndex + 1;                 &#125;                 // 将未读取完成的内容放到缓存中                 tempBs = substring(newStrByte, fromIndex, newStrByte.length);                 if (isEnd) &#123;                     break;                 &#125;             &#125;             // 将剩下的最后内容作为一行，输出，并指明这是最后一行             String lineStr = new String(tempBs, 0, tempBs.length, encode);             readerListener.outLine(lineStr.trim(), lineNum, true);         &#125; catch (Exception e) &#123;             e.printStackTrace();         &#125; finally &#123;             fcin.close();         &#125;     &#125; else &#123;         throw new FileNotFoundException(&quot;没有找到文件：&quot; + fullPath);     &#125;     // 通知观察者,当前工作已经完成     setChanged();     notifyObservers(start+&quot;-&quot;+end); &#125; /**  * 查找一个byte[]从指定位置之后的一个换行符位置  *  * @param src  * @param fromIndex  * @return  * @throws Exception  */ private int indexOf(byte[] src, int fromIndex) throws Exception &#123;     for (int i = fromIndex; i &lt; src.length; i++) &#123;         if (src[i] == key) &#123;             return i;         &#125;     &#125;     return -1; &#125; /**  * 从指定开始位置读取一个byte[]直到指定结束位置为止生成一个全新的byte[]  *  * @param src  * @param fromIndex  * @param endIndex  * @return  * @throws Exception  */ private byte[] substring(byte[] src, int fromIndex, int endIndex) throws Exception &#123;     int size = endIndex - fromIndex;     byte[] ret = new byte[size];     System.arraycopy(src, fromIndex, ret, 0, size);     return ret; &#125; &#125;</code></pre></li></ol><ol><li><p><strong>ReaderFileListener:</strong></p><pre><code> // 一次读取行数，默认为500 private int readColNum = 500; private String encode; private List&lt;String&gt; list = new ArrayList&lt;String&gt;(); /**  * 设置一次读取行数  * @param readColNum  */ protected void setReadColNum(int readColNum) &#123;     this.readColNum = readColNum; &#125; public String getEncode() &#123;     return encode; &#125; public void setEncode(String encode) &#123;     this.encode = encode; &#125; /**  * 每读取到一行数据，添加到缓存中  * @param lineStr 读取到的数据  * @param lineNum 行号  * @param over 是否读取完成  * @throws Exception  */ public void outLine(String lineStr, long lineNum, boolean over) throws Exception &#123;     if(null != lineStr)         list.add(lineStr);     if (!over &amp;&amp; (lineNum % readColNum == 0)) &#123;         output(list);         list.clear();     &#125; else if (over) &#123;         output(list);         list.clear();     &#125; &#125; /**  * 批量输出  *  * @param stringList  * @throws Exception  */ public abstract void output(List&lt;String&gt; stringList) throws Exception;</code></pre></li><li><p><strong>ProcessDataByPostgisListeners</strong></p><pre><code> import java.util.List; /**  * @author: LiMing  * @since: 2022/5/30 17:51  **/ public class ProcessDataByPostgisListeners extends ReaderFileListener &#123;     @Override     public void output(List&lt;String&gt; stringList) throws Exception &#123;         for (String s : stringList) &#123;             System.out.println(s);         &#125;     &#125; &#125;</code></pre></li><li><p><strong>BuildData</strong></p><pre><code> import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.util.concurrent.SynchronousQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class BuildData &#123; public static void main(String[] args) throws Exception &#123; File file = new File(&quot;D:\\Users\\TomorrowLi\\Desktop\\log文件\\92020701_out.log.2022-05-23.16.40.15&quot;); FileInputStream fis = null; try &#123;     ReadFile readFile = new ReadFile();     fis = new FileInputStream(file);     int available = fis.available();     int maxThreadNum = 5;     //核心线程     int corePoolSize=5;     //最大线程数     int maxnumPoolSize=10;     //等待时间     long keepAliveTime=1;     // 线程粗略开始位置     int i = available / maxThreadNum;     ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(corePoolSize,maxnumPoolSize,keepAliveTime, TimeUnit.MINUTES,new SynchronousQueue&lt;&gt;());     for (int j = 0; j &lt; maxThreadNum; j++) &#123;         // 计算精确开始位置         long startNum = j == 0 ? 0 : readFile.getStartNum(file, i * j);         long endNum = j + 1 &lt; maxThreadNum ? readFile.getStartNum(file, i * (j + 1)) : -2;         // 具体监听实现         ReaderFileListener listeners = new ProcessDataByPostgisListeners();         listeners.setEncode(&quot;UTF-8&quot;);         threadPoolExecutor.submit(new ReadFileThread(listeners, startNum, endNum, file.getPath()));     &#125;     threadPoolExecutor.shutdown(); &#125; catch (IOException e) &#123;     e.printStackTrace(); &#125; catch (Exception e) &#123;     e.printStackTrace(); &#125; &#125;  &#125;</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 线程池 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线程池 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HashMap 与 ConcurrentHashMap 的实现原理是怎样的？ConcurrentHashMap 是如何保证线程安全的？</title>
      <link href="/2022/05/11/%E9%9D%A2%E8%AF%95%E9%A2%98-HashMap/"/>
      <url>/2022/05/11/%E9%9D%A2%E8%AF%95%E9%A2%98-HashMap/</url>
      
        <content type="html"><![CDATA[<p><a href="https://osjobs.net/topk/all/" title="Java面试题">TopK面试题</a></p><h2 id="1、HashMap的底层数据结构？"><a href="#1、HashMap的底层数据结构？" class="headerlink" title="1、HashMap的底层数据结构？"></a>1、HashMap的底层数据结构？</h2><pre><code>HashMap底层数据结构 JDK1.7：数组+链表 JDK1.8：数组+链表+红黑树。每个数组都会有Key-value的值。java7是Entry，java8是Node。当链表长度大于8并且数组的长度大于64会转换为红黑树。</code></pre><span id="more"></span><h2 id="2、HashMap的存取原理？"><a href="#2、HashMap的存取原理？" class="headerlink" title="2、HashMap的存取原理？"></a>2、HashMap的存取原理？</h2><ul><li><p>put</p><p>  1）通过hash(Object key)算法得到hash值；</p><p>  2）判断table是否为null或者长度为0，如果是执行resize()进行扩容；</p><p>  3）通过hash值以及table数组长度得到插入的数组索引i，判断数组table[i]是否为空或为null；</p><p>  4）如果table[i] == null，直接新建节点添加，转向 8），如果table[i]不为空，转向 5）；</p><p>  5）判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，这里的相同指的是hashCode以及equals，否则转向 6）；</p><p>  6）判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转7）；</p><p>  7）遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</p><p>  8）插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</p></li><li><p>get</p><p>  我们先简单说一下get(Object key)流程，通过传入的key通过hash()算法得到hash值，在通过(n - 1) &amp; hash找到数组下标，如果数组下标所对应的node值正好key一样就返回，否则找到node.next找到下一个节点，看是否是treenNode，如果是，遍历红黑树找到对应node，如果不是遍历链表找到node</p></li></ul><h2 id="3、Java7和Java8的区别？"><a href="#3、Java7和Java8的区别？" class="headerlink" title="3、Java7和Java8的区别？"></a>3、Java7和Java8的区别？</h2><pre><code>共同点1.容量(capacity):容量为底层数组的长度,JDK7中为Entry数组,JDK8中为Node数组 a. 容量一定为2的次幂2.加载因子(Load factor)：HashMap在其容量自动增加前可达到多满的一种尺度 a. 默认加载因子 = 0.753.扩容机制：扩容时resize(2 * table.length)，扩容到原数组长度的2倍。4.key为null:若key == null，则hash(key) = 0，则将该键-值 存放到数组table 中的第1个位置，即table [0]不同点1.发生hash冲突时 JDK7:发生hash冲突时，新元素插入到链表头中，即新元素总是添加到数组中，就元素移动到链表中。 JDK8：发生hash冲突后，会优先判断该节点的数据结构式是红黑树还是链表，如果是红黑树，则在红黑树中插入数据；如果是链表，则将数据插入到链表的尾部并判断链表长度是否大于8，如果大于8要转成红黑树。2.扩容时 JDK7:在扩容resize（）过程中，采用单链表的头插入方式，在将旧数组上的数据 转移到 新数组上时，转移操作 = 按旧链表的正序遍历链表、在新链表的头部依次插入，即在转移数据、扩容后，容易出现链表逆序的情况 。 多线程下resize()容易出现死循环。此时若（多线程）并发执行 put（）操作，一旦出现扩容情况，则 容易出现 环形链表，从而在获取数据、遍历链表时 形成死循环（Infinite Loop），即 死锁的状态 。</code></pre><h2 id="4、为啥会线程不安全？"><a href="#4、为啥会线程不安全？" class="headerlink" title="4、为啥会线程不安全？"></a>4、为啥会线程不安全？</h2><pre><code>在jdk1.8中对HashMap进行了优化，在发生hash碰撞，不再采用头插法方式，而是直接插入链表尾部，因此不会出现环形链表的情况，但是在多线程的情况下仍然不安全，这里我们看jdk1.8中HashMap的put操作源码：</code></pre><h2 id="5、默认初始化大小是多少？为啥是这么多？为啥大小都是2的幂？"><a href="#5、默认初始化大小是多少？为啥是这么多？为啥大小都是2的幂？" class="headerlink" title="5、默认初始化大小是多少？为啥是这么多？为啥大小都是2的幂？"></a>5、默认初始化大小是多少？为啥是这么多？为啥大小都是2的幂？</h2><pre><code>原因有两点：1.加快哈希运算 2.减少哈希冲突1.加快哈希运算我们都知道比如向hashMap中存入一个值,通常做法是对这个值求hashCode()得到一个数hash,然后在用hash对集合长度求余数,也就是 hash%length=positon得到的结果就是存放的位置。但是求余%的运算效率比较低。有没有更快的运算呢？答案是使用&amp;运算。但是使用&amp;运算怎么样才能和使用%效果一样呢？那就是，当HashMap的长度为2的幂的时候一下公式就成立了:hash%length==hash&amp;(length-1)。所以就可以使用&amp;运算来求位置下标了。2.减少哈希冲突,保证数据分散使用2的幂为长度，则length-1后为奇数，该奇数转为2进制后最后一位肯定是1。假如长度为4,则长度-1为3,再转为2进制==0000011，该值与任何hash做&amp;运算都会形成==奇数==或者==偶数==两种情况,保证数据时分散的。可能有人会想这有什么用？那么我们假如长度不是4而是3，则3-1为2,再转为2进制==0000010，该值与任何hash做&amp;运算都会形成==偶数==,那也就是说我的奇数的下标都不能用了。这样就不仅浪费一般的空间，而且增加了hash冲突的概率.</code></pre><h2 id="6、HashMap的扩容方式？负载因子是多少？为什是这么多？"><a href="#6、HashMap的扩容方式？负载因子是多少？为什是这么多？" class="headerlink" title="6、HashMap的扩容方式？负载因子是多少？为什是这么多？"></a>6、HashMap的扩容方式？负载因子是多少？为什是这么多？</h2><pre><code>为什么负载因子是0.75也是一个综合考虑，如果设置过小，HashMap每put少量的数据，都要进行一次扩容，而扩容操作会消耗大量的性能。如果设置过大的话，如果设成1，容量还是16，假设现在数组上已经占用的15个，再要put数据进来，计算数组index时，发生hash碰撞的概率将达到15/16，这违背的HashMap减少hash碰撞的原则。</code></pre><h2 id="7、HashMap的主要参数都有哪些？"><a href="#7、HashMap的主要参数都有哪些？" class="headerlink" title="7、HashMap的主要参数都有哪些？"></a>7、HashMap的主要参数都有哪些？</h2><pre><code>1.DEFAULT_INITIAL_CAPACITY缺省table大小（也就是说table长度为指定时table的默认值）2.MAXIMUM_CAPACITYtable最大长度3.DEFAULT_LOAD_FACTOR缺省负载因子大小（默认为0.75）4.TREEIFY_THRESHOLD=8树化阈值（也就是说table的node中的链表长度超过这个阈值的时候，该链表会变成树）5.UNTREEIFY_THRESHOLD=6树降级成为链表的阈值（也就是说table的node中的树长度低于这个阈值的时候，树会变成链表）6.MIN_TREEIFY_CAPACITY=64树化的另一个参数，就是当hashmap中的node的个数大于这个值的时候，hashmap中的有些链表才会变成树。这里纠正一个误区，并不是说hashmap中的某个node链表长度大于8就一定会变成树，而是说整个hashmap的node数量大于64，node的链表长度大于8才会变成树</code></pre><h2 id="8、HashMap是怎么处理hash碰撞的？"><a href="#8、HashMap是怎么处理hash碰撞的？" class="headerlink" title="8、HashMap是怎么处理hash碰撞的？"></a>8、HashMap是怎么处理hash碰撞的？</h2><pre><code>1. 使用链地址法（使用散列表）来链接拥有相同hash值的数据；2. 使用2次扰动函数（hash函数）来降低哈希冲突的概率，使得数据分布更平均；3. 引入红黑树进一步降低遍历的时间复杂度，使得遍历更快； </code></pre><h2 id="9、hash的计算规则？"><a href="#9、hash的计算规则？" class="headerlink" title="9、hash的计算规则？"></a>9、hash的计算规则？</h2><pre><code>static final int hash(Object key) &#123;    int h;    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);//key如果是null 新hashcode是0 否则 计算新的hashcode&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Map </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 中垃圾回收机制中如何判断对象需要回收？常见的 GC 回收算法有哪些？</title>
      <link href="/2022/05/06/%E9%9D%A2%E8%AF%95%E9%A2%98-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
      <url>/2022/05/06/%E9%9D%A2%E8%AF%95%E9%A2%98-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</url>
      
        <content type="html"><![CDATA[<p><a href="https://osjobs.net/topk/all/" title="Java面试题">TopK面试题</a></p><h2 id="1、中垃圾回收机制中如何判断对象需要回收"><a href="#1、中垃圾回收机制中如何判断对象需要回收" class="headerlink" title="1、中垃圾回收机制中如何判断对象需要回收"></a>1、中垃圾回收机制中如何判断对象需要回收</h2><ol><li><p><strong>引用计数算法(Reference Counting)</strong></p><p>  &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;在对象(对象头)中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一; 当引用失效时，计数器值就减一; 任何时刻计数器为零的对象就是不可能再被使用的。引用计数算法虽然占用了一些额外的内存空间来进行计数，但它的原理简单，判定效率也很高，在大多数情况下它都是一个不错的算法。这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如单纯的引用计数就很难解决对象之间相互循环引用的问题。</p></li></ol><span id="more"></span><ol><li><p><strong>可达性分析算(Reachability Analysis)</strong></p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;当前主流的商用程序语言(Java、C#)的内存管理子系统，都是通过可达性分析算法来判定对象是否存活的。这个算法的基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”(Reference Chain)，如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的</p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;在Java技术体系里面，固定可作为GC Roots的对象包括以下几种:</p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; 1. 在虚拟机栈(栈帧中的本地变量表)中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。</p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; 2. 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。</p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; 3. 在方法区中常量引用的对象，譬如字符串常量池(String Table)里的引用。</p></li></ol><h2 id="2、GC回收算法"><a href="#2、GC回收算法" class="headerlink" title="2、GC回收算法"></a>2、GC回收算法</h2><ol><li><p><strong>标记-清除(Mark-Sweep)算法:</strong></p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;如它的名字一样，算法分为“标记”和“清除”两个阶段: 首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存 活的对象，统一回收所有未被标记的对象。标记过程就是对象是否属于垃圾的判定过程</p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;它的主要缺点有两个: 第一个是执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增⻓而降低; 第二个是内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作(Full GC)。</p></li><li><p><strong>标记-复制算法:</strong></p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;为了解决标记-清除算法面对大量可回收对象时执行效率低的问题，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况， 只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效，不过其缺陷也显而易⻅，这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费未免太多了一点。</p></li><li><p><strong>标记-整理(Mark-Compact)算法:</strong></p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行，这就更加让使用者不得不小心翼翼地权衡其弊端了，像这样的停顿被最初的虚拟机设计者形象地描述为“Stop The World”(STW)。</p><p> <strong>Stop The World</strong></p><p> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;因为GC时，尽可能要让垃圾回收器专心工作，不能随便让我们写的Java系统继续新建对象了，所以此时JVM会在后台直接进入“Stop the World”：停止Java系统所有工作线程，让我们写的代码无法再运行！然后让GC线程能安心执行GC。这就能让我们的系统暂停运行，不再创建新对象，同时让GC线程尽快完成GC工作：标记和转移Eden及Survivor2的存活对象到Survivor1，然后快速地一次性回收掉Eden和Survivor2中的垃圾对象：GC完毕，即可继续恢复Java系统的工作线程运行，继续在Eden创建新对象：</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试题 </tag>
            
            <tag> GC垃圾回收 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2021新篇章</title>
      <link href="/2021/07/20/Hello/"/>
      <url>/2021/07/20/Hello/</url>
      
        <content type="html"><![CDATA[<h2 id="启程"><a href="#启程" class="headerlink" title="启程"></a>启程</h2><p>&#160; &#160; &#160; &#160; 经过两年的停更，今天决定重新开始。这两年中由于种种原因，不能继续下去。有过快乐，有过悲伤。有舍有得。总之受益良多。加油！！！</p>]]></content>
      
      
      <categories>
          
          <category> 往事 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回忆录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬取网站的种子链接并下载视频到本地</title>
      <link href="/2021/07/18/zhongzi/"/>
      <url>/2021/07/18/zhongzi/</url>
      
        <content type="html"><![CDATA[<p>首先读取网站的链接存到txt文件中</p><pre><code>import reimport urllib.requestfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.support.ui import WebDriverWaitfrom multiprocessing import Poolbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def search(url,next_title):    try:        browser.get(url)        html = browser.page_source        reg = r&#39;&lt;span style=&quot;&quot; id=&quot;streamurj&quot;&gt;(.*?)&lt;/span&gt;&#39;        wang = re.findall(reg, html)        for i in wang:            zhongzi=&#39;https://openload.co/stream/&#39; +i            print(zhongzi)            with open(&#39;lianjie.txt&#39;, &#39;a+&#39;) as f:                f.write(str(zhongzi) + &#39;\n&#39;)                f.close()            #load(product[&#39;url&#39;],product[&#39;title&#39;])    except TimeoutException:        return search(url,next_title)def load(url,title):    print(&#39;正在下载&#39;,url)    urllib.request.urlretrieve(url,&#39;mp4/%s.mp4&#39;% title)    print(&#39;下载成功&#39;,title)def index_page(url):    try:        browser.get(url)        html = browser.page_source        reg = r&#39;&lt;td class=&quot;mytd-padding&quot;&gt;&lt;a href=&quot;(.*?)&quot;&gt;&#39;        video = re.findall(reg, html)        for age in video:            next_url = &#39;http://ourjav.com/&#39; + age.split(&#39;[&#39;)[0]            next_title=age.split(&#39;[&#39;)[-1]            next_page(next_url,next_title)    except TimeoutException:        index_page(url)def next_page(url,next_title):    try:        browser.get(url)        html = browser.page_source        age=r&#39;&lt;iframe id=&quot;flash_game_object&quot; name=&quot;flash_game_object&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot; width=&quot;650&quot; height=&quot;500&quot; src=&quot;(.*?)&quot; allowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;&#39;        video=re.findall(age,html,re.S)        for image in video:            search(image,next_title)    except TimeoutException:        next_page(url, next_title)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        respon=browser.get(line3)        count=count+1        if count==5:            break    print(count)def main(offset):    url = &#39;http://ourjav.com/&#39;+&#39;index.php?&amp;page=%s&#39; % offset    index_page(url)if __name__ == &#39;__main__&#39;:    group = [x for x in range(1, 11)]    pool = Pool()    pool.map(main, group)</code></pre><p>其次从txt文件中读取url进行下载</p><pre><code>from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        browser.get(line3)        count=count+1        if count==5:            break    print(count)def main():    read_load()if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 种子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过知乎抓取粉丝</title>
      <link href="/2021/07/18/zhihu/"/>
      <url>/2021/07/18/zhihu/</url>
      
        <content type="html"><![CDATA[<h3 id="首先以此-网站-为目标路径"><a href="#首先以此-网站-为目标路径" class="headerlink" title="首先以此 网站 为目标路径"></a>首先以此 <a href="https://zhuanlan.zhihu.com/qinlibo-ml/followers">网站</a> 为目标路径</h3><h2 id="1、所需要的库"><a href="#1、所需要的库" class="headerlink" title="1、所需要的库"></a>1、所需要的库</h2><p>   requests</p><h2 id="2、对以瀑布流的json数据进行解析"><a href="#2、对以瀑布流的json数据进行解析" class="headerlink" title="2、对以瀑布流的json数据进行解析"></a>2、对以瀑布流的json数据进行解析</h2><pre><code>data=&#123;        &#39;limit&#39;:20,        &#39;offset&#39;:20    &#125;</code></pre><h2 id="3、代码演示"><a href="#3、代码演示" class="headerlink" title="3、代码演示"></a>3、代码演示</h2><pre><code>#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2018/3/13 15:40# @Author  : tomorrowliimport requestsclass Zhuhu():#初始化参数def __init__(self):    #赋值网址    self.url = &#39;https://zhuanlan.zhihu.com/api/columns/wajuejiprince/followers&#39;    self.headers=&#123;        &#39;user-agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&#39;    &#125;def getMesg(self,param):    self.html=requests.get(self.url,params=param,headers=self.headers)    for m in range(20):        &#39;&#39;&#39;        with open(&#39;zhihu.txt&#39;,&#39;a&#39;) as f:            print(self.html.json()[m][&#39;hash&#39;])            f.write(self.html.json()[m][&#39;hash&#39;]+&#39;\n&#39;)        &#39;&#39;&#39;        data=&#123;            &#39;bio&#39;:self.html.json()[m][&#39;bio&#39;],            &#39;name&#39;:self.html.json()[m][&#39;name&#39;],            &#39;profileUrl&#39;:self.html.json()[m][&#39;profileUrl&#39;],            &#39;hash&#39;:self.html.json()[m][&#39;hash&#39;]        &#125;        print(data)def main():    for n in range(0,60,20):        data=&#123;            &#39;limit&#39;:20,            &#39;offset&#39;:n        &#125;        zhihu=Zhuhu()        zhihu.getMesg(data)if __name__ == &#39;__main__&#39;:    main()</code></pre><h2 id="4、可以为每个粉丝进行发私信"><a href="#4、可以为每个粉丝进行发私信" class="headerlink" title="4、可以为每个粉丝进行发私信"></a>4、可以为每个粉丝进行发私信</h2><p>  需要获取每个粉丝的hash和自己cookie值</p>]]></content>
      
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用正则表达式来爬取某论坛的邮箱地址</title>
      <link href="/2021/07/18/youxiang/"/>
      <url>/2021/07/18/youxiang/</url>
      
        <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport requestsdef fand_email(url,counts):    data=requests.get(url)    content=data.text    pattern = r&#39;[0-9a-zA-Z._]+@[0-9a-zA-Z._]+\.[0-9a-zA-Z._]+&#39;    p = re.compile(pattern)    m = p.findall(content)    with open(&#39;emal.txt&#39;,&#39;a+&#39;) as f:        for i in m:            f.write(i+&#39;\n&#39;)            print(i)            counts= counts+1    return countsdef main():    counts=0    numbers=0    for i in range(1,32):        url=&#39;http://tieba.baidu.com/p/2314539885?pn=%s&#39;% i        number=fand_email(url,counts)        numbers=numbers+number    print(numbers)if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 邮箱地址爬取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用selenium自动爬取天眼查上的公司信息</title>
      <link href="/2021/07/18/tianyancha/"/>
      <url>/2021/07/18/tianyancha/</url>
      
        <content type="html"><![CDATA[<h2 id="Quict-Start"><a href="#Quict-Start" class="headerlink" title="Quict Start"></a>Quict Start</h2><h3 id="1、需要的库"><a href="#1、需要的库" class="headerlink" title="1、需要的库"></a>1、需要的库</h3><p>re、selenium、pymysql</p><h3 id="2、程序前的准备"><a href="#2、程序前的准备" class="headerlink" title="2、程序前的准备"></a>2、程序前的准备</h3><p>首先在自己的python路径下放入 <a href="http://npm.taobao.org/mirrors/chromedriver/">chromedriver.exe</a> 选择自己浏览器合适的版本下载</p><h3 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h3><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport reimport MySQLdbhost=&#39;localhost&#39;#主机名user=&#39;root&#39;#数据库的用户名passwd=&#39;&#39;#数据库的密码db=&#39;test&#39;#数据库的名字conn=MySQLdb.connect(host,user,passwd,db,charset=&#39;utf8&#39;)cursor = conn.cursor()boser=webdriver.Chrome()boser.maximize_window()wait=WebDriverWait(boser , 10)def getlogin():    boser.get(&#39;https://www.tianyancha.com/login&#39;)    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb30.position-rel &gt; input&quot;))    )    password=wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &#39;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb40.position-rel &gt; input&#39;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR , &#39;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.c-white.b-c9.pt8.f18.text-center.login_btn&#39;))    )    user.send_keys(&#39;用户名&#39;)    password.send_keys(&#39;密码&#39;)    submit.click()    getSearch()def getSearch():    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR , &quot;#home-main-search&quot;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR ,                                    &#39;#web-content &gt; div &gt; div.mainV3_tab1_enter.position-rel &gt; div.mainv2_tab1.position-rel &gt; div &gt; div.main-tab-outer &gt; div:nth-child(2) &gt; div &gt; div:nth-child(1) &gt; div.input-group.inputV2 &gt; div &gt; span&#39;))    )    key=&#39;佛山市顺德区&#39;    user.send_keys(key)    submit.click()    html=boser.page_source    geturl(html)    getUrlNext()def getUrlNext():    for i in range(2,5):        boser.get(r&#39;https://www.tianyancha.com/search/p%s?key=佛山市顺德区&#39;% i)        html=boser.page_source        geturl(html)def geturl(html):    try:        reg=r&#39;&lt;a href=&quot;.*?&quot; target=&quot;_blank&quot; tyc-event-click=&quot;&quot; tyc-event-ch=&quot;CompanySearch.Company&quot; style=&quot;word-break: break-all;font-weight: 600;&quot; class=&quot;query_name sv-search-company f18 in-block vertical-middle&quot;&gt;&lt;span&gt;(.*?)&lt;/span&gt;&lt;/a&gt;&#39;        names=re.findall(reg,html,re.S)        reg=r&#39;&lt;a title=&quot;.*?&quot; class=&quot;legalPersonName hover_underline&quot; target=&quot;_blank&quot; href=&quot;.*?&quot;&gt;(.*?)&lt;/a&gt;&#39;        daibiao=re.findall(reg,html,re.S)        reg=r&#39;&lt;div class=&quot;title overflow-width&quot; style=&quot;border-right: none&quot;&gt;.*?&lt;span title=&quot;.*?&quot;&gt;(.*?)&lt;/span&gt;&lt;/div&gt;&#39;        days = re.findall(reg,html,re.S)        reg=r&#39;&lt;span class=&quot;sec-c3&quot;&gt;联系电话：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&#39;        phones = re.findall(reg , html , re.S)        reg=r&#39;&lt;div class=&quot;add&quot;&gt;&lt;span class=&quot;sec-c3&quot;&gt;.*?&lt;/span&gt;&lt;span&gt;：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&#39;        location=re.findall(reg,html,re.S)        for i in range(20):            data=&#123;                &#39;name&#39;:names[i].replace(&#39;&lt;em&gt;&#39;,&#39;&#39;).replace(&#39;&lt;/em&gt;&#39;,&#39;&#39;),                &#39;daibiao&#39;:daibiao[i],                &#39;day&#39;:days[i].replace(&#39;&lt;em&gt;&#39;,&#39;&#39;).replace(&#39;&lt;/em&gt;&#39;,&#39;&#39;),                &#39;phone&#39;:phones[i].replace(&#39;&lt;em&gt;&#39;,&#39;&#39;).replace(&#39;&lt;/em&gt;&#39;,&#39;&#39;),                &#39;location&#39;: location[i].replace(&#39;&lt;em&gt;&#39;, &#39;&#39;).replace(&#39;&lt;/em&gt;&#39;,&#39;&#39;)            &#125;            print(data)            cursor.execute(&quot;insert into tianyancha(name,daibiao,day,phone,location) values(&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;)&quot;.format(data[&#39;name&#39;],data[&#39;daibiao&#39;],data[&#39;day&#39;],data[&#39;phone&#39;],data[&#39;location&#39;]))            print(data[&#39;daibiao&#39;],&#39;成功存入数据库&#39;)            conn.commit()    except:        return Nonedef main():    getlogin()if __name__ == &#39;__main__&#39;:    main()</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E5%A4%A9%E7%9C%BC%E6%9F%A5.PNG?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring的IOC</title>
      <link href="/2021/07/18/springIOC/"/>
      <url>/2021/07/18/springIOC/</url>
      
        <content type="html"><![CDATA[<h2 id="1、Spring的IOC"><a href="#1、Spring的IOC" class="headerlink" title="1、Spring的IOC"></a>1、Spring的IOC</h2><h3 id="1-1、实例化-Bean-的三种方式"><a href="#1-1、实例化-Bean-的三种方式" class="headerlink" title="1.1、实例化 Bean 的三种方式"></a>1.1、实例化 Bean 的三种方式</h3><pre><code>第一种方式：使用默认无参构造函数（最为常用）    &lt;!--在默认情况下：    它会根据默认无参构造函数来创建类对象。如果 bean 中没有默认无参构造函数，将会创建失败。    &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;/&gt;</code></pre><span id="more"></span><pre><code>第二种方式：spring 管理静态工厂-使用静态工厂的方法创建对象    /**    * 模拟一个静态工厂，创建业务层实现类    */    public class StaticFactory &#123;    public static IAccountService createAccountService()&#123;    return new AccountServiceImpl();    &#125;    &#125;    &lt;!-- 此种方式是:    使用 StaticFactory 类中的静态方法 createAccountService 创建对象，并存入 spring 容器    id 属性：指定 bean 的 id，用于从容器中获取    class 属性：指定静态工厂的全限定类名    factory-method 属性：指定生产对象的静态方法    --&gt;    &lt;bean id=&quot;accountService&quot;     class=&quot;com.itheima.factory.StaticFactory&quot;     factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt;第三种方式：spring 管理实例工厂-使用实例工厂的方法创建对象        /**        * 模拟一个实例工厂，创建业务层实现类        * 此工厂创建对象，必须现有工厂实例对象，再调用方法        */        public class InstanceFactory &#123;        public IAccountService createAccountService()&#123;        return new AccountServiceImpl();        &#125;        &#125;        &lt;!-- 此种方式是：        先把工厂的创建交给 spring 来管理。        然后在使用工厂的 bean 来调用里面的方法        factory-bean 属性：用于指定实例工厂 bean 的 id。        factory-method 属性：用于指定实例工厂中创建对象的方法。        --&gt;        &lt;bean id=&quot;instancFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt;        &lt;bean id=&quot;accountService&quot;         factory-bean=&quot;instancFactory&quot;         factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt;</code></pre><h3 id="1-2、依赖注入（DI）的三种方式"><a href="#1-2、依赖注入（DI）的三种方式" class="headerlink" title="1.2、依赖注入（DI）的三种方式"></a>1.2、依赖注入（DI）的三种方式</h3><pre><code>第一种方式：构造函数注入    &lt;!-- 使用构造函数的方式，给 service 中的属性传值        要求：            类中需要提供一个对应参数列表的构造函数。        涉及的标签：            constructor-arg        属性：            index:指定参数在构造函数参数列表的索引位置            type:指定参数在构造函数中的数据类型            name:指定参数在构造函数中的名称 用这个找给谁赋值            =======上面三个都是找给谁赋值，下面两个指的是赋什么值的==============            value:它能赋的值是基本数据类型和 String 类型            ref:它能赋的值是其他 bean 类型，也就是说，必须得是在配置文件中配置过的 bean        --&gt;        &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;            &lt;constructor-arg name=&quot;name&quot; value=&quot;张三&quot;&gt;&lt;/constructor-arg&gt;            &lt;constructor-arg name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/constructor-arg&gt;            &lt;constructor-arg name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/constructor-arg&gt;        &lt;/bean&gt;        &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;第二种方式：set 方法注入    &lt;!-- 通过配置文件给 bean 中的属性传值：使用 set 方法的方式        涉及的标签：           property        属性：            name：找的是类中 set 方法后面的部分            ref：给属性赋值是其他 bean 类型的            value：给属性赋值是基本数据类型和 string 类型的            实际开发中，此种方式用的较多。        --&gt;        &lt;bean id=&quot;accountService&quot;  class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;            &lt;property name=&quot;name&quot; value=&quot;test&quot;&gt;&lt;/property&gt;            &lt;property name=&quot;age&quot; value=&quot;21&quot;&gt;&lt;/property&gt;            &lt;property name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/property&gt;        &lt;/bean&gt;        &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> spring </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring的AOP</title>
      <link href="/2021/07/18/springAOP/"/>
      <url>/2021/07/18/springAOP/</url>
      
        <content type="html"><![CDATA[<h2 id="1、springAOP"><a href="#1、springAOP" class="headerlink" title="1、springAOP"></a>1、springAOP</h2><pre><code>作用：      在程序运行期间，不修改源码对已有方法进行增强。 优势：      减少重复代码         提高开发效率        维护方便 </code></pre><h3 id="1-1-基于接口的动态代理"><a href="#1-1-基于接口的动态代理" class="headerlink" title="1.1 基于接口的动态代理"></a>1.1 基于接口的动态代理</h3><pre><code>提供者：JDK 官方的 Proxy 类。  要求：被代理类最少实现一个接口。 </code></pre><h3 id="1，2-基于子类的动态代理"><a href="#1，2-基于子类的动态代理" class="headerlink" title="1，2 基于子类的动态代理"></a>1，2 基于子类的动态代理</h3><pre><code>提供者：第三方的 CGLib，如果报 asmxxxx 异常，需要导入 asm.jar。要求：被代理类不能用 final 修饰的类（最终类）。 </code></pre>]]></content>
      
      
      <categories>
          
          <category> spring </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬取淘宝美食信息并存储到数据库中</title>
      <link href="/2021/07/18/selenium-taobao/"/>
      <url>/2021/07/18/selenium-taobao/</url>
      
        <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport MySQLdbfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom pyquery import PyQuery as pqbroser=webdriver.Chrome()wait=WebDriverWait(broser, 10)def search(table):    try:        broser.get(&#39;https://www.taobao.com&#39;)        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#q&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &#39;#J_TSearchForm &gt; div.search-button &gt; button&#39;))        )        inputs.send_keys(&#39;美食&#39;)        submit.click()        total = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR,&#39;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total&#39;))        )        get_product(table)        return total.text    except TimeoutException:        return search(table)def next_page(page_number,table):    try:        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit&quot;))        )        inputs.clear()        inputs.send_keys(page_number)        submit.click()        wait.until(            EC.text_to_be_present_in_element((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span&quot;),str(page_number))        )        get_product(table)    except TimeoutException:        next_page(page_number,table)def get_product(table):    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,&#39;#mainsrp-itemlist .items .item&#39;)))    html=broser.page_source    doc=pq(html)    items=doc(&#39;#mainsrp-itemlist .items .item&#39;).items()    for item in items:        product=&#123;            &#39;image&#39;:item.find(&#39;.pic .img&#39;).attr(&#39;src&#39;),            &#39;price&#39;:item.find(&#39;.price&#39;).text().replace(&#39;\n&#39;,&#39;&#39;),            &#39;deal&#39;:item.find(&#39;.deal-cnt&#39;).text()[:-3],            &#39;title&#39;:item.find(&#39;.title&#39;).text().replace(&#39;\n&#39;,&#39;&#39;),            &#39;shop&#39;:item.find(&#39;.shop&#39;).text(),            &#39;location&#39;:item.find(&#39;.location&#39;).text()        &#125;        print(product)        inserttable(table, product[&#39;image&#39;], product[&#39;price&#39;], product[&#39;deal&#39;], product[&#39;title&#39;],product[&#39;shop&#39;],product[&#39;location&#39;])#连接数据库 mysqldef connectDB():        host=&quot;localhost&quot;        dbName=&quot;test&quot;        user=&quot;root&quot;        password=&quot;&quot;        #此处添加charset=&#39;utf8&#39;是为了在数据库中显示中文，此编码必须与数据库的编码一致        db=MySQLdb.connect(host,user,password,dbName,charset=&#39;utf8&#39;)        return db        cursorDB=db.cursor()        return cursorDB#创建表，SQL语言。CREATE TABLE IF NOT EXISTS 表示：表createTableName不存在时就创建def creatTable(createTableName):    createTableSql=&quot;CREATE TABLE IF NOT EXISTS &quot;+ createTableName+&quot;(image VARCHAR(255),price VARCHAR(255),deal  VARCHAR(255),title VARCHAR(255),shop VARCHAR(255),location VARCHAR(255))&quot;    DB_create=connectDB()    print(&#39;链接数据库成功&#39;)    cursor_create=DB_create.cursor()    cursor_create.execute(createTableSql)    DB_create.close()    print(&#39;creat table &#39;+createTableName+&#39; successfully&#39;)    return createTableName#数据插入表中def inserttable(insertTable,insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction):    insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(image,price,deal,title,shop,location)VALUES(%s,%s,%s,%s,%s,%s)&quot;#         insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(time,title,text,clicks)VALUES(&quot;+insertTime+&quot; , &quot;+insertTitle+&quot; , &quot;+insertText+&quot; , &quot;+insertClicks+&quot;)&quot;    DB_insert=connectDB()    cursor_insert=DB_insert.cursor()    cursor_insert.execute(insertContentSql,(insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction))    DB_insert.commit()    DB_insert.close()    print (&#39;inert contents to  &#39;+insertTable+&#39; successfully&#39;)def main():    table = creatTable(&#39;yh1&#39;)    print(&#39;创建表成功&#39;)    total=search(table)    total=int(re.compile(&#39;(\d+)&#39;).search(total).group(1))    for i in range(2,total+1):        next_page(i,table)if __name__==&#39;__main__&#39;:    main()</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E6%B7%98%E5%AE%9D%E7%BE%8E%E9%A3%9F.PNG?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美食信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用scrapy爬取伯乐在线的所有文章</title>
      <link href="/2021/07/18/scrapy_python/"/>
      <url>/2021/07/18/scrapy_python/</url>
      
        <content type="html"><![CDATA[<p>一、首先我们所需要的库</p><pre><code>1、resquests2、re</code></pre><p>二、创建一个scrapy</p><pre><code>scrapy startproject ArticleSpiderscrapy genspider jobbole www.jobbole.com</code></pre><p>三、爬取所有文章列表的url以及爬取下一页</p><pre><code>#所要爬取链接的起始urlstart_urls = [&#39;http://blog.jobbole.com/all-posts/&#39;]def parse(self, response):    #获取第一页下的所有a标签    url_list=response.css(&#39;#archive .floated-thumb .post-thumb a&#39;)    for url in url_list:        #获取img标签下的src属性，图片的链接dizhi        url_img=url.css(&#39;img::attr(src)&#39;).extract_first()        #获取每一个url_list的详情页面的url        urls=url.xpath(&#39;@href&#39;)[0].extract()        #通过parse.urljoin传递一个绝对地址        #通过meta属性向item中添加font_image_url属性        yield scrapy.Request(parse.urljoin(response.url,urls),meta=&#123;&#39;font_image_url&#39;:url_img&#125;,callback=self.get_parse)    #获取下一页的链接地址、    next=response.css(&#39;.next.page-numbers::attr(href)&#39;)[0].extract()    #一直循环，知道没有下一页为止    if next:        #回调parse函数        yield scrapy.Request(parse.urljoin(response.url,next),callback=self.parse)    else:        return None</code></pre><p>四、对详情<a href="http://blog.jobbole.com/all-posts/">文章页面</a>进行分析</p><p>&#160;&#160;&#160;&#160;我们要的数据是文章的 </p><pre><code>[标题，日期，标签，文章，点赞数，收藏数，评论数]#标题title= response.css(&#39;.grid-8 .entry-header h1::text&#39;)[0].extract()#日期data=response.css(&#39;.grid-8 .entry-meta p::text&#39;)[0].extract().strip().replace(&#39;·&#39;,&#39;&#39;).strip()#标签tag_list = response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()tag_list = [element for element in tag_list if not element.strip().endswith(&quot;评论&quot;)]#删除标签内的评论数tags = &quot;,&quot;.join(tag_list)#把标签数组以&#39;,&#39;链接生成字符串#文章article= response.css(&#39;.grid-8 .entry&#39;)[0].extract()#点赞数votetotal=response.css(&#39;.post-adds h10::text&#39;)[0].extract()match_re = re.match(&#39;.*(\d+).*&#39;, votetotal)#用正则筛选出我们所需要的数字if match_re:    votetotal=int(match_re.group(1))#返回第一个else:    votetotal=0#如国没有则默认为0#收藏数bookmark=response.css(&#39;.post-adds .bookmark-btn::text&#39;)[0].extract()match_re = re.match(&#39;.*(\d+).*&#39;, bookmark)  if match_re:    bookmark=int(match_re.group(1))else:    bookmark=0#评论数comments=response.css(&#39;.post-adds a .hide-on-480::text&#39;)[0].extract()match_re = re.match(&#39;.*(\d+).*&#39;, comments)if match_re: comments=int(match_re.group(1))else: comments=0</code></pre><p>&#160;&#160;&#160;&#160;get_parse方法来获取详情页的数据</p><pre><code>def get_parse(self,response):    #接收传过来的 font_iamgge_url    image_url=response.meta.get(&#39;font_image_url&#39;,&#39;&#39;)    title= response.css(&#39;.grid-8 .entry-header h1::text&#39;)[0].extract()    data=response.css(&#39;.grid-8 .entry-meta p::text&#39;)[0].extract().strip().replace(&#39;·&#39;,&#39;&#39;).strip()    category=response.css(&#39;.grid-8 .entry-meta p a::text&#39;)[0].extract()    tag=response.css(&#39;.grid-8 .entry-meta p a::text&#39;)[-1].extract().strip().replace(&#39;·&#39;,&#39;&#39;).strip()    article= response.css(&#39;.grid-8 .entry&#39;)[0].extract()    votetotal=response.css(&#39;.post-adds h10::text&#39;)[0].extract()    match_re = re.match(&#39;.*(\d+).*&#39;, votetotal)    if match_re:        votetotal=int(match_re.group(1))    else:        votetotal=0    bookmark=response.css(&#39;.post-adds .bookmark-btn::text&#39;)[0].extract()    match_re = re.match(&#39;.*(\d+).*&#39;, bookmark)    if match_re:        bookmark=int(match_re.group(1))    else:        bookmark=0    comments=response.css(&#39;.post-adds a .hide-on-480::text&#39;)[0].extract()    match_re = re.match(&#39;.*(\d+).*&#39;, comments)    if match_re:        comments=int(match_re.group(1))    else:        comments=0    #对item对象实例化    item=ArticlespiderItem()    item[&#39;url&#39;]=response.url    #调用md5把url压缩为固定的哈希值    item[&#39;url_object_id&#39;] = get_md5(response.url)    item[&#39;image_url&#39;]=[image_url]    #调用dtaetime库把字符串转化为date属性    try:        data=datetime.datetime.strftime(data,&quot;%Y/%m/%d&quot;).date()    except Exception as e:        #如果有异常就获取当前系统的时间        data=datetime.datetime.now().date()    item[&#39;data&#39;]=data    item[&#39;title&#39;] = title    item[&#39;category&#39;] = category    item[&#39;tag&#39;] = tag    item[&#39;article&#39;] = article    item[&#39;votetotal&#39;] = votetotal    item[&#39;bookmark&#39;]=bookmark    item[&#39;comments&#39;] = comments</code></pre><p>五、下载每一篇文章的图片</p><pre><code>在settings.py文件中加入#获取item中iamge_url的图片链接IMAGES_URLS_FIELD=&#39;image_url&#39;#获取当前的文件路径object_url=os.path.abspath(os.path.dirname(__file__))#创建image文件夹来存储图片IMAGES_STORE=os.path.join(object_url,&#39;image&#39;)</code></pre><p>&#160;&#160;&#160;&#160;在image文件下会自动生成图片的名字，在ImagesPipeline中我们会找到path变量，我们可以找到每个url所对应的图片的名字，把它存到item中</p><pre><code>class ArticleImagePipeline(ImagesPipeline):def item_completed(self, results, item, info):    if &#39;image_url&#39; in item:        for ok,value in results:            image_file_path=value[&#39;path&#39;]        item[&#39;image_url_path&#39;]=image_file_path    return item</code></pre><p>六、随着以后爬取速度加快，存的速度赶不上爬的速度，导致存的堆积影响性能，所以使用twisted将musql变成异步操作</p><pre><code>from twisted.enterprise import adbapiclass MysqlTwistePipline(object):def __init__(self,dbpool):    self.dbpool=dbpool@classmethoddef from_settings(cls,settings):    dbparms=dict(        host=settings[&#39;MYSQL_HOST&#39;],        db=settings[&#39;MYSQL_DB&#39;] ,        user=settings[&#39;MYSQL_USER&#39;] ,        passwd=settings[&#39;MYSQL_PASSWORD&#39;] ,        charset=&#39;utf8&#39;,        cursorclass=MySQLdb.cursors.DictCursor,        use_unicode=True,    )    dbpool=adbapi.ConnectionPool(&#39;MySQLdb&#39;,**dbparms)    return cls(dbpool)def process_item(self , item , spider):    #使用twisted将musql变成异步操作    query=self.dbpool.runInteraction(self.do_insert,item)    query.addErrback(self.hand_erro)def hand_erro(self,failure):    print(failure)def do_insert(self,cursor,item):    url = item[&#39;url&#39;]    url_object_id = item[&#39;url_object_id&#39;]    image_urls = item[&#39;image_url&#39;]    image_url = image_urls[0]    image_url_path = item[&#39;image_url_path&#39;]    title = item[&#39;title&#39;]    data = item[&#39;data&#39;]    category = item[&#39;category&#39;]    tag = item[&#39;tag&#39;]    article = item[&#39;article&#39;]    votetotal = item[&#39;votetotal&#39;]    bookmark = item[&#39;bookmark&#39;]    comments = item[&#39;comments&#39;]    cursor.execute(        &#39;insert into jobole(title,data,url,url_object_id,image_url,image_url_path,tag,category,article,votetotal,bookmark,comments) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)&#39; ,        (title , data , url , url_object_id , image_url , image_url_path , tag , category , article , votetotal ,         bookmark , comments))</code></pre>]]></content>
      
      
      <categories>
          
          <category> scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> scrapy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬取房天下的租房信息，并对数据进行可视化展示</title>
      <link href="/2021/07/18/ScrapyZufang/"/>
      <url>/2021/07/18/ScrapyZufang/</url>
      
        <content type="html"><![CDATA[<p>一、首先准备需要的库</p><pre><code>1、pandas//是python的一个数据分析包2、matplotlib//是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。3、jieba//jieba(结巴)是一个强大的分词库,完美支持中文分词4、wordcloud//基于Python的词云生成类库5、numpy//NumPy系统是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix））</code></pre><p>二、用scrapy创建一个项目</p><pre><code>scrapy startproject zufangSpiderscrapy genspider zufang http://zu.sh.fang.com/cities.aspx</code></pre><p>三、实现房天下的的数据爬取</p><pre><code>#初始化urldef start_requests(self):    yield scrapy.Reques(self.city_url,callback=self.get_city)#调用get_city()方法 def get_city(self,response):    url=response.xpath(&#39;/html/body/div[3]/div[1]/a/@href&#39;).extract()    #循环热门城市    for i in url:        product=&#123;            &#39;city&#39;:i        &#125;        yield scrapy.Request(product[&#39;city&#39;],callback=self.city_parse, dont_filter=True)        #循环爬取每一页        for j in range(2,10):            next_url=product[&#39;city&#39;]+&#39;house/i3%s&#39;%j            yield scrapy.Request(next_url,callback=self.city_parse,dont_filter=True)#调用city_parse()方法获取每一页的数据def city_parse(self, response):    zufang = response.xpath(&#39;//div[@class=&quot;houseList&quot;]&#39;)    for fangzi in zufang:        title=fangzi.xpath(&#39;//p[@class=&quot;title&quot;]/a/text()&#39;).extract()        area =fangzi.xpath(&#39;//p[@class=&quot;gray6 mt20&quot;]/a[1]/span[1]/text()&#39;).extract()        rent_style = fangzi.xpath(&#39;//p[@class=&quot;font16 mt20 bold&quot;]/text()[1]&#39;).extract()        house_type= fangzi.xpath(&#39;//p[@class=&quot;font16 mt20 bold&quot;]/text()[2]&#39;).extract()        house_area = fangzi.xpath(&#39;//p[@class=&quot;font16 mt20 bold&quot;]/text()[3]&#39;).extract()        if fangzi.xpath(&#39;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&#39;):            orientation = fangzi.xpath(&#39;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&#39;).extract()        else:            orientation=&#39;&#39;        price = fangzi.xpath(&#39;//p[@class=&quot;mt5 alingC&quot;]/span/text()&#39;).extract()        for i in range(len(title)):            item = ZufangScrapyItem()            item[&#39;title&#39;]=title[i]            item[&#39;area&#39;]=area[i]            item[&#39;rent_style&#39;]=rent_style[i].strip()            item[&#39;house_type&#39;]=house_type[i]            item[&#39;house_area&#39;]=house_area[i]            item[&#39;orientation&#39;]=orientation[i].strip()            item[&#39;price&#39;]=price[i]            yield item</code></pre><p>四、对数据进行可视化</p><pre><code>1、首先运行 scrapy crawl zufang -o zufang.csv把数据保存成csv文件2、import pandas as pddata=pd.read_csv(r&#39;H:\Python\zufang_scrapy\zufang.csv&#39;)data.head()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang.jpg?raw=true" alt="zufang.csv"></p><pre><code>3、import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]data[&#39;orientation&#39;].value_counts().plot(kind=&#39;barh&#39;,rot=0)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang1.jpg?raw=true" alt="image"></p><pre><code>4、final=&#39;&#39;stopword=[&#39;NaN&#39;]for n in range(data.shape[0]):seg_list=list(jieba.cut(data[&#39;area&#39;][n]))for seg in seg_list:    if seg not in stopword:        final=final+seg+&#39; &#39;my_wordcloud=WordCloud(collocations=False,font_path=r&#39;C:\Windows\Fonts\simfang.ttf&#39;,width=2000,height=600,margin=2).generate(final)plt.imshow(my_wordcloud)plt.axis(&#39;off&#39;)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang2.jpg?raw=true" alt="image"></p><p>详细代码：可以访问我的 <a href="https://github.com/TomorrowLi/ScraptZufaang">GitHub</a> 地址</p>]]></content>
      
      
      <categories>
          
          <category> Scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Scrapy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用scrapy爬取知乎用户信息并存储在mongodb数据库中</title>
      <link href="/2021/07/18/scrapyZhihu/"/>
      <url>/2021/07/18/scrapyZhihu/</url>
      
        <content type="html"><![CDATA[<p>一、首先在setting.py中修改True改为False</p><pre><code># Obey robots.txt rules#允许爬取robots.txt文件内不能爬取的资源ROBOTSTXT_OBEY = True#知乎是由反扒措施的必须添加User-agent以及authorizationDEFAULT_REQUEST_HEADERS = &#123;  &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,  &#39;Accept-Language&#39;: &#39;en&#39;,    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&#39; ,    &#39;authorization&#39;:&#39;Bearer 2|1:0|10:1521707334|4:z_c0|80:MS4xS1NYS0FnQUFBQUFtQUFBQVlBSlZUVWEzb0Z0ZDl0MGhaa0VJOWM0ODhiejhRenVUd0tURnFnPT0=|98bb6f4900c1b4b52ece0282393ede5e31046c9a90216e69dec1feece8086ee0&#39;&#125;</code></pre><p>二、编写spider的项目文件</p><p>1、zhihu_user.py</p><pre><code># -*- coding: utf-8 -*-import jsonimport scrapyfrom scrapy import Spider,Requestfrom ..items import UserItemclass ZhihuUserSpider(Spider):    name = &#39;zhihu_user&#39;    allowed_domains = [&#39;www.zhihu.com&#39;]    start_urls = [&#39;http://www.zhihu.com/&#39;]    start_user=&#39;excited-vczh&#39;    user_url=&#39;https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;&#39;    user_query=&#39;allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&#39;    follows_url=&#39;https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;&#39;    follows_query=&#39;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&#39;    followers_url=&#39;https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;&#39;    followers_query=&#39;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&#39;    def start_requests(self):        yield Request(self.user_url.format(user=self.start_user,include=self.user_query),self.parse_user)        yield Request(self.follows_url.format(user=self.start_user,include=self.follows_query,offset=0,limit=20),callback=self.parse_query)    def parse_user(self, response):        result=json.loads(response.text)        item=UserItem()        for field in item.fields:            if field in result.keys():                item[field]=result.get(field)        yield item        yield Request(self.follows_url.format(user=result.get(&#39;url_token&#39;),include=self.follows_query,offset=0,limit=20),callback=self.parse_query)        yield Request(self.followers_url.format(user=result.get(&#39;url_token&#39;),include=self.followers_query,offset=0,limit=20),callback=self.parse_querys)    def parse_query(self, response):        results=json.loads(response.text)        if &#39;data&#39; in results.keys():            for result in results.get(&#39;data&#39;):                yield Request(self.user_url.format(user=result.get(&#39;url_token&#39;),include=self.user_query),callback=self.parse_user)        if &#39;paging&#39; in results.keys() and results.get(&#39;paging&#39;).get(&#39;is_end&#39;)==False:            next_page=results.get(&#39;paging&#39;).get(&#39;next&#39;)            yield Request(next_page,self.parse_query)    def parse_querys(self, response):        results=json.loads(response.text)        if &#39;data&#39; in results.keys():            for result in results.get(&#39;data&#39;):                yield Request(self.user_url.format(user=result.get(&#39;url_token&#39;),include=self.user_query),callback=self.parse_user)        if &#39;paging&#39; in results.keys() and results.get(&#39;paging&#39;).get(&#39;is_end&#39;)==False:            next_page=results.get(&#39;paging&#39;).get(&#39;next&#39;)            yield Request(next_page,self.parse_querys)</code></pre><p>2、items.py</p><pre><code># -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyfrom scrapy import Item,Fieldclass UserItem(Item):    # define the fields for your item here like:    # name = scrapy.Field()    headline=Field()    avatar_url=Field()    name=Field()    type=Field()    url_token=Field()    user_type=Field()</code></pre><p>3、pipelines.py</p><pre><code># -*- coding: utf-8 -*-# Define your item pipelines here## Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongoclass MongoPipeline(object):    collection_name = &#39;scrapy_items&#39;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&#39;MONGO_URI&#39;),            mongo_db=crawler.settings.get(&#39;MONGO_DATABASE&#39;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[&#39;user&#39;].update(&#123;&#39;url_token&#39;:item[&#39;url_token&#39;]&#125;,&#123;&#39;$set&#39;:item&#125;,True)        #self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、setting.py</p><pre><code>BOT_NAME = &#39;zhihu&#39;SPIDER_MODULES = [&#39;zhihu.spiders&#39;]NEWSPIDER_MODULE = &#39;zhihu.spiders&#39;MONGO_URI=&#39;localhost&#39;MONGO_DATABASE=&#39;zhihu&#39;</code></pre><p>三、最后启动setting.py中的</p><pre><code>ITEM_PIPELINES = &#123;       &#39;zhihu.pipelines.MongoPipeline&#39;: 300,&#125;#去掉注释</code></pre><p>四、结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zhihuMongodb.PNG?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scrapy </tag>
            
            <tag> zhihu爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scrapy爬虫框架的简介</title>
      <link href="/2021/07/18/scrapy/"/>
      <url>/2021/07/18/scrapy/</url>
      
        <content type="html"><![CDATA[<p>一、安装scrapy所依赖的库</p><pre><code>1、安装wheel    pip install wheel2、安装lxml    https://pypi.python.org/pypi/lxml/4.1.03、安装pyopenssl    https://pypi.python.org/pypi/pyOpenSSL/17.5.04、安装Twisted    https://www.lfd.uci.edu/~gohlke/pythonlibs/5、安装pywin32    https://sourceforge.net/projects/pywin32/files/6、安装scrapy    pip install scrapy</code></pre><p>二、爬虫举例</p><p>1、创建项目</p><pre><code>scrapy startproject zhihu</code></pre><p>2、创建spider项目程序</p><pre><code>cd zhihuscrapy genspider zhihuspider www.zhihu.com</code></pre><p>3、自动创建目录及文件</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/scrapy.jpg?raw=true" alt="image"></p><p>4、生成的爬虫具有基本的结构，我们可以直接在此基础上编写代码</p><pre><code># -*- coding: utf-8 -*-import scrapyclass ZhihuSpider(scrapy.Spider):name = &quot;zhihuspider&quot;allowed_domains = [&quot;zhihu.com&quot;]start_urls = [&#39;http://www.zhihu.com/&#39;]def parse(self, response):    pass</code></pre><p>5、然后，可以我们按照name来运行爬虫</p><pre><code>scrapy crawl &#39;zhihuspider&#39;</code></pre><p>二、项目文件详细分析</p><p>1、spriders文件夹</p><pre><code>#项目文件,以后的代码都要在这里写入zhihuspider.py</code></pre><p>2、items.py</p><pre><code>from scrapy import Item,Fieldclass UserItem(Item):    #Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。    # define the fields for your item here like:    #爬取所需要的数据的名称在这里定义，很像字典    # name = scrapy.Field()</code></pre><p>3、pipelines.py</p><p>&#160; &#160; &#160; &#160; 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理</p><p>###主要是处理以下4点任务</p><p>1.清理HTML数据</p><p>2.验证爬取的数据(检查item包含某些字段)</p><p>3.查重(并丢弃)</p><p>4.将爬取结果保存到数据库中</p><pre><code>#存到mogodb数据库import pymongoclass MongoPipeline(object):    collection_name = &#39;scrapy_items&#39;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&#39;MONGO_URI&#39;),            mongo_db=crawler.settings.get(&#39;MONGO_DATABASE&#39;, &#39;items&#39;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、settings.py</p><p>&#160; &#160; &#160; &#160; 用于设置常量的,例如 mongodb 的 url 和 database</p><pre><code>BOT_NAME = &#39;zhihu&#39;SPIDER_MODULES = [&#39;zhihu.spiders&#39;]NEWSPIDER_MODULE = &#39;zhihu.spiders&#39;MONGO_URI=&#39;localhost&#39;MONGO_DATABASE=&#39;zhihu&#39;#默认是True的这个变量是用来是否爬robot文件的改为False是允许的意思ROBOTSTXT_OBEY = True</code></pre>]]></content>
      
      
      <categories>
          
          <category> scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scrapy </tag>
            
            <tag> 爬虫框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python爬虫的精华篇</title>
      <link href="/2021/07/18/python/"/>
      <url>/2021/07/18/python/</url>
      
        <content type="html"><![CDATA[<h2 id="最简单的反爬手段"><a href="#最简单的反爬手段" class="headerlink" title="最简单的反爬手段"></a>最简单的反爬手段</h2><p>&#160; &#160; &#160; &#160; 有一部分网站（甚至包括部分商业网站）其实并没有真正认真做过反爬虫这件事情，你不需要做任何伪装，即可分分钟抓光它的网站。面对这样的网站，你只需要开大马力抓抓抓就好了，如果处于情怀考虑，你可以抓的慢一点，让它的服务器别太累。</p><p>&#160; &#160; &#160; &#160; 除去这样的情况，最简单的反爬虫机制，应该算是U-A校验了。浏览器在发送请求的时候，会附带一部分浏览器及当前系统环境的参数给服务器，这部分数据放在HTTP请求的header部分。</p><h2 id="通过访问频度反爬"><a href="#通过访问频度反爬" class="headerlink" title="通过访问频度反爬"></a>通过访问频度反爬</h2><p>&#160; &#160; &#160; &#160; 这种反爬虫机制的原理是，真人通过浏览器访问网站的速度（相对程序来讲）是很慢的，所以如果你一秒访问了200个页面——这里不代表实际数值，只是表达在较短时间内发送了较多请求，具体阈值需要具体考虑——服务器则认为你是一个爬虫。</p><p>适用情况：限制频率情况。</p><p>Requests，Urllib2都可以使用time库的sleep()函数：</p><pre><code>import timetime.sleep(1)</code></pre><h2 id="通过验证码限制"><a href="#通过验证码限制" class="headerlink" title="通过验证码限制"></a>通过验证码限制</h2><p>&#160; &#160; &#160; &#160; 这样的方式与上面的方式相比更加难处理的是，不管你的访问频次怎么样，你都需要输入验证码才行。比如12306，不管你是登录还是购票，也不论你是第一次还是第一万次购买，你都需要输入一个验证码之后才能继续。这时候，在绝大部分情况下，你必须要想办法识别验证码了。之所以说是大多数情况下，是因为在极少数极少数情况下（尤其是政府网站），棒槌程序员通过客户端的JavaScript来校验验证码，这时候对你的爬虫来讲，其实是没有验证码的（比如中国商标网）。除开你遇到这种几乎可以忽略不计的棒槌开发的网站，其他时候你只有通过识别验证码来继续后面的操作了。</p><p>1、首先你可以找到验证吗图片的url<br>2、把图片保存到本地,然后手动输入验证码达到验证的效果，可以参考我前面的文章，有关豆瓣验证码的处理</p><h2 id="通过经常变换网页结构"><a href="#通过经常变换网页结构" class="headerlink" title="通过经常变换网页结构"></a>通过经常变换网页结构</h2><p>&#160; &#160; &#160; &#160; 常见于一些社交网站，他们会经常更换网页的结构，如果你是通过依赖网页结构来解析需要的数据（不幸的是大部分情况下我们都需要通过网页结构来解析需要的数据），则在原本的网页位置找不到原本需要的内容。这时候，要分不同情况具体处理了。如果你只打算一次性抓取特定数据，那么赶快行动，它以后结构变了无所谓，反正你也不打算在抓它一次。如果是需要持续性抓取的网站，就要仔细思考下应对方案了。一个简单粗暴但是比较费力的办法是，写一个校验脚本，定期校验网页结构，如果与预期不符，那么赶快通知自己（或者特定开发者）修改解析部分，以应对网页结构变化。</p><h2 id="对于Ajax请求的处理"><a href="#对于Ajax请求的处理" class="headerlink" title="对于Ajax请求的处理"></a>对于Ajax请求的处理</h2><p>&#160; &#160; &#160; &#160; 对于“加载更多”情况，使用Ajax来传输很多数据。它的工作原理是：从网页的url加载网页的源代码之后，会在浏览器里执行JavaScript程序。这些程序会加载更多的内容，“填充”到网页里。这就是为什么如果你直接去爬网页本身的url，你会找不到页面的实际内容。这里，若使用Google Chrome分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。</p><h2 id="自动化测试工具Selenium"><a href="#自动化测试工具Selenium" class="headerlink" title="自动化测试工具Selenium"></a>自动化测试工具Selenium</h2><p>&#160; &#160; &#160; &#160; Selenium是一款自动化测试工具。它能实现操纵浏览器，包括字符填充、鼠标点击、获取元素、页面切换等一系列操作。总之，凡是浏览器能做的事，Selenium都能够做到。</p><h2 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h2><p>&#160; &#160; &#160; &#160; 适用情况：限制IP地址情况，也可解决由于“频繁点击”而需要输入验证码登陆的情况。<br>这种情况最好的办法就是维护一个代理IP池，网上有很多免费的代理IP，良莠不齐，可以通过筛选找到能用的。对于“频繁点击”的情况，我们还可以通过限制爬虫访问网站的频率来避免被网站禁掉。</p><pre><code>    proxies = &#123;&#39;http&#39;:&#39;http://XX.XX.XX.XX:XXXX&#39;&#125;    Requests：//强烈推荐使用    import requests    response = requests.get(url=url, proxies=proxies)    Urllib2：    import urllib2    proxy_support = urllib2.ProxyHandler(proxies)    opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)    urllib2.install_opener(opener) # 安装opener，此后调用urlopen()时都会使用安装过的opener对象    response = urllib2.urlopen(url)</code></pre><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>&#160; &#160; &#160; &#160; 由于Python的是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。<br>multiprocessing提供模块一个了Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：<br>    from multiprocessing import Process<br>    import os</p><pre><code>#子进程要执行的代码def run_proc(name):    print &#39;Run child process %s (%s)...&#39; % (name, os.getpid())if __name__==&#39;__main__&#39;:    print &#39;Parent process %s.&#39; % os.getpid()    p = Process(target=run_proc, args=(&#39;test&#39;,))    print &#39;Process will start.&#39;    p.start()    p.join()    print &#39;Process end.&#39;</code></pre><p>执行结果如下：</p><pre><code>Parent process 928.Process will start.Run child process test (929)...Process end.</code></pre><p>参考 <a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/00143192823818768cd506abbc94eb5916192364506fa5d000">廖雪峰</a> 的python教程</p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>今日头条上的美女图片进行爬取</title>
      <link href="/2021/07/18/python-student/"/>
      <url>/2021/07/18/python-student/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到<a href="https://www.python.org/">python</a>!学习。 这是我人生旅途的第一课，非常重要。 我从这个案例中学到的许多关于python爬虫的知识，这是我python爬虫的启蒙。</p><p>#所需要的库<br>requests：用于网页请求</p><p>BeautifulSoup：选择所要的元素</p><p>json：用来解析json数据的库</p><p>re：用于正则表达式的筛选</p><p>代码演示：</p><pre><code>import jsonimport refrom hashlib import md5import osimport requestsfrom urllib.parse import urlencodefrom bs4 import BeautifulSoupfrom config import *from multiprocessing import Pool#对要爬取页面的解析def get_page_index(offset, keyword):    data = &#123;        &#39;offset&#39;: offset,        &#39;format&#39;: &#39;json&#39;,        &#39;keyword&#39;: keyword,        &#39;autoload&#39;: &#39;true&#39;,        &#39;count&#39;: &#39;20&#39;,        &#39;cur_tab&#39;: 3,        &#39;from&#39;:&#39;gallery&#39;    &#125;    url = &#39;https://www.toutiao.com/search_content/?&#39; + urlencode(data)    try:        header = &#123;            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&#39;        &#125;        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&#39;请求索引失败&#39;)        return None#获取所要的数据在json中def prase_get_index(html):    data = json.loads(html)    if data and &#39;data&#39; in data.keys():        for item in data.get(&#39;data&#39;):            yield item.get(&#39;article_url&#39;)#用正则对json数据进行解析def prase_page_urlli(html):    soup = BeautifulSoup(html, &#39;lxml&#39;)    title = soup.select(&#39;title&#39;)[0].get_text()    print(title)    images_pattern= re.compile(&#39;gallery: JSON.parse\((.*?)\)&#39;,re.S)    result=re.search(images_pattern,html)    if result:        data=json.loads(result.group(1))        data=eval(data)        if data and &#39;sub_images&#39;in data.keys():            sub_images=data.get(&#39;sub_images&#39;)            image=[item.get(&#39;url&#39;).replace(&#39;\\&#39;,&#39;&#39;) for item in sub_images]            for images_page in image:                dowload_image(images_page)            return&#123;                &#39;title&#39;:title,                &#39;image&#39;:image            &#125;#对所要下载的图片链接进行访问def get_page_urlli(url):    try:        header = &#123;            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&#39;        &#125;        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&#39;请求失败&#39;,url)        return None#下载图片def dowload_image(url):    print(&quot;正在下载&quot;,url)    try:        header = &#123;            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&#39;        &#125;        response = requests.get(url,headers=header)        if response.status_code == 200:            save_image(response.content)        return None    except:        print(&#39;请求图片出错&#39;,url)        return None#把图片保存到本地路径下def save_image(content):    file_path=&#39;&#123;0&#125;/&#123;1&#125;.&#123;2&#125;&#39;.format(os.getcwd(),md5(content).hexdigest(),&#39;jpg&#39;)    if not os.path.exists(file_path):        with open(file_path,&#39;wb&#39;) as f:            f.write(content)            f.close()def main(offset):    html = get_page_index(offset, keyword)    for url in prase_get_index(html):        html=get_page_urlli(url)        if html:            result=prase_page_urlli(html)            print(result)if __name__ == &#39;__main__&#39;:    group=[x*20 for x in range(stat,end+1)]    pool=Pool()    pool.map(main,group)</code></pre><p>详细的解释。</p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美女图片 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度文库下载和百度云加速下载</title>
      <link href="/2021/07/18/pojie/"/>
      <url>/2021/07/18/pojie/</url>
      
        <content type="html"><![CDATA[<h3 id="一、百度文库、道客巴巴登免费下载"><a href="#一、百度文库、道客巴巴登免费下载" class="headerlink" title="一、百度文库、道客巴巴登免费下载"></a>一、百度文库、道客巴巴登免费下载</h3><p>&#160;&#160;&#160;&#160;1、百度文库下载：首先进入你要下载的文档页面</p><pre><code>例如:https://wenku.baidu.com/view/eeb5d53069eae009591bec34.html?from=search</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;2、把图中的链接复制到冰点文库的下载</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu1.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;3、文件下载地址：</p><pre><code>链接：https://pan.baidu.com/s/1G0zTlsG61ipGnGfmHPEijw密码: bzjm</code></pre><h3 id="二、百度云加速下载"><a href="#二、百度云加速下载" class="headerlink" title="二、百度云加速下载"></a>二、百度云加速下载</h3><p>&#160;&#160;&#160;&#160;1、首先要登陆你的百度云账号</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baiduyun.PNG?raw=true" alt="image"></p><pre><code>链接：https://pan.baidu.com/s/1Lzm5_8lRPHO0e5r38rl-3A密码: mcc5</code></pre>]]></content>
      
      
      <categories>
          
          <category> 破解软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 破解软解 </tag>
            
            <tag> 百度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对百度糯米、美团、大众点评的数据进行爬取并保存到 mongodb 数据库和mysql数据库</title>
      <link href="/2021/07/18/nuomi-meituan-dazhong/"/>
      <url>/2021/07/18/nuomi-meituan-dazhong/</url>
      
        <content type="html"><![CDATA[<p>一、首先准备我们需要的库</p><pre><code>1、requests//用来请求网页2、json//用来解析json数据，用于把json数据转换为字典3、re//利用正则对字符串查找3、pyquery//利用css查找4、pymongo//存取pymongo5、MySQLdb//存取mysql</code></pre><p>创建一个配置文件config.py用于存储数据库的密码</p><pre><code>MYSQL_HOST=&#39;localhost&#39;MYSQL_USER=&#39;root&#39;MYSQL_PASSWORD=&#39;&#39;MYSQL_DB=&#39;test&#39;MOGO_URL=&#39;localhost&#39;MOGO_DB=&#39;nuomi&#39;MOGO_TABLE=&#39;product&#39;MOGO_DB_M=&#39;meituan&#39;MOGO_TABLE_M=&#39;product&#39;MOGO_DB_D=&#39;dazhong&#39;MOGO_TABLE_D=&#39;product&#39;</code></pre><p>二、百度糯米</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&#39;https://www.nuomi.com/pcindex/main/changecity&#39;    response=requests.get(url)    response.encoding=&#39;utf-8&#39;    doc=pq(response.text)    items=doc(&#39;.city-list .cities li&#39;).items()    for item in items:        product=&#123;            &#39;city&#39;:item.find(&#39;a&#39;).text(),            &#39;url&#39;:&#39;https:&#39;+item.find(&#39;a&#39;).attr(&#39;href&#39;)        &#125;        get_pase(product[&#39;url&#39;],keyword)</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_pase(url,keyword):    head=&#123;        &#39;k&#39;:keyword,    &#125;    urls=url+&#39;/search?&#39;+urlencode(head)    response=requests.get(urls)    response.encoding = &#39;utf-8&#39;    req=re.findall(&#39;noresult-tip&#39;,response.text)    if req:        print(&#39;抱歉,没有找到你搜索的内容&#39;)    else:        req=r&#39;&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;.*?&quot; class=&quot;shop-infoo-list-item-img&quot; /&gt;&lt;/a&gt;&#39;        url_req=re.findall(req,response.text)        for i in url_req:            url_pase=&#39;https:&#39;+i            get_pase_url(url_pase)        req=r&#39;&lt;a href=&quot;(.*?)&quot; .*? class=&quot;ui-pager-normal&quot; .*?&lt;/a&gt;&#39;        url_next=re.findall(req,response.text)        for i in url_next:            url_pases=url+i            get_pase_url(url_pases)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_pase_url(url):    response=requests.get(url)    response.encoding = &#39;utf-8&#39;    doc=pq(response.text)    product=&#123;        &#39;title&#39;:doc(&#39;.shop-box .shop-title&#39;).text(),        &#39;score&#39;:doc(&#39;body &gt; div.main-container &gt; div.shop-box &gt; p &gt; span.score&#39;).text(),        &#39;price&#39;:doc(&#39;.shop-info .price&#39;).text(),        &#39;location&#39;:doc(&#39;.item .detail-shop-address&#39;).text(),        &#39;phone&#39;:doc(&#39;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(2) &gt; p&#39;).text(),        &#39;time&#39;:doc(&#39;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(3) &gt; p&#39;).text(),        &#39;tuijian&#39;:doc(&#39;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(4) &gt; p&#39;).text()    &#125;    print(product)    save_mysql(product)    #save_mongodb(product)</code></pre><p>4、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&#39;utf8&#39;)    cursor = conn.cursor()    cursor.execute(&quot;insert into nuomi(title,score,price,location,phone,time,tuijian) values(&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;)&quot;.format(product[&#39;title&#39;] , product[&#39;score&#39;] , product[&#39;price&#39;] , product[&#39;location&#39;] , product[&#39;phone&#39;] ,product[&#39;time&#39;] , product[&#39;tuijian&#39;]))    print(&#39;成功存入数据库&#39;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB]    try:        if db[MOGO_TABLE].insert(result):            print(&#39;保存成功&#39;,result)    except Exception:        print(&#39;保存失败&#39;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%99%BE%E5%BA%A6%E7%B3%AF%E7%B1%B3.jpg?raw=true" alt="image"></p><p>二、美团</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&#39;http://www.meituan.com/changecity/&#39;    response=requests.get(url)    response.encoding=&#39;utf-8&#39;    doc=pq(response.text)    items=doc(&#39;.city-area .cities .city&#39;).items()    for item in items:        product=&#123;            &#39;url&#39;:&#39;http:&#39;+item.attr(&#39;href&#39;),            &#39;city&#39;:item.text()        &#125;        get_url_number(product[&#39;url&#39;])</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_number(url):    try:        response=requests.get(url)        req=r&#39;&#123;&quot;currentCity&quot;:&#123;&quot;id&quot;:(.*?),&quot;name&quot;:&quot;.*?&quot;,&quot;pinyin&quot;:&#39;        number_url=re.findall(req,response.text)        for code in range(0,500,32):            url=&#39;http://apimobile.meituan.com/group/v4/poi/pcsearch/&#123;&#125;?limit=32&amp;offset=&#123;&#125;&amp;q=&#123;&#125;&#39;.format(number_url[0],code,keyword)            response=requests.get(url)            data=json.loads(response.text)            imageUrl=data[&#39;data&#39;][&#39;searchResult&#39;][0][&#39;imageUrl&#39;]            address=data[&#39;data&#39;][&#39;searchResult&#39;][0][&#39;address&#39;]            lowestprice=data[&#39;data&#39;][&#39;searchResult&#39;][0][&#39;lowestprice&#39;]            title=data[&#39;data&#39;][&#39;searchResult&#39;][0][&#39;title&#39;]            url_id=data[&#39;data&#39;][&#39;searchResult&#39;][0][&#39;id&#39;]            product=&#123;                &#39;url_id&#39;:url_id,                &#39;imageUrl&#39;:imageUrl,                &#39;address&#39;:address,                &#39;lowestprice&#39;:lowestprice,                &#39;title&#39;:title            &#125;            save_mysql(product)    except Exception:        return None</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&#39;utf8&#39;)    cursor = conn.cursor()    cursor.execute(&quot;insert into meituan(url_id,imageUrl,address,lowestprice,title) values(&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;)&quot;.format(product[&#39;url_id&#39;], product[&#39;imageUrl&#39;], product[&#39;address&#39;], product[&#39;lowestprice&#39;], product[&#39;title&#39;]))    print(&#39;成功存入数据库&#39;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_M]    try:        if db[MOGO_TABLE_M].insert(result):            print(&#39;保存成功&#39;,result)    except Exception:        print(&#39;保存失败&#39;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%BE%8E%E5%9B%A2.jpg?raw=true" alt="image"></p><p>三、大众点评</p><p>1、获取全国的url</p><pre><code>def get_url_city_id():    url = &#39;https://www.dianping.com/ajax/citylist/getAllDomesticCity&#39;    headers = &#123;        &#39;User-Agent&#39;: &#39;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&#39; ,    &#125;    response = requests.get(url , headers=headers)    data=json.loads(response.text)    for i in range(1,35):        url_data=data[&#39;cityMap&#39;][str(i)]        for item in url_data:            product=&#123;                &#39;cityName&#39;:item[&#39;cityName&#39;],                &#39;cityId&#39;:item[&#39;cityId&#39;],                &#39;cityEnName&#39;:item[&#39;cityEnName&#39;]            &#125;            get_url_keyword(product)            break</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_keyword(product):    urls = &#39;https://www.dianping.com/search/keyword/&#123;&#125;/0_%&#123;&#125;&#39;.format(product[&#39;cityId&#39;], keyword)    headers = &#123;        &#39;User-Agent&#39;: &#39;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&#39; ,    &#125;    response = requests.get(urls, headers=headers)    req=r&#39;data-hippo-type=&quot;shop&quot; title=&quot;.*?&quot; target=&quot;_blank&quot; href=&quot;(.*?)&quot;&#39;    data=re.findall(req,response.text)    for url in data:        get_url_data(url)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_url_data(url):    headers= &#123;        &#39;User-Agent&#39;: &#39;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&#39; ,        &#39;Host&#39;: &#39;www.dianping.com&#39;,        &#39;Pragma&#39;: &#39;no - cache&#39;,        &#39;Upgrade - Insecure - Requests&#39;: &#39;1&#39;    &#125;    response = requests.get(url,headers=headers)    doc=pq(response.text)    title=doc(&#39;#basic-info &gt; h1&#39;).text().replace(&#39;\n&#39;,&#39;&#39;).replace(&#39;\xa0&#39;,&#39;&#39;)    avgPriceTitle=doc(&#39;#avgPriceTitle&#39;).text()    taste=doc(&#39;#comment_score &gt; span:nth-of-type(1)&#39;).text()    Environmental=doc(&#39;#comment_score &gt; span:nth-of-type(2)&#39;).text()    service=doc(&#39;#comment_score &gt; span:nth-of-type(3)&#39;).text()    street_address=doc(&#39;#basic-info &gt; div.expand-info.address &gt; span.item&#39;).text()    tel=doc(&#39;#basic-info &gt; p &gt; span.item&#39;).text()    info_name=doc(&#39;#basic-info &gt; div.promosearch-wrapper &gt; p &gt; span&#39;).text()    time=doc(&#39;#basic-info &gt; div.other.J-other &gt; p:nth-of-type(1) &gt; span.item&#39;).text()    product=&#123;        &#39;title&#39;:title,        &#39;avgPriceTitle&#39;:avgPriceTitle,        &#39;taste&#39;: taste ,        &#39;Environmental&#39;:Environmental,        &#39;service&#39;: service ,        &#39;street_address&#39;:street_address,        &#39;tel&#39;: tel ,        &#39;info_name&#39;:info_name,        &#39;time&#39;:time    &#125;    save_mysql(product)</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&#39;utf8&#39;)    cursor=conn.cursor()    cursor.execute(&quot;insert into dazhong(title,avgPriceTitle,taste,Environmental,service,street_address,tel,info_name,time) values(&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;,&#39;&#123;&#125;&#39;)&quot;.format(product[&#39;title&#39;],product[&#39;avgPriceTitle&#39;],product[&#39;taste&#39;],product[&#39;Environmental&#39;],product[&#39;service&#39;],product[&#39;street_address&#39;],product[&#39;tel&#39;],product[&#39;info_name&#39;],product[&#39;time&#39;]))    print(&#39;成功存入数据库&#39; , product)def save_mogodb(product):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_D]    try:        if db[MOGO_TABLE_D].insert(product):            print(&#39;保存成功&#39;,product)    except Exception:        print(&#39;保存失败&#39;,product)</code></pre><p>详细的描述可以访问我的Github <a href="https://github.com/TomorrowLi/meituan-nuomi-dazhong.git">TomorrowLi</a> 里面有我的源码</p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 全国订餐信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最新的模拟知乎登陆</title>
      <link href="/2021/07/18/loginZhihu/"/>
      <url>/2021/07/18/loginZhihu/</url>
      
        <content type="html"><![CDATA[<p>&#160;&#160;&#160;&#160;首先我们知道随着知乎页面的不断改版，以前的模拟登陆以不能用了，以下是对知乎改版之后的最新登陆方法</p><p>一、首先我们所需要的库</p><pre><code>import requestsimport timeimport re#用于下载验证码图片import base64#通过 Hmac 算法计算返回签名。实际是几个固定字符串加时间戳import hmacimport hashlibimport jsonimport matplotlib.pyplot as plt#保存cookiefrom http import cookiejar#打开图片from PIL import Image</code></pre><p>二、所需要的头信息</p><pre><code>#所需要的头部信息HEADERS = &#123;&#39;Connection&#39;: &#39;keep-alive&#39;,&#39;Host&#39;: &#39;www.zhihu.com&#39;,&#39;Referer&#39;: &#39;https://www.zhihu.com/&#39;,&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 &#39;              &#39;(KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36&#39;&#125;#登陆使的urlLOGIN_URL = &#39;https://www.zhihu.com/signup&#39;LOGIN_API = &#39;https://www.zhihu.com/api/v3/oauth/sign_in&#39;FORM_DATA = &#123;    #客户端id基本不会改变    &#39;client_id&#39;: &#39;c3cef7c66a1843f8b3a9e6a1e3160e20&#39;,    &#39;grant_type&#39;: &#39;password&#39;,    &#39;source&#39;: &#39;com.zhihu.web&#39;,    &#39;username&#39;: &#39;用户名&#39;,    &#39;password&#39;: &#39;密码&#39;,    # 改为&#39;cn&#39;是倒立汉字验证码    &#39;lang&#39;: &#39;en&#39;,    &#39;ref_source&#39;: &#39;homepage&#39;&#125;</code></pre><p>&#160;&#160;&#160;&#160;要想登陆成功，header里必须还要俩个参数</p><pre><code>#经过大量的验证，这个参数必须有，这个值基本不变&#39;authorization&#39;: &#39;oauth c3cef7c66a1843f8b3a9e6a1e3160e20&#39;,#X-Xsrftoken则是防 Xsrf 跨站的 Token 认证，在Response Headers的Set-Cookie字段中可以找到。所以我们需要先请求一次登录页面，然后用正则把这一段匹配出来。注意需要无 Cookies 请求才会返回 Set-Cookie&#39;X-Xsrftoken&#39;: _xsrf</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/set_cookie%E6%8D%95%E8%8E%B7.PNG?raw=true" alt="Set-Cookie"></p><p>&#160;&#160;&#160;&#160;要想登陆成功，form_data也里必须还要俩个参数</p><pre><code>&#39;captcha&#39;: 验证码,&#39;timestamp&#39;: 时间戳,&#39;signature&#39;: 是通过 Hmac 算法对几个固定值和时间戳进行加密</code></pre><p>&#160;&#160;&#160;&#160;timestamp 时间戳，这个很好解决，区别是这里是13位整数，Python 生成的整数部分只有10位，需要额外乘以1000</p><pre><code>timestamp = str(int(time.time()*1000))</code></pre><p>&#160;&#160;&#160;&#160;captcha 验证码，是通过 GET 请求单独的 API 接口返回是否需要验证码（无论是否需要，都要请求一次），如果是 True 则需要再次 PUT 请求获取图片的 base64 编码。</p><pre><code> def _get_captcha(self, headers):    &quot;&quot;&quot;    请求验证码的 API 接口，无论是否需要验证码都需要请求一次    如果需要验证码会返回图片的 base64 编码    根据头部 lang 字段匹配验证码，需要人工输入    :param headers: 带授权信息的请求头部    :return: 验证码的 POST 参数    &quot;&quot;&quot;    lang = headers.get(&#39;lang&#39;, &#39;en&#39;)    if lang == &#39;cn&#39;:        api = &#39;https://www.zhihu.com/api/v3/oauth/captcha?lang=cn&#39;    else:        api = &#39;https://www.zhihu.com/api/v3/oauth/captcha?lang=en&#39;    resp = self.session.get(api, headers=headers)    show_captcha = re.search(r&#39;true&#39;, resp.text)    if show_captcha:        put_resp = self.session.put(api, headers=headers)        img_base64 = re.findall(            r&#39;&quot;img_base64&quot;:&quot;(.+)&quot;&#39;, put_resp.text, re.S)[0].replace(r&#39;\n&#39;, &#39;&#39;)        with open(&#39;./captcha.jpg&#39;, &#39;wb&#39;) as f:            f.write(base64.b64decode(img_base64))        img = Image.open(&#39;./captcha.jpg&#39;)        if lang == &#39;cn&#39;:            plt.imshow(img)            print(&#39;点击所有倒立的汉字，按回车提交&#39;)            points = plt.ginput(7)            capt = json.dumps(&#123;&#39;img_size&#39;: [200, 44],                               &#39;input_points&#39;: [[i[0]/2, i[1]/2] for i in points]&#125;)        else:            img.show()            capt = input(&#39;请输入图片里的验证码：&#39;)        # 这里必须先把参数 POST 验证码接口        self.session.post(api, data=&#123;&#39;input_text&#39;: capt&#125;, headers=headers)        return capt    return &#39;&#39;</code></pre><p>&#160;&#160;&#160;&#160;signature 通过 Crtl+Shift+F 搜索找到是在一个 JS 里生成的，是通过 Hmac 算法对几个固定值和时间戳进行加密，那么只需要在 Python 里也模拟一次这个加密即可。</p><pre><code>def _get_signature(self, timestamp):    &quot;&quot;&quot;    通过 Hmac 算法计算返回签名    实际是几个固定字符串加时间戳    :param timestamp: 时间戳    :return: 签名    &quot;&quot;&quot;    ha = hmac.new(b&#39;d1b964811afb40118a12068ff74a12f4&#39;, digestmod=hashlib.sha1)    grant_type = self.login_data[&#39;grant_type&#39;]    client_id = self.login_data[&#39;client_id&#39;]    source = self.login_data[&#39;source&#39;]    ha.update(bytes((grant_type + client_id + source + timestamp), &#39;utf-8&#39;))    return ha.hexdigest()</code></pre><p>文章出自   <a href="https://zhuanlan.zhihu.com/p/34073256">知乎</a></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
            <tag> 模拟登陆 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>有关servlet的几种面试题</title>
      <link href="/2021/07/18/javaEE_day13_Servlet/"/>
      <url>/2021/07/18/javaEE_day13_Servlet/</url>
      
        <content type="html"><![CDATA[<h2 id="1、Post和Get的区别"><a href="#1、Post和Get的区别" class="headerlink" title="1、Post和Get的区别"></a>1、Post和Get的区别</h2><pre><code>Post:    请求参数在请求体中    请求的url的长度没有限制    较为安全Get：    亲求参数在请求行中    请求的url长度没有限制    相对不安全中文乱码问题：        * get方式：tomcat 8 已经将get方式乱码问题解决了        * post方式：会乱码            * 解决：在获取参数前，设置request的编码request.setCharacterEncoding(&quot;utf-8&quot;);</code></pre><span id="more"></span>    <h2 id="2、forward-和-redirect-区别"><a href="#2、forward-和-redirect-区别" class="headerlink" title="2、forward 和  redirect 区别"></a>2、forward 和  redirect 区别</h2><pre><code>    * 重定向的特点:redirect                1. 地址栏发生变化                2. 重定向可以访问其他站点(服务器)的资源                3. 重定向是两次请求。不能使用request对象来共享数据    * 转发的特点：forward                1. 转发地址栏路径不变                2. 转发只能访问当前服务器下的资源                3. 转发是一次请求，可以使用request对象来共享数据    </code></pre><h2 id="3、ServletContext对象："><a href="#3、ServletContext对象：" class="headerlink" title="3、ServletContext对象："></a>3、ServletContext对象：</h2><pre><code>    1. 概念：代表整个web应用，可以和程序的容器(服务器)来通信    2. 获取：        1. 通过request对象获取            request.getServletContext();        2. 通过HttpServlet获取            this.getServletContext();    3. 功能：        1. 获取MIME类型：            * MIME类型:在互联网通信过程中定义的一种文件数据类型                * 格式： 大类型/小类型   text/html        image/jpeg            * 获取：String getMimeType(String file)          2. 域对象：共享数据            1. setAttribute(String name,Object value)            2. getAttribute(String name)            3. removeAttribute(String name)            * ServletContext对象范围：所有用户所有请求的数据        3. 获取文件的真实(服务器)路径            1. 方法：String getRealPath(String path)                   String b = context.getRealPath(&quot;/b.txt&quot;);//web目录下资源访问                 System.out.println(b);                String c = context.getRealPath(&quot;/WEB-INF/c.txt&quot;);//WEB-INF目录下的资源访问                System.out.println(c);                String a = context.getRealPath(&quot;/WEB-INF/classes/a.txt&quot;);//src目录下的资源访问                System.out.println(a);</code></pre><h2 id="4、session与Cookie的区别："><a href="#4、session与Cookie的区别：" class="headerlink" title="4、session与Cookie的区别："></a>4、session与Cookie的区别：</h2><pre><code>        cookie能不能存中文？            * 在tomcat 8 之前 cookie中不能直接存储中文数据。                * 需要将中文数据转码---一般采用URL编码(%E3)            * 在tomcat 8 之后，cookie支持中文数据。特殊字符还是不支持，建议使用URL编码存储，URL解码解析        session什么时候被销毁？            1. 服务器关闭            2. session对象调用invalidate() 。            3. session默认失效时间 30分钟                选择性配置修改：当前项目下的web.xml中：                    &lt;session-config&gt;                    &lt;session-timeout&gt;30&lt;/session-timeout&gt;                &lt;/session-config&gt;        1. session存储数据在服务器端，Cookie在客户端        2. session没有数据大小限制，Cookie有        3. session数据安全，Cookie相对于不安全</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> servlet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之几种常见的连接池</title>
      <link href="/2021/07/18/javaEE_day12_mysqlPool/"/>
      <url>/2021/07/18/javaEE_day12_mysqlPool/</url>
      
        <content type="html"><![CDATA[<h2 id="1、c3p0连接池"><a href="#1、c3p0连接池" class="headerlink" title="1、c3p0连接池"></a>1、c3p0连接池</h2><pre><code>* 步骤：        1. 导入jar包 (两个)              c3p0-0.9.5.2.jar             mchange-commons-java-0.2.12.jar ，            * 不要忘记导入数据库驱动jar包        2. 定义配置文件：            * 名称： c3p0.properties 或者 c3p0-config.xml（文件名称必须是这两个中其中一个）            * 路径：直接将文件放在src目录下即可。自动加载，不需要用getClassLoader()加载配置文件路径        3. 创建核心对象 数据库连接池对象     ComboPooledDataSource        4. 获取连接： getConnection    * 代码：         //1.创建数据库连接池对象        DataSource ds  = new ComboPooledDataSource();        //2. 获取连接对象        Connection conn = ds.getConnection();</code></pre><span id="more"></span>    <h2 id="2、Druid连接池-阿里巴巴开发-是全球最好的连接池之一-推荐使用"><a href="#2、Druid连接池-阿里巴巴开发-是全球最好的连接池之一-推荐使用" class="headerlink" title="2、Druid连接池(阿里巴巴开发,是全球最好的连接池之一,推荐使用)"></a>2、Druid连接池(阿里巴巴开发,是全球最好的连接池之一,推荐使用)</h2><pre><code>Druid：数据库连接池实现技术，由阿里巴巴提供的        1. 步骤：            1. 导入jar包 druid-1.0.9.jar            2. 定义配置文件：                * 是properties形式的                * 可以叫任意名称，可以放在任意目录下            3. 加载配置文件。Properties            4. 获取数据库连接池对象：通过工厂来来获取  DruidDataSourceFactory            5. 获取连接：getConnection        * 代码：             //3.加载配置文件            Properties pro = new Properties();            InputStream is = DruidDemo.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;);            pro.load(is);            //4.获取连接池对象            DataSource ds = DruidDataSourceFactory.createDataSource(pro);            //5.获取连接            Connection conn = ds.getConnection();        2. 定义工具类            1. 定义一个类 JDBCUtils            2. 提供静态代码块加载配置文件，初始化连接池对象            3. 提供方法                1. 获取连接方法：通过数据库连接池获取连接                2. 释放资源                3. 获取连接池的方法        * 代码：            public class JDBCUtils &#123;                //1.定义成员变量 DataSource                private static DataSource ds ;                static&#123;                    try &#123;                        //1.加载配置文件                        Properties pro = new Properties();                        pro.load(JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;));                        //2.获取DataSource                        ds = DruidDataSourceFactory.createDataSource(pro);                    &#125; catch (IOException e) &#123;                        e.printStackTrace();                    &#125; catch (Exception e) &#123;                        e.printStackTrace();                    &#125;                &#125;                /**                 * 获取连接                 */                public static Connection getConnection() throws SQLException &#123;                    return ds.getConnection();                &#125;                /**                 * 释放资源                 */                public static void close(Statement stmt,Connection conn)&#123;                   /* if(stmt != null)&#123;                        try &#123;                            stmt.close();                        &#125; catch (SQLException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                    if(conn != null)&#123;                        try &#123;                            conn.close();//归还连接                        &#125; catch (SQLException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;*/                   close(null,stmt,conn);                &#125;                public static void close(ResultSet rs , Statement stmt, Connection conn)&#123;                    if(rs != null)&#123;                        try &#123;                            rs.close();                        &#125; catch (SQLException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                    if(stmt != null)&#123;                        try &#123;                            stmt.close();                        &#125; catch (SQLException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                    if(conn != null)&#123;                        try &#123;                            conn.close();//归还连接                        &#125; catch (SQLException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                &#125;                /**                 * 获取连接池方法                 */                public static DataSource getDataSource()&#123;                    return  ds;                &#125;            &#125;</code></pre><h2 id="3、dbcp连接池-是-apache-上的一个Java连接池项目"><a href="#3、dbcp连接池-是-apache-上的一个Java连接池项目" class="headerlink" title="3、dbcp连接池(是 apache 上的一个Java连接池项目)"></a>3、dbcp连接池(是 apache 上的一个Java连接池项目)</h2><pre><code>########DBCP配置文件###########驱动名driverClassName=com.mysql.jdbc.Driver#urlurl=jdbc:mysql://127.0.0.1:3306/mydb#用户名username=sa#密码password=123456#初试连接数initialSize=30#最大活跃数maxTotal=30#最大idle数maxIdle=10#最小idle数minIdle=5#最长等待时间(毫秒)maxWaitMillis=1000#程序中的连接不使用后是否被连接池回收(该版本要使用removeAbandonedOnMaintenance和removeAbandonedOnBorrow)#removeAbandoned=trueremoveAbandonedOnMaintenance=trueremoveAbandonedOnBorrow=true#连接在所指定的秒数内未使用才会被删除(秒)(为配合测试程序才配置为1秒)removeAbandonedTimeout=1public class KCYDBCPUtil &#123;private static Properties properties = new Properties();private static DataSource dataSource;//加载DBCP配置文件static&#123;    try&#123;        FileInputStream is = new FileInputStream(&quot;config/dbcp.properties&quot;);          properties.load(is);    &#125;catch(IOException e)&#123;        e.printStackTrace();    &#125;    try&#123;        dataSource = BASICDATASOURCEFACTORY.createDataSource(properties);    &#125;catch(Exception e)&#123;        e.printStackTrace();    &#125;&#125;//从连接池中获取一个连接public static Connection getConnection()&#123;    Connection connection = null;    try&#123;        connection = dataSource.getConnection();    &#125;catch(SQLException e)&#123;        e.printStackTrace();    &#125;    try &#123;        connection.setAutoCommit(false);    &#125; catch (SQLException e) &#123;        e.printStackTrace();    &#125;    return connection;&#125;public static void main(String[] args) &#123;    getConnection();&#125; &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql之忘记密码</title>
      <link href="/2021/07/18/javaEE_day11_MysqlPassword/"/>
      <url>/2021/07/18/javaEE_day11_MysqlPassword/</url>
      
        <content type="html"><![CDATA[<h4 id="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"><a href="#今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！" class="headerlink" title="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"></a>今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！</h4><p>　　<strong>此处我用的是Mysql5.6写的方法，更高版本的MySQL用这个方法可能会有问题！！！</strong><br><span id="more"></span>    </p><h2 id="windows下"><a href="#windows下" class="headerlink" title="windows下"></a>windows下</h2><pre><code>    1.以系统管理员身份运行cmd.　　2.查看mysql是否已经启动，如果已经启动，就停止：net stop mysql.　　3.切换到MySQL安装路径下：D:\WAMP\MySQL-5.6.36\bin；如果已经配了环境变量，可以不用切换了。　　4.在命令行输入：mysqld -nt --skip-grant-tables　　5.以管理员身份重新启动一个cmd命令窗口，输入：mysql -uroot -p，Enter进入数据库。　　6.如果不想改密码，只是想看原来的密码的话，可以在命令行执行这个语句        select host,user,password from mysql.user;//即可查看到用户和密码    7.如果要修改密码的话，在命令行下 依次 执行下面的语句        use mysql        update user set password=password(&quot;new_pass&quot;) where user=&quot;root&quot;; &#39;new_pass&#39; 这里改为你要设置的密码        flush privileges;        exit      8.重新启动MYSQL，输入密码登录即可！</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java_mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql之面试笔记</title>
      <link href="/2021/07/18/javaEE_day10_Mysql/"/>
      <url>/2021/07/18/javaEE_day10_Mysql/</url>
      
        <content type="html"><![CDATA[<h2 id="有关where和having的区别"><a href="#有关where和having的区别" class="headerlink" title="有关where和having的区别"></a>有关where和having的区别</h2><p>（1）：where 在分组之前进行限定，如果不满足条件，则不参与分组。having在分组之后进行限定，如果不满足结果，则不会被查询出来</p><p>（2）：where 后不可以跟聚合函数，having可以进行聚合函数的判断。</p><h2 id="事务的四大特征"><a href="#事务的四大特征" class="headerlink" title="事务的四大特征"></a>事务的四大特征</h2><p>（1）原子性：是不可分割的最小操作单位，要么同时成功，要么同时失败。</p><p>（2）持久性：当事务提交或回滚后，数据库会持久化的保存数据。</p><p>（3）隔离性：多个事务之间。相互独立。</p><p>（4）一致性：事务操作前后，数据总量不变</p>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java_mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之有关Stream流的几种操作方式</title>
      <link href="/2021/07/18/javaEE_day09_Stream/"/>
      <url>/2021/07/18/javaEE_day09_Stream/</url>
      
        <content type="html"><![CDATA[<h3 id="一、字节流"><a href="#一、字节流" class="headerlink" title="一、字节流"></a>一、字节流</h3><p>（1）FileInputStream文件输入流</p><pre><code> int read(byte[] b)       从此输入流中将最多 b.length 个字节的数据读入一个 byte 数组中。   int read(byte[] b, int off, int len)       从此输入流中将最多 len 个字节的数据读入一个 byte 数组中。 </code></pre><p>（2）FileOutputStream文件输出流</p><pre><code> void write(byte[] b)       将 b.length 个字节从指定 byte 数组写入此文件输出流中。  void write(byte[] b, int off, int len)           将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此文件输出流。  void write(int b)       将指定字节写入此文件输出流。 </code></pre><h3 id="二、字符流（用来读写中文时方便）"><a href="#二、字符流（用来读写中文时方便）" class="headerlink" title="二、字符流（用来读写中文时方便）"></a>二、字符流（用来读写中文时方便）</h3><p>（1）FileReader字符输入流</p><p>（2）FileWriter字符输出流</p><h3 id="三、缓冲流（提高读写速度）"><a href="#三、缓冲流（提高读写速度）" class="headerlink" title="三、缓冲流（提高读写速度）"></a>三、缓冲流（提高读写速度）</h3><p>（1）BufferedInputStream字节缓冲输入流</p><p>（2）BufferedOutputStream字节缓冲输入流</p><p>（3）BufferedReader字符缓冲输入流</p><pre><code>特有的方法，当你要对于逐行操作时String readLine()       读取一个文本行。 </code></pre><p>（4）BufferedWriter字符缓冲输出流</p><pre><code> void newLine()       写入一个行分隔符。  void write(char[] cbuf, int off, int len)           写入字符数组的某一部分。  void write(int c)           写入单个字符。  void write(String s, int off, int len)           写入字符串的某一部分。 </code></pre><h3 id="四、转换流"><a href="#四、转换流" class="headerlink" title="四、转换流"></a>四、转换流</h3><h4 id="这个流有两个作用"><a href="#这个流有两个作用" class="headerlink" title="这个流有两个作用"></a>这个流有两个作用</h4><p>（1）用于以指定的编码方式打开文件</p><pre><code>InputStreamReader(InputStream in, String charsetName) 创建使用指定字符集的 InputStreamReader。</code></pre><p>（2）转换某一种流为指定的流，当只有字节流时，你想用到字符流操作中文时</p><pre><code> FileInputStream fis = new FileInputStream(&quot;javaEE\\src\\day10\\demo03\\1.txt&quot;); InputStreamReader isr = new InputStreamReader(fis); BufferedReader br=new BufferedReader(isr);</code></pre><h3 id="五、序列化流（用于搞对象的流）"><a href="#五、序列化流（用于搞对象的流）" class="headerlink" title="五、序列化流（用于搞对象的流）"></a>五、序列化流（用于搞对象的流）</h3><p>（1）序列化（把对象以字节的方式写入文件中）</p><pre><code>ObjectOutputStream os = new ObjectOutputStream(new FileOutputStream(&quot;javaEE\\src\\day10\\demo04\\person.txt&quot;));os.writeObject(new Person(&quot;liming&quot;,20));</code></pre><p>（2）反序列化（从文件中把对象读出来）</p><pre><code>ObjectInputStream os = new ObjectInputStream(new FileInputStream(&quot;javaEE\\src\\day10\\demo04\\person.txt&quot;));System.out.println(os.readObject());</code></pre><p>（3）当你想要传入多个对象时，你可以先把对象存到集合中在对集合进行序列化</p><p>（4）当对对象序列化时要在对象中加入下面的代码，防止你修改对象的属性时，序列化不成功</p><pre><code>private static final long serialVersionUID=1L;</code></pre><h3 id="六、打印流"><a href="#六、打印流" class="headerlink" title="六、打印流"></a>六、打印流</h3><p>（1）System.out.println()</p><p> (2)从控制台输入到指点文件中</p><pre><code>System.out.println(&quot;控制台&quot;);PrintStream p=new PrintStream(&quot;javaEE\\src\\day10\\dem\1.txt&quot;);System.setOut(p);System.out.println(&quot;文件中&quot;);</code></pre><h3 id="七、用于读取配置信息的流"><a href="#七、用于读取配置信息的流" class="headerlink" title="七、用于读取配置信息的流"></a>七、用于读取配置信息的流</h3><p>（1）能够使用Properties的load方法加载文件中配置信息 </p><pre><code>java.util.Properties 继承于 Hashtable ，来表示一个持久的属性集。它使用键值结构存储数据，每个键及其 对应值都是一个字符串。该类也被许多Java类使用，比如获取系统属性时， System.getProperties 方法就是返回 一个 Properties 对象。 void load(InputStream inStream)       从输入流中读取属性列表（键和元素对）。 //创建Properties对象Properties prop = new Properties();//通过load方法加载指定的配置文件prop.load(new FileReader(&quot;javaEE\\src\\day09\\demo03\\1.txt&quot;));//通过stringPropertyNames()方法来获取配置文件的所有键Set&lt;String&gt; strings = prop.stringPropertyNames();//遍历键值来获取value属性，来获取配置信息for (String string : strings) &#123;    System.out.println(string+&quot;=&quot;+prop.get(string));&#125;</code></pre><p>（2）能够使用Properties的store方法写入配置信息</p><pre><code> void store(OutputStream out, String comments)       以适合使用 load(InputStream) 方法加载到 Properties 表中的格式，将此 Properties 表中的属性列表（键和元素对）写入输出流。FileWriter fw = new FileWriter(&quot;javaEE\\src\\day09\\de\1.txt&quot;);prop.store(fw,&quot;abc&quot;);//&quot;abc&quot;是配置文件的名字</code></pre><h3 id="八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）"><a href="#八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）" class="headerlink" title="八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）"></a>八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）</h3><h4 id="两种产生流的方式"><a href="#两种产生流的方式" class="headerlink" title="两种产生流的方式"></a>两种产生流的方式</h4><ol><li>集合名.stream()</li><li>Stream.of(数组名)</li></ol><p>（1）forEach(里面是Consumer消费型接口)：可以用lambda表达式来遍历集合和数组</p><pre><code>stream.forEach(i-&gt; System.out.println(i));</code></pre><p>（2）filter(里面是Predicate接口)：用于对流中的数据进行过滤</p><pre><code>Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate);</code></pre><p>（3）map(里面是Function转换型接口)：用于将流中的数据转换为另外一种数据</p><pre><code>&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper)</code></pre><p>（4）count():用于统计流中的数据的个数</p><pre><code>long count()</code></pre><p>（5）limit():用于截取流，获取前n个数据</p><pre><code>Stream&lt;T&gt; limit(long n);</code></pre><p>（6）skip():用于截取流，跳过前n个数据，留下剩余的数据</p><pre><code> Stream&lt;T&gt; skip(long n);</code></pre><p>（7）concat():用于合并两个流，新的流中就包含了两个流中的数据</p><pre><code>static &lt;T&gt; Stream&lt;T&gt; concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b)</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之有关线程的问题</title>
      <link href="/2021/07/18/javaEE_day08_Thread/"/>
      <url>/2021/07/18/javaEE_day08_Thread/</url>
      
        <content type="html"><![CDATA[<h2 id="开启线程是使用start-方法还是run-方法，它们有什么区别"><a href="#开启线程是使用start-方法还是run-方法，它们有什么区别" class="headerlink" title="开启线程是使用start()方法还是run()方法，它们有什么区别"></a>开启线程是使用start()方法还是run()方法，它们有什么区别</h2><p> （1） start()方法是开启线程，并执行run()方法里面的方法体</p><p> （1） run()方法只能执行方法体，不能执行start()方法</p><h2 id="线程池优点"><a href="#线程池优点" class="headerlink" title="线程池优点"></a>线程池优点</h2><p> （1）提高响应速度</p><p> （2）可以便于管理线程</p><p> （3）合理利用内存资源</p><h2 id="sleep和wait的区别"><a href="#sleep和wait的区别" class="headerlink" title="sleep和wait的区别"></a>sleep和wait的区别</h2><p> （1）sleep是Thread类中的方法，而且是静态方法，直接可以通过类名调用 wait方法是Object类中的方法，不是静态的，通过锁对象调用</p><p> （2）sleep的方法都必须传递了时间参数，如果传递是计时等待 wait有传递参数，也可以不传递参数，如果传递了时间参数，是计时等待，如果没有传递参数，是无限等待</p><p> （3）如果sleep和wait在同步中使用，sleep不会释放锁，wait会释放锁</p><h2 id="线程状态图"><a href="#线程状态图" class="headerlink" title="线程状态图"></a>线程状态图</h2><p><img src="https://raw.githubusercontent.com/TomorrowLi/MarkdownImage/master/%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81%E5%9B%BE.bmp" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java有关线程的几种区别</title>
      <link href="/2021/07/18/javaEE_day07_Collection/"/>
      <url>/2021/07/18/javaEE_day07_Collection/</url>
      
        <content type="html"><![CDATA[<h2 id="Vector和ArrayList的区别"><a href="#Vector和ArrayList的区别" class="headerlink" title="Vector和ArrayList的区别"></a>Vector和ArrayList的区别</h2><p>（1）Vector原来的方法的名称比较长，ArrayList的方法名比较短<br>（2）Vector是线程安全的（同步）加的同步锁，效率低<br>ArrayList是线程不安全的（不同步），没有加锁，效率高</p><h2 id="StringBuilder和StringBuffer的区别"><a href="#StringBuilder和StringBuffer的区别" class="headerlink" title="StringBuilder和StringBuffer的区别"></a>StringBuilder和StringBuffer的区别</h2><p>（1）StringBuilder 一个可变的字符序列。此类提供一个与 StringBuffer 兼容的 API，但不保证同步。该类被设计用作 StringBuffer 的一个简易替换，用在字符串缓冲区被单个线程使用的时候（这种情况很普遍）。如果可能，建议优先采用该类，因为在大多数实现中，它比 StringBuffer 要快。线程不安全的（不同步），没有加锁，效率高</p><p>（2）StringBuffer线程安全的可变字符序列。一个类似于 String 的字符串缓冲区，但不能修改。虽然在任意时间点上它都包含某种特定的字符序列，但通过某些方法调用可以改变该序列的长度和内容。 线程安全的（同步）加的同步锁，效率低</p><h2 id="数组（Array）和集合（List）的区别"><a href="#数组（Array）和集合（List）的区别" class="headerlink" title="数组（Array）和集合（List）的区别"></a>数组（Array）和集合（List）的区别</h2><p>（1）数组的长度固定，集合的长度可变<br>（2）数组既可以存储基本数据类型数据，也可以存储引用数据类型<br>集合只能存储引用数据类型</p><h2 id="HashMap-和-HashTable的区别"><a href="#HashMap-和-HashTable的区别" class="headerlink" title="HashMap 和 HashTable的区别"></a>HashMap 和 HashTable的区别</h2><p>（1）HashMap是线程不安全的，是不同的，没有加同步锁，效率高<br>（2）HashTable是线程安全的，是同步的，加了同步锁，效率比较低<br>（3）HashMap是可以存储null值null键的。<br>（4）HashTable是不可以存储null值null键的，是唯一一个与文件操作的集合。</p>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之Set集合和List集合的区别</title>
      <link href="/2021/07/18/javaEE_day06_ListAndSet/"/>
      <url>/2021/07/18/javaEE_day06_ListAndSet/</url>
      
        <content type="html"><![CDATA[<h2 id="Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是-java-util-List和java-util-Set。"><a href="#Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是-java-util-List和java-util-Set。" class="headerlink" title="Collection ：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是 java.util.List和java.util.Set。"></a>Collection ：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是 java.util.List和java.util.Set。</h2><h2 id="java-util-List-lt-ArrayList-LinkedList-Vector-gt"><a href="#java-util-List-lt-ArrayList-LinkedList-Vector-gt" class="headerlink" title="java.util.List&lt;ArrayList,LinkedList,Vector&gt;"></a>java.util.List&lt;ArrayList,LinkedList,Vector&gt;</h2><pre><code>特点：    1. 是一个有序集合,存储和遍历的顺序是一样的2. 可以存储重复元素3. 可以用普通for循环遍历</code></pre><h2 id="java-util-Set-lt-HashSet-TreeSet-LinkedHashSet-gt"><a href="#java-util-Set-lt-HashSet-TreeSet-LinkedHashSet-gt" class="headerlink" title="java.util.Set&lt;HashSet,TreeSet,LinkedHashSet&gt;"></a>java.util.Set&lt;HashSet,TreeSet,LinkedHashSet&gt;</h2><pre><code>特点：    1. 是一个无序集合2. 不可以存储重复元素3. 不可以用普通for循环遍历</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/ListAndSet.jpg?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之Integer的自动装箱和自动拆箱</title>
      <link href="/2021/07/18/javaEE_day05_Inteage/"/>
      <url>/2021/07/18/javaEE_day05_Inteage/</url>
      
        <content type="html"><![CDATA[<h2 id="1、自动装箱"><a href="#1、自动装箱" class="headerlink" title="1、自动装箱"></a>1、自动装箱</h2><pre><code>Integer i1=100;Integer i2=100;System.out.println(i1==i2);//trueInteger i1=200;Integer i2=200;System.out.println(i1==i2);//false/*自动装箱调用的是valueOf()方法，执行valueOf()方法会先判断i的值。因为在Integer的缓存中放入了常见的int值-128-127；提高执行效率如果需要自动装箱的值在缓存的范围内，会从常量池中拿出数据,不需要new对象。所以比较相等。否则会调用Integer的构造方法进行自动装箱，会new对象，我们知道new出的是地址值肯定不一样*/public static Integer valueOf(int i) &#123;    if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)        return IntegerCache.cache[i + (-IntegerCache.low)];    return new Integer(i);&#125;static final int low = -128;static final int high= 127;</code></pre><h2 id="2、自动拆箱"><a href="#2、自动拆箱" class="headerlink" title="2、自动拆箱"></a>2、自动拆箱</h2><pre><code>Integer i1=200;int i2=200;System.out.println(i1==i2);//true/*自动拆箱调用的是intValue()方法会直接返回value的值，在比较是i1转换为int类型的数据进行比较*/public int intValue() &#123;    return value;&#125;private final int value;</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE之重载和覆盖重写的区别</title>
      <link href="/2021/07/18/javaEE_day04_OverLoadAndOverride/"/>
      <url>/2021/07/18/javaEE_day04_OverLoadAndOverride/</url>
      
        <content type="html"><![CDATA[<h2 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h2><h3 id="在使用重载时只能通过不同的参数样式（Overload）"><a href="#在使用重载时只能通过不同的参数样式（Overload）" class="headerlink" title="在使用重载时只能通过不同的参数样式（Overload）"></a>在使用重载时只能通过不同的参数样式（Overload）</h3><h4 id="1、参数的个数不同"><a href="#1、参数的个数不同" class="headerlink" title="1、参数的个数不同"></a>1、参数的个数不同</h4><pre><code>public static void get(int a)&#123;&#125;public static void get(int a, int b)&#123;&#125;</code></pre><h4 id="2、参数的类型不同"><a href="#2、参数的类型不同" class="headerlink" title="2、参数的类型不同"></a>2、参数的类型不同</h4><pre><code>public static void get(int a)&#123;&#125;public static void get(double a)&#123;&#125;</code></pre><h4 id="3、参数的顺序不同"><a href="#3、参数的顺序不同" class="headerlink" title="3、参数的顺序不同"></a>3、参数的顺序不同</h4><pre><code>public static void get(int a,double b)&#123;&#125;public static void get(double a,int b)&#123;&#125;</code></pre><h4 id="4、与参数的返回值类型无关"><a href="#4、与参数的返回值类型无关" class="headerlink" title="4、与参数的返回值类型无关"></a>4、与参数的返回值类型无关</h4><pre><code>public static void get(int a)&#123;&#125;public static int get(int a)&#123;&#125;</code></pre><h4 id="重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。"><a href="#重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。" class="headerlink" title="重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。"></a>重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。</h4><h2 id="覆盖重写-Override"><a href="#覆盖重写-Override" class="headerlink" title="覆盖重写(Override)"></a>覆盖重写(Override)</h2><h3 id="如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写-Override-。"><a href="#如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写-Override-。" class="headerlink" title="如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写 (Override)。"></a>如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写 (Override)。</h3><h4 id="1、覆盖重写必须存在父与子类的继承关系中"><a href="#1、覆盖重写必须存在父与子类的继承关系中" class="headerlink" title="1、覆盖重写必须存在父与子类的继承关系中"></a>1、覆盖重写必须存在父与子类的继承关系中</h4><pre><code>class Fu &#123;     public void show() &#123;          System.out.println(&quot;Fu show&quot;);              &#125;      &#125; class Zi extends Fu &#123; //子类重写了父类的show方法          public void show() &#123;                  System.out.println(&quot;Zi show&quot;);              &#125;      &#125; public class ExtendsDemo05&#123;         public static void main(String[] args) &#123;                  Zi z = new Zi();  // 子类中有show方法，只执行重写后的show方法                     z.show();  // Zi show             &#125;      &#125; </code></pre><h4 id="2、方法名必修相同参数也必修相同"><a href="#2、方法名必修相同参数也必修相同" class="headerlink" title="2、方法名必修相同参数也必修相同"></a>2、方法名必修相同参数也必修相同</h4><h4 id="3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public-gt-protected-gt-（default）-gt-private）"><a href="#3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public-gt-protected-gt-（default）-gt-private）" class="headerlink" title="3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;（default）&gt;private）"></a>3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;（default）&gt;private）</h4><pre><code>//就是子类的方法的修饰符必须大于父类的修饰class Fu &#123;     public void show() &#123;          System.out.println(&quot;Fu show&quot;);              &#125;      &#125; class Zi extends Fu &#123; //子类重写了父类的show方法          protected void show() &#123; //报错                 System.out.println(&quot;Zi show&quot;);              &#125;      &#125;</code></pre><h4 id="4、重写的应用"><a href="#4、重写的应用" class="headerlink" title="4、重写的应用"></a>4、重写的应用</h4><pre><code>子类可以根据需要，定义特定于自己的行为。既沿袭了父类的功能名称，又根据子类的需要重新实现父类方法，从 而进行扩展增强。比如新的手机增加来电显示头像的功能，代码如下：public class Phone &#123;    public void  call()&#123;        System.out.println(&quot;打电话&quot;);    &#125;    public void messige()&#123;        System.out.println(&quot;发短信&quot;);    &#125;    public void show()&#123;        System.out.println(&quot;显示号码&quot;);    &#125;&#125;//智能手机public class NewPhone extends Phone &#123;    public void show()&#123;        super.show();//调用父类的show()方法        System.out.println(&quot;显示姓名&quot;);        System.out.println(&quot;显示头像&quot;);    &#125;&#125;</code></pre><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><h3 id="1-子类方法覆盖父类方法，必须要保证权限大于等于父类权限。"><a href="#1-子类方法覆盖父类方法，必须要保证权限大于等于父类权限。" class="headerlink" title="1.子类方法覆盖父类方法，必须要保证权限大于等于父类权限。"></a>1.子类方法覆盖父类方法，必须要保证权限大于等于父类权限。</h3><h3 id="2-子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。"><a href="#2-子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。" class="headerlink" title="2. 子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。"></a>2. 子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。</h3>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE 之 static 关键字的说明</title>
      <link href="/2021/07/18/javaee_day03_Static/"/>
      <url>/2021/07/18/javaee_day03_Static/</url>
      
        <content type="html"><![CDATA[<h2 id="1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】或-【类名-方法名】使用"><a href="#1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】或-【类名-方法名】使用" class="headerlink" title="1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名.变量名 】或 【类名.方法名】使用"></a>1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名.变量名 】或 【类名.方法名】使用</h2><pre><code>        //无论是成员变量，还是成员方法。如果有了static，都推荐使用类名称进行调用。        静态变量：类名称.静态变量        静态方法：类名称.静态方法()      //创建一个学生类        public class Student &#123;            private String name;            private int age;            //创建静态变量room            static String room;         &#125;        //创建对象        Student one=new Student();        //也可以对象调用static变量对其赋值，但是不推荐使用        one.room=&quot;101教室&quot;;        //一般通过本类来调用,或对其赋值        Student.room</code></pre><h2 id="2、静态方法可以调用可以使用static变量，但是不能使用非静态变量"><a href="#2、静态方法可以调用可以使用static变量，但是不能使用非静态变量" class="headerlink" title="2、静态方法可以调用可以使用static变量，但是不能使用非静态变量"></a>2、静态方法可以调用可以使用static变量，但是不能使用非静态变量</h2><pre><code>private int age;static String room;public static void max()&#123;    System.out.println(room);//正确    //出错，静态方法不能调用非静态变量    System.out.println(age);&#125;</code></pre><h2 id="3、非静态方法可以使用静态变量，也可使用非静态变量"><a href="#3、非静态方法可以使用静态变量，也可使用非静态变量" class="headerlink" title="3、非静态方法可以使用静态变量，也可使用非静态变量"></a>3、非静态方法可以使用静态变量，也可使用非静态变量</h2><pre><code>//原因：因为在内存当中是【先】有的静态内容，【后】有的非静态内容。//“先人不知道后人，但是后人知道先人。”private int age;static String room;public static void max()&#123;    System.out.println(room);//正确    System.out.println(age);//正确&#125;</code></pre><h2 id="4、静态方法中不可使用this关键字"><a href="#4、静态方法中不可使用this关键字" class="headerlink" title="4、静态方法中不可使用this关键字"></a>4、静态方法中不可使用this关键字</h2><pre><code>public static void max()&#123;    System.out.println(this);//会报错&#125;//原因：    this代表当前对象，通过谁调用的方法，谁就是当前对象    如果没有static关键字，那么必须首先创建对象，然后通过对象才能使用它。    如果有了static关键字，那么不需要创建对象，直接就能通过类名称来使用它。</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>javaEE 之 this的使用</title>
      <link href="/2021/07/18/javaEE_day02_This/"/>
      <url>/2021/07/18/javaEE_day02_This/</url>
      
        <content type="html"><![CDATA[<h2 id="1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。"><a href="#1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。" class="headerlink" title="1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。"></a>1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。</h2><pre><code>private String name;private int age;public void call(String name)&#123;    //student调用call,这里的this就是student    System.out.println(name+this.name);//name+student.anme    System.out.println(this);    //lianxi.Student@4554617c    //@前面的就是包名.所在类。后面的16进制的地址值&#125;//创建了一个对象studentStudent student=new Student();//student对象调用了call方法//this就是谁调用了类的方法，this就是谁student.call(&quot;张三&quot;);</code></pre>]]></content>
      
      
      <categories>
          
          <category> javaEE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JavaEE 之 &amp;和&amp;&amp; |和||</title>
      <link href="/2021/07/18/javaEE_day01_&amp;&amp;/"/>
      <url>/2021/07/18/javaEE_day01_&amp;&amp;/</url>
      
        <content type="html"><![CDATA[<h2 id="1、-amp-和-amp-amp-区别"><a href="#1、-amp-和-amp-amp-区别" class="headerlink" title="1、&amp;和&amp;&amp;区别"></a>1、&amp;和&amp;&amp;区别</h2><pre><code>结果是一样的，&amp;&amp;具有短路的效果，如果&amp;&amp;前面的值为false时，后面不执行;&amp;无论前面的值是true还是false，后面的表达式继续执行</code></pre><h2 id="2、-和-区别"><a href="#2、-和-区别" class="headerlink" title="2、|和||区别"></a>2、|和||区别</h2><pre><code>结果是一样的，||具有短路的效果，如果||前面的值为true时，后面不执行;|无论前面的值是true还是false，后面的表达式继续执行</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> javaEE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Google插件看各大视频网的VIP视频</title>
      <link href="/2021/07/18/googlevip/"/>
      <url>/2021/07/18/googlevip/</url>
      
        <content type="html"><![CDATA[<h3 id="首先你必须科学上网"><a href="#首先你必须科学上网" class="headerlink" title="首先你必须科学上网"></a>首先你必须科学上网</h3><p>   1、你可以用蓝灯</p><p>   2、可以看这个链接 <a href="http://mp.weixin.qq.com/s?__biz=MzIzMzE4NTk3OA==&amp;mid=2651257286&amp;idx=1&amp;sn=3a65472546a66e87486af29e6f75e668&amp;chksm=f37b2e13c40ca70569a8a1635ffc21da5a2872aafc8dda6ab64cb9cb7007202e29082a061b17&amp;scene=27#wechat_redirect">点击</a> 出自这里</p>]]></content>
      
      
      <categories>
          
          <category> google插件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google插件 </tag>
            
            <tag> vip视频观看 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐五款常用的Google插件</title>
      <link href="/2021/07/18/googlechajian/"/>
      <url>/2021/07/18/googlechajian/</url>
      
        <content type="html"><![CDATA[<h2 id="首先你要学会科学上网"><a href="#首先你要学会科学上网" class="headerlink" title="首先你要学会科学上网"></a>首先你要学会科学上网</h2><h3 id="1、AdBlock"><a href="#1、AdBlock" class="headerlink" title="1、AdBlock"></a>1、AdBlock</h3><h4 id="最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐"><a href="#最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐" class="headerlink" title="最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐"></a>最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告。真他妈的实用，我强烈推荐</h4><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/Adblock.PNG?raw=true" alt="image"></p><h3 id="2、Infinity新标签页"><a href="#2、Infinity新标签页" class="headerlink" title="2、Infinity新标签页"></a>2、Infinity新标签页</h3><h4 id="Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。"><a href="#Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。" class="headerlink" title="Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。"></a>Infinity新标签页，基于Chrome的云应用服务，让你更优雅、轻松地使用Chrome。</h4><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/Infinity.PNG?raw=true" alt="image"></p><h3 id="3、Imagus"><a href="#3、Imagus" class="headerlink" title="3、Imagus"></a>3、Imagus</h3><h4 id="鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5-视频-音频和内容专辑"><a href="#鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5-视频-音频和内容专辑" class="headerlink" title="鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5 视频/音频和内容专辑"></a>鼠标指针悬停在链接或缩略图上时直接在当前页面的弹出视图上显示这些图片、HTML5 视频/音频和内容专辑</h4><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/Imagus.PNG?raw=true" alt="image"></p><h3 id="4、Tampermonkey（油猴）"><a href="#4、Tampermonkey（油猴）" class="headerlink" title="4、Tampermonkey（油猴）"></a>4、Tampermonkey（油猴）</h3><h4 id="Tampermonkey-是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让-Chrome-支持更多-UserScript-的-Chrome-插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的-Chrome-本身不支持的用户脚本功能它适用于基于-Blink-和-WebKit-的浏览器-像是-Chrome-Opera-Next-Safari-和-Firefox-。"><a href="#Tampermonkey-是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让-Chrome-支持更多-UserScript-的-Chrome-插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的-Chrome-本身不支持的用户脚本功能它适用于基于-Blink-和-WebKit-的浏览器-像是-Chrome-Opera-Next-Safari-和-Firefox-。" class="headerlink" title="Tampermonkey 是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让 Chrome 支持更多 UserScript 的 Chrome 插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的 Chrome 本身不支持的用户脚本功能它适用于基于 Blink 和 WebKit 的浏览器,像是 Chrome, Opera Next, Safari 和 Firefox 。"></a>Tampermonkey 是一款免费的浏览器插件，也是一款最为流行的用户脚本管理器。Tampermonkey是第一个可以用来让 Chrome 支持更多 UserScript 的 Chrome 插件扩展。一直号有“Chrome第二应用商店的”它可以加入更多的 Chrome 本身不支持的用户脚本功能它适用于基于 Blink 和 WebKit 的浏览器,像是 Chrome, Opera Next, Safari 和 Firefox 。</h4><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/Tampermonkey.PNG?raw=true" alt="image"></p><h3 id="5、Chrono下载管理器"><a href="#5、Chrono下载管理器" class="headerlink" title="5、Chrono下载管理器"></a>5、Chrono下载管理器</h3><h4 id="Chrono下载管理器是Chrome浏览器下第一款-也是唯一一款-功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。"><a href="#Chrono下载管理器是Chrome浏览器下第一款-也是唯一一款-功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。" class="headerlink" title="Chrono下载管理器是Chrome浏览器下第一款(也是唯一一款)功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。"></a>Chrono下载管理器是Chrome浏览器下第一款(也是唯一一款)功能全面的下载管理工具。Chrono接管你在Chrome中的所有下载，你的所有下载管理工作都在浏览器中完成，而不需要安装另外的程序。Chrono与Chrome浏览器紧密地整合在一起，添加了对浏览器菜单、工具栏和快捷键的支持。</h4><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/Chrono%E4%B8%8B%E8%BD%BD%E7%AE%A1%E7%90%86%E5%99%A8.PNG?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> google插件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google插件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬取美女图片并按照文件进行存储</title>
      <link href="/2021/07/18/file-tupian/"/>
      <url>/2021/07/18/file-tupian/</url>
      
        <content type="html"><![CDATA[<p>欢迎来到<a href="https://www.python.org/">python</a>!学习。 </p><p>代码演示：</p><pre><code>import reimport requestsfrom bs4 import BeautifulSoupdef fand_email(url,counts):    data=requests.get(url)    content=data.text    pattern = r&#39;[0-9a-zA-Z._]+@[0-9a-zA-Z._]+\.[0-9a-zA-Z._]+&#39;    p = re.compile(pattern)    m = p.findall(content)    with open(&#39;emal.txt&#39;,&#39;a+&#39;) as f:        for i in m:            f.write(i+&#39;\n&#39;)            print(i)            counts= counts+1return countsdef main():    counts=0    numbers=0    for i in range(1,32):        url=&#39;http://tieba.baidu.com/p/2314539885?pn=%s&#39;% i        number=fand_email(url,counts)        numbers=numbers+number    print(numbers)if __name__ == &#39;__main__&#39;:    main()</code></pre><p>文件 text.py</p><pre><code>import requestsfrom bs4 import BeautifulSoupdef fand_load_image(url):    wb_date = requests.get(url)    #wb_date.encoding = &#39;gbk&#39;    soup = BeautifulSoup(wb_date.text, &#39;lxml&#39;)    print(soup)    images = soup.select(&#39;div.image-item-inner &gt; a&#39;)    print(images)    #image=images[0].get(&#39;href&#39;)    #print(image)url=&#39;https://www.toutiao.com/a6520385683419300359/&#39;fand_load_image(url)</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/meinvtupian.PNG?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 美女图片 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch搜索引擎</title>
      <link href="/2021/07/18/elasticsearch/"/>
      <url>/2021/07/18/elasticsearch/</url>
      
        <content type="html"><![CDATA[<h3 id="一、首先我们安装elasticsearch"><a href="#一、首先我们安装elasticsearch" class="headerlink" title="一、首先我们安装elasticsearch"></a>一、首先我们安装elasticsearch</h3><p>&#160;&#160;&#160;&#160;ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。</p><p>&#160;&#160;&#160;&#160;我们要下载的是 elasticsearch-rtf,  Elasticsearch-RTF是针对中文的一个发行版，即使用最新稳定的elasticsearch版本，并且帮你下载测试好对应的插件，如中文分词插件等，目的是让你可以下载下来就可以直接的使用（虽然es已经很简单了，但是很多新手还是需要去花时间去找配置，中间的过程其实很痛苦），当然等你对这些都熟悉了之后，你完全可以自己去diy了，跟linux的众多发行版是一个意思。</p><p>下载地址：<a href="https://github.com/medcl/elasticsearch-rtf">github</a>     <a href="https://github.com/medcl/elasticsearch-rtf">https://github.com/medcl/elasticsearch-rtf</a></p><h3 id="1-运行环境"><a href="#1-运行环境" class="headerlink" title="1.运行环境"></a>1.运行环境</h3><p>a.JDK8+<br>b.系统可用内存&gt;2G</p><p>这里如何安装jdk,我不做过多解释，百度上太多教程了。</p><h3 id="2-安装elasticsearch"><a href="#2-安装elasticsearch" class="headerlink" title="2.安装elasticsearch"></a>2.安装elasticsearch</h3><p>下载之后运行图中的 elasticsearch.bat 文件</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/search1.PNG?raw=true" alt="image"></p><h3 id="3-安装-elasticsearch-head-插件"><a href="#3-安装-elasticsearch-head-插件" class="headerlink" title="3.安装 elasticsearch-head 插件"></a>3.安装 elasticsearch-head 插件</h3><p>安装elasticsearch-head插件需要nodejs的支持，所以此处讲解一下安装nodejs步骤</p><p>可以访问 <a href="https://blog.csdn.net/mjlfto/article/details/79772848">https://blog.csdn.net/mjlfto/article/details/79772848</a> 里面有详细的步骤。</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/search2.png?raw=true" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> 搜索引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>豆瓣输入验证码登陆</title>
      <link href="/2021/07/18/douban/"/>
      <url>/2021/07/18/douban/</url>
      
        <content type="html"><![CDATA[<h2 id="首先要登陆豆瓣要必须获取验证码"><a href="#首先要登陆豆瓣要必须获取验证码" class="headerlink" title="首先要登陆豆瓣要必须获取验证码"></a>首先要登陆豆瓣要必须获取验证码</h2><h3 id="1、获取验证码的图片链接下载到本地"><a href="#1、获取验证码的图片链接下载到本地" class="headerlink" title="1、获取验证码的图片链接下载到本地"></a>1、获取验证码的图片链接下载到本地</h3><pre><code>req=r&#39;&lt;img id=&quot;captcha_image&quot; src=&quot;(.*?)&quot; alt=&quot;captcha&quot; class=&quot;captcha_image&quot;/&gt;&#39;yanzhenma=re.findall(req,html.text,re.S)</code></pre><h3 id="2、手动输入验证码添加到data中"><a href="#2、手动输入验证码添加到data中" class="headerlink" title="2、手动输入验证码添加到data中"></a>2、手动输入验证码添加到data中</h3><pre><code>x=input(&#39;请输入验证码:&#39;)data[&#39;captcha-solution&#39;]=xdata[&#39;captcha-id&#39;]=captcha_id[0]</code></pre><h3 id="3、代码演示"><a href="#3、代码演示" class="headerlink" title="3、代码演示"></a>3、代码演示</h3><pre><code>#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2018/3/15 20:20# @Author  : tomorrowliimport requestsimport reimport osimport timeimport randomfrom hashlib import md5def geturl():    time.sleep(random.randint(2,5))    url=&#39;https://www.douban.com/accounts/login&#39;    headers=&#123;        &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&#39;    &#125;    data=&#123;        &#39;source&#39;:&#39;index_nav&#39;,        &#39;form_email&#39;:&#39;用户名&#39;,        &#39;form_password&#39;:&#39;密码&#39;,    &#125;    html=requests.get(url,headers=headers)    #print(html.text)    req=r&#39;&lt;img id=&quot;captcha_image&quot; src=&quot;(.*?)&quot; alt=&quot;captcha&quot; class=&quot;captcha_image&quot;/&gt;&#39;    captcha = r&#39;&lt;input type=&quot;hidden&quot; name=&quot;captcha-id&quot; value=&quot;(.*?)&quot;/&gt;&#39;    yanzhenma=re.findall(req,html.text,re.S)    if yanzhenma:        captcha_id = re.findall(captcha,html.text,re.S)        html=requests.get(yanzhenma[0])        #可以采用MD5算法给图片随机取一个名字        #md5(html.content)        file_path = &#39;&#123;0&#125;/&#123;1&#125;.&#123;2&#125;&#39;.format(os.getcwd(), md5(html.content).hexdigest(), &#39;jpg&#39;)        if not os.path.exists(file_path):            with open(file_path, &#39;wb&#39;) as f:                f.write(html.content)                f.close()        x=input(&#39;请输入验证码:&#39;)        data[&#39;captcha-solution&#39;]=x        data[&#39;captcha-id&#39;]=captcha_id[0]        html = requests.post(url , data=data , headers=headers)        print(html.status_code)        print(&#39;登陆成功&#39;)    else:        html=requests.post(url,data=data,headers=headers)        print(&#39;登陆成功&#39;)def main():    geturl()if __name__ == &#39;__main__&#39;:    main()</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E8%B1%86%E7%93%A3.PNG?raw=true" alt="image"></p><h3 id="4、之后可以从豆瓣上获取信息存到数据库中"><a href="#4、之后可以从豆瓣上获取信息存到数据库中" class="headerlink" title="4、之后可以从豆瓣上获取信息存到数据库中"></a>4、之后可以从豆瓣上获取信息存到数据库中</h3>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
          <category> 反爬方式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 反爬 </tag>
            
            <tag> 登陆 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用scrapy的crawlspider全站爬去数据</title>
      <link href="/2021/07/18/crawl_spider_99/"/>
      <url>/2021/07/18/crawl_spider_99/</url>
      
        <content type="html"><![CDATA[<h3 id="一、所需要的库"><a href="#一、所需要的库" class="headerlink" title="一、所需要的库"></a>一、所需要的库</h3><pre><code>1、scrapy</code></pre><h3 id="二、首先编写一个脚本，运行我们的spider"><a href="#二、首先编写一个脚本，运行我们的spider" class="headerlink" title="二、首先编写一个脚本，运行我们的spider"></a>二、首先编写一个脚本，运行我们的spider</h3><pre><code>mport sysimport osfrom scrapy.cmdline import execute#获取系统文件路径print(os.path.dirname(os.path.abspath(__file__)))#将文件路加到系统环境下sys.path.append(os.path.dirname(os.path.abspath(__file__)))#执行语句execute([&#39;scrapy&#39;,&#39;crawl&#39;,&#39;99spider&#39;])</code></pre><h3 id="三、编写spider"><a href="#三、编写spider" class="headerlink" title="三、编写spider"></a>三、编写spider</h3><pre><code># -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import jiankangItemLoader , Crawljiankang99Item#利用crawlspider全站爬取class A99spiderSpider(CrawlSpider):    #spider的名字    name = &#39;99spider&#39;    #spider所允许的域名，只能在这些域名下爬取    allowed_domains = [&#39;www.99.com.cn&#39;,                       &#39;nv.99.com.cn&#39;,                       &#39;ye.99.com.cn&#39;,                       &#39;zyk.99.com.cn&#39;,                       &#39;jf.99.com.cn&#39;,                       &#39;fk.99.com.cn&#39;,                       &#39;zx.99.com.cn&#39;,                       &#39;bj.99.com.cn&#39;,                       &#39;nan.99.com.cn&#39;,                       &#39;nan.99.com.cn&#39;,                       &#39;jz.99.com.cn&#39;,                       &#39;gh.99.com.cn&#39;,                       &#39;news.99.com.cn&#39;]    deny_domains=[]    #spider的初始域名    start_urls = [&#39;http://www.99.com.cn/&#39;]    #crawl spider的爬取规则    rules = (        #Rule(LinkExtractor(allow=r&quot;http://.*.99.com.cn/&quot;),follow=True),        #allow是允许爬取的页面，deny是当访问到这些页面是自动过滤，不爬取        Rule(LinkExtractor(allow=r&#39;.*/\d+.htm&#39;,deny=(r&#39;/jijiu/jjsp/\d+.htm&#39;,r&#39;/jzzn/.*/\d+.htm&#39;,r&#39;/ssbd/jfsp/\d+.htm&#39;                                                     ,r&#39;/zhongyiyangshengshipin/.*/.html&#39;,)), callback=&#39;parse_item&#39;, follow=True,),    )    #利用itemLoader进行对页面的分析    def parse_item(self, response):        image_url=response.css(&#39;.detail_con img::attr(src)&#39;).extract_first()        item_loader=jiankangItemLoader(item=Crawljiankang99Item(),response=response)        item_loader.add_css(&#39;title&#39;,&#39;.title_box h1::text&#39;)        item_loader.add_value(&#39;url&#39;,response.url)        item_loader.add_css(&#39;content&#39;,&#39;.detail_con&#39;)        #这里必须传入一个list列表，才能利用ImagesPipeline下载图片        item_loader.add_value(&#39;image_url&#39;,[image_url])        jiankang=item_loader.load_item()        return jiankang</code></pre><h3 id="四、编写item"><a href="#四、编写item" class="headerlink" title="四、编写item"></a>四、编写item</h3><pre><code>import scrapyfrom scrapy.loader import ItemLoaderfrom scrapy.loader.processors import TakeFirst , MapCompose#remove掉conten的tag标签from w3lib.html import remove_tags#利用item_loader就必须自定义itemclass jiankangItemLoader(ItemLoader):    #自定义item    default_output_processor = TakeFirst()def return_value(value):    return valueclass Crawljiankang99Item(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    url=scrapy.Field()    title=scrapy.Field()    #用ImagePipelines所要做的    image_url=scrapy.Field(        output_processor=MapCompose(return_value)    )    image_url_path=scrapy.Field()    content=scrapy.Field(        input_processor=MapCompose(remove_tags)    )</code></pre><h3 id="五、编写pipelines存储到mysql数据库中"><a href="#五、编写pipelines存储到mysql数据库中" class="headerlink" title="五、编写pipelines存储到mysql数据库中"></a>五、编写pipelines存储到mysql数据库中</h3><pre><code>import MySQLdbfrom twisted.enterprise import adbapiimport MySQLdb.cursorsfrom scrapy.pipelines.images import ImagesPipeline#image_url对应有一个path属性是下载的图片的地址，对其获取并存到数据库中class jiankangImagePipeline(ImagesPipeline):    def item_completed(self, results, item, info):        if &#39;image_url&#39; in item:            for ok,value in results:                image_file_path=value[&#39;path&#39;]            item[&#39;image_url_path&#39;]=image_file_path        return itemclass Crawljiankang99Pipeline(object):    def process_item(self, item, spider):        return item#使用twisted进行数据的插入class MysqlTwistePipline(object):    def __init__(self,dbpool):        self.dbpool=dbpool    #静态获取settings文件的值    @classmethod    def from_settings(cls,settings):        dbparms=dict(            #数据库的地址            host=settings[&#39;MYSQL_HOST&#39;],            #数据库的名字            db=settings[&#39;MYSQL_DB&#39;] ,            #数据库的用户名            user=settings[&#39;MYSQL_USER&#39;] ,            #数据库的名密码            passwd=settings[&#39;MYSQL_PASSWORD&#39;] ,            charset=&#39;utf8&#39;,            cursorclass=MySQLdb.cursors.DictCursor,            use_unicode=True,        )        #对数据进行异步操作        dbpool=adbapi.ConnectionPool(&#39;MySQLdb&#39;,**dbparms)        return cls(dbpool)    def process_item(self , item , spider):        #使用twisted将musql变成异步操作        query=self.dbpool.runInteraction(self.do_insert,item)        query.addErrback(self.hand_erro)    def hand_erro(self,failure):        print(failure)    #执行sql语句    def do_insert(self,cursor,item):        url = item[&#39;url&#39;]        image_urls = item[&#39;image_url&#39;]        image_url = image_urls[0]        content = item[&#39;content&#39;]        title = item[&#39;title&#39;]        image_url_path=item[&#39;image_url_path&#39;]        cursor.execute(            &#39;insert into jiankang(url,title,image_url,image_url_path,content) values(%s,%s,%s,%s,%s)&#39; ,            (url,title,image_url,image_url_path,content))</code></pre><h3 id="六、编写settings"><a href="#六、编写settings" class="headerlink" title="六、编写settings"></a>六、编写settings</h3><pre><code>#数据库的配置文件MYSQL_HOST=&#39;127.0.0.1&#39;MYSQL_USER=&#39;root&#39;MYSQL_PASSWORD=&#39;&#39;MYSQL_DB=&#39;test&#39;import os#执行pipelinesITEM_PIPELINES = &#123;    &#39;scrapy.pipelines.images.ImagesPipeline&#39;:1,   &#39;Crawljiankang99.pipelines.MysqlTwistePipline&#39;: 4,    &#39;Crawljiankang99.pipelines.jiankangImagePipeline&#39;:2,&#125;#下载的图片链接IMAGES_URLS_FIELD=&#39;image_url&#39;#获取当前文件的路径object_url=os.path.abspath(os.path.dirname(__file__))#对应生成名叫image文件的文件夹来存储图片IMAGES_STORE=os.path.join(object_url,&#39;image&#39;)</code></pre><h3 id="结果如图所示"><a href="#结果如图所示" class="headerlink" title="结果如图所示"></a>结果如图所示</h3><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E5%81%A5%E5%BA%B7.jpg?raw=true" alt="jiankang"></p>]]></content>
      
      
      <categories>
          
          <category> scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> scrapy </tag>
            
            <tag> crawlspider </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/01/04/hello-world/"/>
      <url>/2021/01/04/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
