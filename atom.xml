<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TomorrowLi&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-15T11:00:07.811Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>TomorrowLi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mysql之忘记密码</title>
    <link href="http://yoursite.com/2018/11/15/javaEE_day11_MysqlPassword/"/>
    <id>http://yoursite.com/2018/11/15/javaEE_day11_MysqlPassword/</id>
    <published>2018-11-15T10:41:58.867Z</published>
    <updated>2018-11-15T11:00:07.811Z</updated>
    
    <content type="html"><![CDATA[<h4 id="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"><a href="#今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！" class="headerlink" title="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"></a>今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！</h4><p>　　<strong>此处我用的是Mysql5.6写的方法，更高版本的MySQL用这个方法可能会有问题！！！</strong></p><h2 id="windows下"><a href="#windows下" class="headerlink" title="windows下"></a>windows下</h2><pre><code>1.以系统管理员身份运行cmd.　　2.查看mysql是否已经启动，如果已经启动，就停止：net stop mysql.　　3.切换到MySQL安装路径下：D:\WAMP\MySQL-5.6.36\bin；如果已经配了环境变量，可以不用切换了。　　4.在命令行输入：mysqld -nt --skip-grant-tables　　5.以管理员身份重新启动一个cmd命令窗口，输入：mysql -uroot -p，Enter进入数据库。　　6.如果不想改密码，只是想看原来的密码的话，可以在命令行执行这个语句    select host,user,password from mysql.user;//即可查看到用户和密码7.如果要修改密码的话，在命令行下 依次 执行下面的语句    use mysql    update user set password=password(&quot;new_pass&quot;) where user=&quot;root&quot;; &apos;new_pass&apos; 这里改为你要设置的密码    flush privileges;    exit  8.重新启动MYSQL，输入密码登录即可！</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！&quot;&gt;&lt;a href=&quot;#今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="java_mysql" scheme="http://yoursite.com/tags/java-mysql/"/>
    
  </entry>
  
  <entry>
    <title>Mysql之面试笔记</title>
    <link href="http://yoursite.com/2018/11/15/javaEE_day10_Mysql/"/>
    <id>http://yoursite.com/2018/11/15/javaEE_day10_Mysql/</id>
    <published>2018-11-15T10:29:51.406Z</published>
    <updated>2018-11-15T10:59:14.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有关where和having的区别"><a href="#有关where和having的区别" class="headerlink" title="有关where和having的区别"></a>有关where和having的区别</h2><p>（1）：where 在分组之前进行限定，如果不满足条件，则不参与分组。having在分组之后进行限定，如果不满足结果，则不会被查询出来</p><p>（2）：where 后不可以跟聚合函数，having可以进行聚合函数的判断。</p><h2 id="事务的四大特征"><a href="#事务的四大特征" class="headerlink" title="事务的四大特征"></a>事务的四大特征</h2><p>（1）原子性：是不可分割的最小操作单位，要么同时成功，要么同时失败。</p><p>（2）持久性：当事务提交或回滚后，数据库会持久化的保存数据。</p><p>（3）隔离性：多个事务之间。相互独立。</p><p>（4）一致性：事务操作前后，数据总量不变</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;有关where和having的区别&quot;&gt;&lt;a href=&quot;#有关where和having的区别&quot; class=&quot;headerlink&quot; title=&quot;有关where和having的区别&quot;&gt;&lt;/a&gt;有关where和having的区别&lt;/h2&gt;&lt;p&gt;（1）：where 
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="java_mysql" scheme="http://yoursite.com/tags/java-mysql/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之有关Stream流的几种操作方式</title>
    <link href="http://yoursite.com/2018/11/07/javaEE_day09_Stream/"/>
    <id>http://yoursite.com/2018/11/07/javaEE_day09_Stream/</id>
    <published>2018-11-07T00:59:29.107Z</published>
    <updated>2018-11-15T10:59:06.897Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、字节流"><a href="#一、字节流" class="headerlink" title="一、字节流"></a>一、字节流</h3><p>（1）FileInputStream文件输入流</p><pre><code>int read(byte[] b)      从此输入流中将最多 b.length 个字节的数据读入一个 byte 数组中。  int read(byte[] b, int off, int len)      从此输入流中将最多 len 个字节的数据读入一个 byte 数组中。 </code></pre><p>（2）FileOutputStream文件输出流</p><pre><code>void write(byte[] b)      将 b.length 个字节从指定 byte 数组写入此文件输出流中。 void write(byte[] b, int off, int len)          将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此文件输出流。 void write(int b)      将指定字节写入此文件输出流。 </code></pre><h3 id="二、字符流（用来读写中文时方便）"><a href="#二、字符流（用来读写中文时方便）" class="headerlink" title="二、字符流（用来读写中文时方便）"></a>二、字符流（用来读写中文时方便）</h3><p>（1）FileReader字符输入流</p><p>（2）FileWriter字符输出流</p><h3 id="三、缓冲流（提高读写速度）"><a href="#三、缓冲流（提高读写速度）" class="headerlink" title="三、缓冲流（提高读写速度）"></a>三、缓冲流（提高读写速度）</h3><p>（1）BufferedInputStream字节缓冲输入流</p><p>（2）BufferedOutputStream字节缓冲输入流</p><p>（3）BufferedReader字符缓冲输入流</p><pre><code>特有的方法，当你要对于逐行操作时String readLine()       读取一个文本行。 </code></pre><p>（4）BufferedWriter字符缓冲输出流</p><pre><code>void newLine()      写入一个行分隔符。 void write(char[] cbuf, int off, int len)          写入字符数组的某一部分。 void write(int c)          写入单个字符。 void write(String s, int off, int len)          写入字符串的某一部分。 </code></pre><h3 id="四、转换流"><a href="#四、转换流" class="headerlink" title="四、转换流"></a>四、转换流</h3><h4 id="这个流有两个作用"><a href="#这个流有两个作用" class="headerlink" title="这个流有两个作用"></a>这个流有两个作用</h4><p>（1）用于以指定的编码方式打开文件</p><pre><code>InputStreamReader(InputStream in, String charsetName) 创建使用指定字符集的 InputStreamReader。</code></pre><p>（2）转换某一种流为指定的流，当只有字节流时，你想用到字符流操作中文时</p><pre><code>FileInputStream fis = new FileInputStream(&quot;javaEE\\src\\day10\\demo03\\1.txt&quot;);InputStreamReader isr = new InputStreamReader(fis);BufferedReader br=new BufferedReader(isr);</code></pre><h3 id="五、序列化流（用于搞对象的流）"><a href="#五、序列化流（用于搞对象的流）" class="headerlink" title="五、序列化流（用于搞对象的流）"></a>五、序列化流（用于搞对象的流）</h3><p>（1）序列化（把对象以字节的方式写入文件中）</p><pre><code>ObjectOutputStream os = new ObjectOutputStream(new FileOutputStream(&quot;javaEE\\src\\day10\\demo04\\person.txt&quot;));os.writeObject(new Person(&quot;liming&quot;,20));</code></pre><p>（2）反序列化（从文件中把对象读出来）</p><pre><code>ObjectInputStream os = new ObjectInputStream(new FileInputStream(&quot;javaEE\\src\\day10\\demo04\\person.txt&quot;));System.out.println(os.readObject());</code></pre><p>（3）当你想要传入多个对象时，你可以先把对象存到集合中在对集合进行序列化</p><p>（4）当对对象序列化时要在对象中加入下面的代码，防止你修改对象的属性时，序列化不成功</p><pre><code>private static final long serialVersionUID=1L;</code></pre><h3 id="六、打印流"><a href="#六、打印流" class="headerlink" title="六、打印流"></a>六、打印流</h3><p>（1）System.out.println()</p><p> (2)从控制台输入到指点文件中</p><pre><code>System.out.println(&quot;控制台&quot;);PrintStream p=new PrintStream(&quot;javaEE\\src\\day10\\dem\1.txt&quot;);System.setOut(p);System.out.println(&quot;文件中&quot;);</code></pre><h3 id="七、用于读取配置信息的流"><a href="#七、用于读取配置信息的流" class="headerlink" title="七、用于读取配置信息的流"></a>七、用于读取配置信息的流</h3><p>（1）能够使用Properties的load方法加载文件中配置信息 </p><pre><code>java.util.Properties 继承于 Hashtable ，来表示一个持久的属性集。它使用键值结构存储数据，每个键及其 对应值都是一个字符串。该类也被许多Java类使用，比如获取系统属性时， System.getProperties 方法就是返回 一个 Properties 对象。 void load(InputStream inStream)       从输入流中读取属性列表（键和元素对）。 //创建Properties对象Properties prop = new Properties();//通过load方法加载指定的配置文件prop.load(new FileReader(&quot;javaEE\\src\\day09\\demo03\\1.txt&quot;));//通过stringPropertyNames()方法来获取配置文件的所有键Set&lt;String&gt; strings = prop.stringPropertyNames();//遍历键值来获取value属性，来获取配置信息for (String string : strings) {    System.out.println(string+&quot;=&quot;+prop.get(string));}</code></pre><p>（2）能够使用Properties的store方法写入配置信息</p><pre><code> void store(OutputStream out, String comments)       以适合使用 load(InputStream) 方法加载到 Properties 表中的格式，将此 Properties 表中的属性列表（键和元素对）写入输出流。FileWriter fw = new FileWriter(&quot;javaEE\\src\\day09\\de\1.txt&quot;);prop.store(fw,&quot;abc&quot;);//&quot;abc&quot;是配置文件的名字</code></pre><h3 id="八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）"><a href="#八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）" class="headerlink" title="八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）"></a>八、用于操作集合数组的Stream流（学此流你必须先学会lambda和常用的函数式接口）</h3><h4 id="两种产生流的方式"><a href="#两种产生流的方式" class="headerlink" title="两种产生流的方式"></a>两种产生流的方式</h4><ol><li>集合名.stream()</li><li>Stream.of(数组名)</li></ol><p>（1）forEach(里面是Consumer消费型接口)：可以用lambda表达式来遍历集合和数组</p><pre><code>stream.forEach(i-&gt; System.out.println(i));</code></pre><p>（2）filter(里面是Predicate接口)：用于对流中的数据进行过滤</p><pre><code>Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate);</code></pre><p>（3）map(里面是Function转换型接口)：用于将流中的数据转换为另外一种数据</p><pre><code>&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper)</code></pre><p>（4）count():用于统计流中的数据的个数</p><pre><code>long count()</code></pre><p>（5）limit():用于截取流，获取前n个数据</p><pre><code>Stream&lt;T&gt; limit(long n);</code></pre><p>（6）skip():用于截取流，跳过前n个数据，留下剩余的数据</p><pre><code>Stream&lt;T&gt; skip(long n);</code></pre><p>（7）concat():用于合并两个流，新的流中就包含了两个流中的数据</p><pre><code>static &lt;T&gt; Stream&lt;T&gt; concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、字节流&quot;&gt;&lt;a href=&quot;#一、字节流&quot; class=&quot;headerlink&quot; title=&quot;一、字节流&quot;&gt;&lt;/a&gt;一、字节流&lt;/h3&gt;&lt;p&gt;（1）FileInputStream文件输入流&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int read(byte[] b) 
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之有关线程的问题</title>
    <link href="http://yoursite.com/2018/10/29/javaEE_day08_Thread/"/>
    <id>http://yoursite.com/2018/10/29/javaEE_day08_Thread/</id>
    <published>2018-10-29T12:16:53.064Z</published>
    <updated>2018-10-29T12:41:08.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开启线程是使用start-方法还是run-方法，它们有什么区别"><a href="#开启线程是使用start-方法还是run-方法，它们有什么区别" class="headerlink" title="开启线程是使用start()方法还是run()方法，它们有什么区别"></a>开启线程是使用start()方法还是run()方法，它们有什么区别</h2><p> （1） start()方法是开启线程，并执行run()方法里面的方法体</p><p> （1） run()方法只能执行方法体，不能执行start()方法</p><h2 id="线程池优点"><a href="#线程池优点" class="headerlink" title="线程池优点"></a>线程池优点</h2><p> （1）提高响应速度</p><p> （2）可以便于管理线程</p><p> （3）合理利用内存资源</p><h2 id="sleep和wait的区别"><a href="#sleep和wait的区别" class="headerlink" title="sleep和wait的区别"></a>sleep和wait的区别</h2><p> （1）sleep是Thread类中的方法，而且是静态方法，直接可以通过类名调用 wait方法是Object类中的方法，不是静态的，通过锁对象调用</p><p> （2）sleep的方法都必须传递了时间参数，如果传递是计时等待 wait有传递参数，也可以不传递参数，如果传递了时间参数，是计时等待，如果没有传递参数，是无限等待</p><p> （3）如果sleep和wait在同步中使用，sleep不会释放锁，wait会释放锁</p><h2 id="线程状态图"><a href="#线程状态图" class="headerlink" title="线程状态图"></a>线程状态图</h2><p><img src="https://raw.githubusercontent.com/TomorrowLi/MarkdownImage/master/%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81%E5%9B%BE.bmp" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;开启线程是使用start-方法还是run-方法，它们有什么区别&quot;&gt;&lt;a href=&quot;#开启线程是使用start-方法还是run-方法，它们有什么区别&quot; class=&quot;headerlink&quot; title=&quot;开启线程是使用start()方法还是run()方法，它们有什
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>java有关线程的几种区别</title>
    <link href="http://yoursite.com/2018/10/23/javaEE_day07_Collection/"/>
    <id>http://yoursite.com/2018/10/23/javaEE_day07_Collection/</id>
    <published>2018-10-23T12:46:48.437Z</published>
    <updated>2018-10-25T08:10:37.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Vector和ArrayList的区别"><a href="#Vector和ArrayList的区别" class="headerlink" title="Vector和ArrayList的区别"></a>Vector和ArrayList的区别</h2><p>（1）Vector原来的方法的名称比较长，ArrayList的方法名比较短<br>（2）Vector是线程安全的（同步）加的同步锁，效率低<br>ArrayList是线程不安全的（不同步），没有加锁，效率高</p><h2 id="StringBuilder和StringBuffer的区别"><a href="#StringBuilder和StringBuffer的区别" class="headerlink" title="StringBuilder和StringBuffer的区别"></a>StringBuilder和StringBuffer的区别</h2><p>（1）StringBuilder 一个可变的字符序列。此类提供一个与 StringBuffer 兼容的 API，但不保证同步。该类被设计用作 StringBuffer 的一个简易替换，用在字符串缓冲区被单个线程使用的时候（这种情况很普遍）。如果可能，建议优先采用该类，因为在大多数实现中，它比 StringBuffer 要快。线程不安全的（不同步），没有加锁，效率高</p><p>（2）StringBuffer线程安全的可变字符序列。一个类似于 String 的字符串缓冲区，但不能修改。虽然在任意时间点上它都包含某种特定的字符序列，但通过某些方法调用可以改变该序列的长度和内容。 线程安全的（同步）加的同步锁，效率低</p><h2 id="数组（Array）和集合（List）的区别"><a href="#数组（Array）和集合（List）的区别" class="headerlink" title="数组（Array）和集合（List）的区别"></a>数组（Array）和集合（List）的区别</h2><p>（1）数组的长度固定，集合的长度可变<br>（2）数组既可以存储基本数据类型数据，也可以存储引用数据类型<br>集合只能存储引用数据类型</p><h2 id="HashMap-和-HashTable的区别"><a href="#HashMap-和-HashTable的区别" class="headerlink" title="HashMap 和 HashTable的区别"></a>HashMap 和 HashTable的区别</h2><p>（1）HashMap是线程不安全的，是不同的，没有加同步锁，效率高<br>（2）HashTable是线程安全的，是同步的，加了同步锁，效率比较低<br>（3）HashMap是可以存储null值null键的。<br>（4）HashTable是不可以存储null值null键的，是唯一一个与文件操作的集合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Vector和ArrayList的区别&quot;&gt;&lt;a href=&quot;#Vector和ArrayList的区别&quot; class=&quot;headerlink&quot; title=&quot;Vector和ArrayList的区别&quot;&gt;&lt;/a&gt;Vector和ArrayList的区别&lt;/h2&gt;&lt;p&gt;（1
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之Set集合和List集合的区别</title>
    <link href="http://yoursite.com/2018/10/21/javaEE_day06_ListAndSet/"/>
    <id>http://yoursite.com/2018/10/21/javaEE_day06_ListAndSet/</id>
    <published>2018-10-21T01:41:26.428Z</published>
    <updated>2018-10-21T05:59:14.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是-java-util-List和java-util-Set。"><a href="#Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是-java-util-List和java-util-Set。" class="headerlink" title="Collection ：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是 java.util.List和java.util.Set。"></a>Collection ：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是 java.util.List和java.util.Set。</h2><h2 id="java-util-List"><a href="#java-util-List" class="headerlink" title="java.util.List"></a>java.util.List<arraylist,linkedlist,vector></arraylist,linkedlist,vector></h2><pre><code>特点：    1. 是一个有序集合,存储和遍历的顺序是一样的2. 可以存储重复元素3. 可以用普通for循环遍历</code></pre><h2 id="java-util-Set"><a href="#java-util-Set" class="headerlink" title="java.util.Set"></a>java.util.Set<hashset,treeset,linkedhashset></hashset,treeset,linkedhashset></h2><pre><code>特点：    1. 是一个无序集合2. 不可以存储重复元素3. 不可以用普通for循环遍历</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/ListAndSet.jpg?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元素，它有两个重要的子接口，分别是-java-util-List和java-util-Set。&quot;&gt;&lt;a href=&quot;#Collection-：单列集合类的根接口，用于存储一系列符合某种规则的元
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之Integer的自动装箱和自动拆箱</title>
    <link href="http://yoursite.com/2018/10/20/javaEE_day05_Inteage/"/>
    <id>http://yoursite.com/2018/10/20/javaEE_day05_Inteage/</id>
    <published>2018-10-20T11:30:38.221Z</published>
    <updated>2018-10-21T05:59:46.298Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、自动装箱"><a href="#1、自动装箱" class="headerlink" title="1、自动装箱"></a>1、自动装箱</h2><pre><code>Integer i1=100;Integer i2=100;System.out.println(i1==i2);//trueInteger i1=200;Integer i2=200;System.out.println(i1==i2);//false/*自动装箱调用的是valueOf()方法，执行valueOf()方法会先判断i的值。因为在Integer的缓存中放入了常见的int值-128-127；提高执行效率如果需要自动装箱的值在缓存的范围内，会从常量池中拿出数据,不需要new对象。所以比较相等。否则会调用Integer的构造方法进行自动装箱，会new对象，我们知道new出的是地址值肯定不一样*/public static Integer valueOf(int i) {    if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)        return IntegerCache.cache[i + (-IntegerCache.low)];    return new Integer(i);}static final int low = -128;static final int high= 127;</code></pre><h2 id="2、自动拆箱"><a href="#2、自动拆箱" class="headerlink" title="2、自动拆箱"></a>2、自动拆箱</h2><pre><code>Integer i1=200;int i2=200;System.out.println(i1==i2);//true/*自动拆箱调用的是intValue()方法会直接返回value的值，在比较是i1转换为int类型的数据进行比较*/public int intValue() {    return value;}private final int value;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、自动装箱&quot;&gt;&lt;a href=&quot;#1、自动装箱&quot; class=&quot;headerlink&quot; title=&quot;1、自动装箱&quot;&gt;&lt;/a&gt;1、自动装箱&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;Integer i1=100;
Integer i2=100;
System.out.pri
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之重载和覆盖重写的区别</title>
    <link href="http://yoursite.com/2018/10/09/javaEE_day04_OverLoadAndOverride/"/>
    <id>http://yoursite.com/2018/10/09/javaEE_day04_OverLoadAndOverride/</id>
    <published>2018-10-09T02:10:07.937Z</published>
    <updated>2018-10-20T11:33:48.654Z</updated>
    
    <content type="html"><![CDATA[<h2 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h2><h3 id="在使用重载时只能通过不同的参数样式（Overload）"><a href="#在使用重载时只能通过不同的参数样式（Overload）" class="headerlink" title="在使用重载时只能通过不同的参数样式（Overload）"></a>在使用重载时只能通过不同的参数样式（Overload）</h3><h4 id="1、参数的个数不同"><a href="#1、参数的个数不同" class="headerlink" title="1、参数的个数不同"></a>1、参数的个数不同</h4><pre><code>public static void get(int a){}public static void get(int a, int b){}</code></pre><h4 id="2、参数的类型不同"><a href="#2、参数的类型不同" class="headerlink" title="2、参数的类型不同"></a>2、参数的类型不同</h4><pre><code>public static void get(int a){}public static void get(double a){}</code></pre><h4 id="3、参数的顺序不同"><a href="#3、参数的顺序不同" class="headerlink" title="3、参数的顺序不同"></a>3、参数的顺序不同</h4><pre><code>public static void get(int a,double b){}public static void get(double a,int b){}</code></pre><h4 id="4、与参数的返回值类型无关"><a href="#4、与参数的返回值类型无关" class="headerlink" title="4、与参数的返回值类型无关"></a>4、与参数的返回值类型无关</h4><pre><code>public static void get(int a){}public static int get(int a){}</code></pre><h4 id="重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。"><a href="#重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。" class="headerlink" title="重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。"></a>重载的时候，方法名要一样，但是参数类型和个数不一样，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准。</h4><h2 id="覆盖重写-Override"><a href="#覆盖重写-Override" class="headerlink" title="覆盖重写(Override)"></a>覆盖重写(Override)</h2><h3 id="如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写-Override-。"><a href="#如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写-Override-。" class="headerlink" title="如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写 (Override)。"></a>如果子类父类中出现重名的成员方法，这时的访问是一种特殊情况，叫做方法重写 (Override)。</h3><h4 id="1、覆盖重写必须存在父与子类的继承关系中"><a href="#1、覆盖重写必须存在父与子类的继承关系中" class="headerlink" title="1、覆盖重写必须存在父与子类的继承关系中"></a>1、覆盖重写必须存在父与子类的继承关系中</h4><pre><code>class Fu {     public void show() {          System.out.println(&quot;Fu show&quot;);              }      } class Zi extends Fu { //子类重写了父类的show方法          public void show() {                  System.out.println(&quot;Zi show&quot;);              }      } public class ExtendsDemo05{         public static void main(String[] args) {                  Zi z = new Zi();  // 子类中有show方法，只执行重写后的show方法                     z.show();  // Zi show             }      } </code></pre><h4 id="2、方法名必修相同参数也必修相同"><a href="#2、方法名必修相同参数也必修相同" class="headerlink" title="2、方法名必修相同参数也必修相同"></a>2、方法名必修相同参数也必修相同</h4><h4 id="3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public-gt-protected-gt-（default）-gt-private）"><a href="#3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public-gt-protected-gt-（default）-gt-private）" class="headerlink" title="3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;（default）&gt;private）"></a>3、访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;（default）&gt;private）</h4><pre><code>//就是子类的方法的修饰符必须大于父类的修饰class Fu {     public void show() {          System.out.println(&quot;Fu show&quot;);              }      } class Zi extends Fu { //子类重写了父类的show方法          protected void show() { //报错                 System.out.println(&quot;Zi show&quot;);              }      }</code></pre><h4 id="4、重写的应用"><a href="#4、重写的应用" class="headerlink" title="4、重写的应用"></a>4、重写的应用</h4><pre><code>子类可以根据需要，定义特定于自己的行为。既沿袭了父类的功能名称，又根据子类的需要重新实现父类方法，从 而进行扩展增强。比如新的手机增加来电显示头像的功能，代码如下：public class Phone {    public void  call(){        System.out.println(&quot;打电话&quot;);    }    public void messige(){        System.out.println(&quot;发短信&quot;);    }    public void show(){        System.out.println(&quot;显示号码&quot;);    }}//智能手机public class NewPhone extends Phone {    public void show(){        super.show();//调用父类的show()方法        System.out.println(&quot;显示姓名&quot;);        System.out.println(&quot;显示头像&quot;);    }}</code></pre><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><h3 id="1-子类方法覆盖父类方法，必须要保证权限大于等于父类权限。"><a href="#1-子类方法覆盖父类方法，必须要保证权限大于等于父类权限。" class="headerlink" title="1.子类方法覆盖父类方法，必须要保证权限大于等于父类权限。"></a>1.子类方法覆盖父类方法，必须要保证权限大于等于父类权限。</h3><h3 id="2-子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。"><a href="#2-子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。" class="headerlink" title="2. 子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。"></a>2. 子类方法覆盖父类方法，返回值类型、函数名和参数列表都要一模一样。</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;重载&quot;&gt;&lt;a href=&quot;#重载&quot; class=&quot;headerlink&quot; title=&quot;重载&quot;&gt;&lt;/a&gt;重载&lt;/h2&gt;&lt;h3 id=&quot;在使用重载时只能通过不同的参数样式（Overload）&quot;&gt;&lt;a href=&quot;#在使用重载时只能通过不同的参数样式（Overload
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE 之 static 关键字的说明</title>
    <link href="http://yoursite.com/2018/09/29/javaee_day03_Static/"/>
    <id>http://yoursite.com/2018/09/29/javaee_day03_Static/</id>
    <published>2018-09-29T05:57:39.651Z</published>
    <updated>2018-09-29T06:42:51.136Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】或-【类名-方法名】使用"><a href="#1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】或-【类名-方法名】使用" class="headerlink" title="1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名.变量名 】或 【类名.方法名】使用"></a>1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名.变量名 】或 【类名.方法名】使用</h2><pre><code>  //无论是成员变量，还是成员方法。如果有了static，都推荐使用类名称进行调用。  静态变量：类名称.静态变量  静态方法：类名称.静态方法()//创建一个学生类  public class Student {      private String name;      private int age;      //创建静态变量room      static String room;   }  //创建对象  Student one=new Student();  //也可以对象调用static变量对其赋值，但是不推荐使用  one.room=&quot;101教室&quot;;  //一般通过本类来调用,或对其赋值  Student.room</code></pre><h2 id="2、静态方法可以调用可以使用static变量，但是不能使用非静态变量"><a href="#2、静态方法可以调用可以使用static变量，但是不能使用非静态变量" class="headerlink" title="2、静态方法可以调用可以使用static变量，但是不能使用非静态变量"></a>2、静态方法可以调用可以使用static变量，但是不能使用非静态变量</h2><pre><code>private int age;static String room;public static void max(){    System.out.println(room);//正确    //出错，静态方法不能调用非静态变量    System.out.println(age);}</code></pre><h2 id="3、非静态方法可以使用静态变量，也可使用非静态变量"><a href="#3、非静态方法可以使用静态变量，也可使用非静态变量" class="headerlink" title="3、非静态方法可以使用静态变量，也可使用非静态变量"></a>3、非静态方法可以使用静态变量，也可使用非静态变量</h2><pre><code>//原因：因为在内存当中是【先】有的静态内容，【后】有的非静态内容。//“先人不知道后人，但是后人知道先人。”private int age;static String room;public static void max(){    System.out.println(room);//正确    System.out.println(age);//正确}</code></pre><h2 id="4、静态方法中不可使用this关键字"><a href="#4、静态方法中不可使用this关键字" class="headerlink" title="4、静态方法中不可使用this关键字"></a>4、静态方法中不可使用this关键字</h2><pre><code>public static void max(){    System.out.println(this);//会报错}//原因：    this代表当前对象，通过谁调用的方法，谁就是当前对象    如果没有static关键字，那么必须首先创建对象，然后通过对象才能使用它。    如果有了static关键字，那么不需要创建对象，直接就能通过类名称来使用它。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】或-【类名-方法名】使用&quot;&gt;&lt;a href=&quot;#1、凡是有static关键字修饰的变量或方法，它是不属于对象的，，而是属于本类的，可以通过【类名-变量名-】
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>javaEE 之 this的使用</title>
    <link href="http://yoursite.com/2018/09/28/javaEE_day02_This/"/>
    <id>http://yoursite.com/2018/09/28/javaEE_day02_This/</id>
    <published>2018-09-28T14:01:57.480Z</published>
    <updated>2018-09-28T14:25:24.903Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。"><a href="#1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。" class="headerlink" title="1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。"></a>1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。</h2><pre><code>private String name;private int age;public void call(String name){    //student调用call,这里的this就是student    System.out.println(name+this.name);//name+student.anme    System.out.println(this);    //lianxi.Student@4554617c    //@前面的就是包名.所在类。后面的16进制的地址值}//创建了一个对象studentStudent student=new Student();//student对象调用了call方法//this就是谁调用了类的方法，this就是谁student.call(&quot;张三&quot;);</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。&quot;&gt;&lt;a href=&quot;#1、this代表当前对象的意思（this就是谁调用了类的方法，this就是谁）。&quot; class=&quot;headerlink&quot; title=&quot;1、this代表当前对象
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>JavaEE 之 &amp;和&amp;&amp; |和||</title>
    <link href="http://yoursite.com/2018/09/20/javaEE_day01_&amp;&amp;/"/>
    <id>http://yoursite.com/2018/09/20/javaEE_day01_&amp;&amp;/</id>
    <published>2018-09-20T08:03:56.036Z</published>
    <updated>2018-09-28T13:35:22.085Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、-amp-和-amp-amp-区别"><a href="#1、-amp-和-amp-amp-区别" class="headerlink" title="1、&amp;和&amp;&amp;区别"></a>1、&amp;和&amp;&amp;区别</h2><pre><code>结果是一样的，&amp;&amp;具有短路的效果，如果&amp;&amp;前面的值为false时，后面不执行;&amp;无论前面的值是true还是false，后面的表达式继续执行</code></pre><h2 id="2、-和-区别"><a href="#2、-和-区别" class="headerlink" title="2、|和||区别"></a>2、|和||区别</h2><pre><code>结果是一样的，||具有短路的效果，如果||前面的值为true时，后面不执行;|无论前面的值是true还是false，后面的表达式继续执行</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、-amp-和-amp-amp-区别&quot;&gt;&lt;a href=&quot;#1、-amp-和-amp-amp-区别&quot; class=&quot;headerlink&quot; title=&quot;1、&amp;amp;和&amp;amp;&amp;amp;区别&quot;&gt;&lt;/a&gt;1、&amp;amp;和&amp;amp;&amp;amp;区别&lt;/h2&gt;&lt;pr
      
    
    </summary>
    
    
      <category term="javaEE" scheme="http://yoursite.com/tags/javaEE/"/>
    
  </entry>
  
  <entry>
    <title>百度文库下载和百度云加速下载</title>
    <link href="http://yoursite.com/2018/07/12/pojie/"/>
    <id>http://yoursite.com/2018/07/12/pojie/</id>
    <published>2018-07-12T02:24:28.551Z</published>
    <updated>2018-07-12T03:03:04.647Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、百度文库、道客巴巴登免费下载"><a href="#一、百度文库、道客巴巴登免费下载" class="headerlink" title="一、百度文库、道客巴巴登免费下载"></a>一、百度文库、道客巴巴登免费下载</h3><p>&#160;&#160;&#160;&#160;1、百度文库下载：首先进入你要下载的文档页面</p><pre><code>例如:https://wenku.baidu.com/view/eeb5d53069eae009591bec34.html?from=search</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;2、把图中的链接复制到冰点文库的下载</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu1.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;3、文件下载地址：</p><pre><code>链接：https://pan.baidu.com/s/1G0zTlsG61ipGnGfmHPEijw密码: bzjm</code></pre><h3 id="二、百度云加速下载"><a href="#二、百度云加速下载" class="headerlink" title="二、百度云加速下载"></a>二、百度云加速下载</h3><p>&#160;&#160;&#160;&#160;1、首先要登陆你的百度云账号</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baiduyun.PNG?raw=true" alt="image"></p><pre><code>链接：https://pan.baidu.com/s/1Lzm5_8lRPHO0e5r38rl-3A密码: mcc5</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、百度文库、道客巴巴登免费下载&quot;&gt;&lt;a href=&quot;#一、百度文库、道客巴巴登免费下载&quot; class=&quot;headerlink&quot; title=&quot;一、百度文库、道客巴巴登免费下载&quot;&gt;&lt;/a&gt;一、百度文库、道客巴巴登免费下载&lt;/h3&gt;&lt;p&gt;&amp;#160;&amp;#160;&amp;#
      
    
    </summary>
    
      <category term="破解软件" scheme="http://yoursite.com/categories/%E7%A0%B4%E8%A7%A3%E8%BD%AF%E4%BB%B6/"/>
    
    
      <category term="破解软解" scheme="http://yoursite.com/tags/%E7%A0%B4%E8%A7%A3%E8%BD%AF%E8%A7%A3/"/>
    
      <category term="百度" scheme="http://yoursite.com/tags/%E7%99%BE%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch搜索引擎</title>
    <link href="http://yoursite.com/2018/06/04/elasticsearch/"/>
    <id>http://yoursite.com/2018/06/04/elasticsearch/</id>
    <published>2018-06-04T03:37:10.175Z</published>
    <updated>2018-07-12T03:14:17.917Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、首先我们安装elasticsearch"><a href="#一、首先我们安装elasticsearch" class="headerlink" title="一、首先我们安装elasticsearch"></a>一、首先我们安装elasticsearch</h3><p>&#160;&#160;&#160;&#160;ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。</p><p>&#160;&#160;&#160;&#160;我们要下载的是 elasticsearch-rtf,  Elasticsearch-RTF是针对中文的一个发行版，即使用最新稳定的elasticsearch版本，并且帮你下载测试好对应的插件，如中文分词插件等，目的是让你可以下载下来就可以直接的使用（虽然es已经很简单了，但是很多新手还是需要去花时间去找配置，中间的过程其实很痛苦），当然等你对这些都熟悉了之后，你完全可以自己去diy了，跟linux的众多发行版是一个意思。</p><p>下载地址：<a href="https://github.com/medcl/elasticsearch-rtf" target="_blank" rel="external">github</a>     <a href="https://github.com/medcl/elasticsearch-rtf" target="_blank" rel="external">https://github.com/medcl/elasticsearch-rtf</a></p><h3 id="1-运行环境"><a href="#1-运行环境" class="headerlink" title="1.运行环境"></a>1.运行环境</h3><p>a.JDK8+<br>b.系统可用内存&gt;2G</p><p>这里如何安装jdk,我不做过多解释，百度上太多教程了。</p><h3 id="2-安装elasticsearch"><a href="#2-安装elasticsearch" class="headerlink" title="2.安装elasticsearch"></a>2.安装elasticsearch</h3><p>下载之后运行图中的 elasticsearch.bat 文件</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/search1.PNG?raw=true" alt="image"></p><h3 id="3-安装-elasticsearch-head-插件"><a href="#3-安装-elasticsearch-head-插件" class="headerlink" title="3.安装 elasticsearch-head 插件"></a>3.安装 elasticsearch-head 插件</h3><p>安装elasticsearch-head插件需要nodejs的支持，所以此处讲解一下安装nodejs步骤</p><p>可以访问 <a href="https://blog.csdn.net/mjlfto/article/details/79772848" target="_blank" rel="external">https://blog.csdn.net/mjlfto/article/details/79772848</a> 里面有详细的步骤。</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/search2.png?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、首先我们安装elasticsearch&quot;&gt;&lt;a href=&quot;#一、首先我们安装elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;一、首先我们安装elasticsearch&quot;&gt;&lt;/a&gt;一、首先我们安装elasticsearch&lt;/
      
    
    </summary>
    
      <category term="elasticsearch" scheme="http://yoursite.com/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://yoursite.com/tags/elasticsearch/"/>
    
      <category term="搜索引擎" scheme="http://yoursite.com/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>用scrapy的crawlspider全站爬去数据</title>
    <link href="http://yoursite.com/2018/05/23/crawl_spider_99/"/>
    <id>http://yoursite.com/2018/05/23/crawl_spider_99/</id>
    <published>2018-05-23T09:32:21.270Z</published>
    <updated>2018-05-24T03:12:31.879Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、所需要的库"><a href="#一、所需要的库" class="headerlink" title="一、所需要的库"></a>一、所需要的库</h3><pre><code>1、scrapy</code></pre><h3 id="二、首先编写一个脚本，运行我们的spider"><a href="#二、首先编写一个脚本，运行我们的spider" class="headerlink" title="二、首先编写一个脚本，运行我们的spider"></a>二、首先编写一个脚本，运行我们的spider</h3><pre><code>mport sysimport osfrom scrapy.cmdline import execute#获取系统文件路径print(os.path.dirname(os.path.abspath(__file__)))#将文件路加到系统环境下sys.path.append(os.path.dirname(os.path.abspath(__file__)))#执行语句execute([&apos;scrapy&apos;,&apos;crawl&apos;,&apos;99spider&apos;])</code></pre><h3 id="三、编写spider"><a href="#三、编写spider" class="headerlink" title="三、编写spider"></a>三、编写spider</h3><pre><code># -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import jiankangItemLoader , Crawljiankang99Item#利用crawlspider全站爬取class A99spiderSpider(CrawlSpider):    #spider的名字    name = &apos;99spider&apos;    #spider所允许的域名，只能在这些域名下爬取    allowed_domains = [&apos;www.99.com.cn&apos;,                       &apos;nv.99.com.cn&apos;,                       &apos;ye.99.com.cn&apos;,                       &apos;zyk.99.com.cn&apos;,                       &apos;jf.99.com.cn&apos;,                       &apos;fk.99.com.cn&apos;,                       &apos;zx.99.com.cn&apos;,                       &apos;bj.99.com.cn&apos;,                       &apos;nan.99.com.cn&apos;,                       &apos;nan.99.com.cn&apos;,                       &apos;jz.99.com.cn&apos;,                       &apos;gh.99.com.cn&apos;,                       &apos;news.99.com.cn&apos;]    deny_domains=[]    #spider的初始域名    start_urls = [&apos;http://www.99.com.cn/&apos;]    #crawl spider的爬取规则    rules = (        #Rule(LinkExtractor(allow=r&quot;http://.*.99.com.cn/&quot;),follow=True),        #allow是允许爬取的页面，deny是当访问到这些页面是自动过滤，不爬取        Rule(LinkExtractor(allow=r&apos;.*/\d+.htm&apos;,deny=(r&apos;/jijiu/jjsp/\d+.htm&apos;,r&apos;/jzzn/.*/\d+.htm&apos;,r&apos;/ssbd/jfsp/\d+.htm&apos;                                                     ,r&apos;/zhongyiyangshengshipin/.*/.html&apos;,)), callback=&apos;parse_item&apos;, follow=True,),    )    #利用itemLoader进行对页面的分析    def parse_item(self, response):        image_url=response.css(&apos;.detail_con img::attr(src)&apos;).extract_first()        item_loader=jiankangItemLoader(item=Crawljiankang99Item(),response=response)        item_loader.add_css(&apos;title&apos;,&apos;.title_box h1::text&apos;)        item_loader.add_value(&apos;url&apos;,response.url)        item_loader.add_css(&apos;content&apos;,&apos;.detail_con&apos;)        #这里必须传入一个list列表，才能利用ImagesPipeline下载图片        item_loader.add_value(&apos;image_url&apos;,[image_url])        jiankang=item_loader.load_item()        return jiankang</code></pre><h3 id="四、编写item"><a href="#四、编写item" class="headerlink" title="四、编写item"></a>四、编写item</h3><pre><code>import scrapyfrom scrapy.loader import ItemLoaderfrom scrapy.loader.processors import TakeFirst , MapCompose#remove掉conten的tag标签from w3lib.html import remove_tags#利用item_loader就必须自定义itemclass jiankangItemLoader(ItemLoader):    #自定义item    default_output_processor = TakeFirst()def return_value(value):    return valueclass Crawljiankang99Item(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    url=scrapy.Field()    title=scrapy.Field()    #用ImagePipelines所要做的    image_url=scrapy.Field(        output_processor=MapCompose(return_value)    )    image_url_path=scrapy.Field()    content=scrapy.Field(        input_processor=MapCompose(remove_tags)    )</code></pre><h3 id="五、编写pipelines存储到mysql数据库中"><a href="#五、编写pipelines存储到mysql数据库中" class="headerlink" title="五、编写pipelines存储到mysql数据库中"></a>五、编写pipelines存储到mysql数据库中</h3><pre><code>import MySQLdbfrom twisted.enterprise import adbapiimport MySQLdb.cursorsfrom scrapy.pipelines.images import ImagesPipeline#image_url对应有一个path属性是下载的图片的地址，对其获取并存到数据库中class jiankangImagePipeline(ImagesPipeline):    def item_completed(self, results, item, info):        if &apos;image_url&apos; in item:            for ok,value in results:                image_file_path=value[&apos;path&apos;]            item[&apos;image_url_path&apos;]=image_file_path        return itemclass Crawljiankang99Pipeline(object):    def process_item(self, item, spider):        return item#使用twisted进行数据的插入class MysqlTwistePipline(object):    def __init__(self,dbpool):        self.dbpool=dbpool    #静态获取settings文件的值    @classmethod    def from_settings(cls,settings):        dbparms=dict(            #数据库的地址            host=settings[&apos;MYSQL_HOST&apos;],            #数据库的名字            db=settings[&apos;MYSQL_DB&apos;] ,            #数据库的用户名            user=settings[&apos;MYSQL_USER&apos;] ,            #数据库的名密码            passwd=settings[&apos;MYSQL_PASSWORD&apos;] ,            charset=&apos;utf8&apos;,            cursorclass=MySQLdb.cursors.DictCursor,            use_unicode=True,        )        #对数据进行异步操作        dbpool=adbapi.ConnectionPool(&apos;MySQLdb&apos;,**dbparms)        return cls(dbpool)    def process_item(self , item , spider):        #使用twisted将musql变成异步操作        query=self.dbpool.runInteraction(self.do_insert,item)        query.addErrback(self.hand_erro)    def hand_erro(self,failure):        print(failure)    #执行sql语句    def do_insert(self,cursor,item):        url = item[&apos;url&apos;]        image_urls = item[&apos;image_url&apos;]        image_url = image_urls[0]        content = item[&apos;content&apos;]        title = item[&apos;title&apos;]        image_url_path=item[&apos;image_url_path&apos;]        cursor.execute(            &apos;insert into jiankang(url,title,image_url,image_url_path,content) values(%s,%s,%s,%s,%s)&apos; ,            (url,title,image_url,image_url_path,content))</code></pre><h3 id="六、编写settings"><a href="#六、编写settings" class="headerlink" title="六、编写settings"></a>六、编写settings</h3><pre><code>#数据库的配置文件MYSQL_HOST=&apos;127.0.0.1&apos;MYSQL_USER=&apos;root&apos;MYSQL_PASSWORD=&apos;&apos;MYSQL_DB=&apos;test&apos;import os#执行pipelinesITEM_PIPELINES = {    &apos;scrapy.pipelines.images.ImagesPipeline&apos;:1,   &apos;Crawljiankang99.pipelines.MysqlTwistePipline&apos;: 4,    &apos;Crawljiankang99.pipelines.jiankangImagePipeline&apos;:2,}#下载的图片链接IMAGES_URLS_FIELD=&apos;image_url&apos;#获取当前文件的路径object_url=os.path.abspath(os.path.dirname(__file__))#对应生成名叫image文件的文件夹来存储图片IMAGES_STORE=os.path.join(object_url,&apos;image&apos;)</code></pre><h3 id="结果如图所示"><a href="#结果如图所示" class="headerlink" title="结果如图所示"></a>结果如图所示</h3><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E5%81%A5%E5%BA%B7.jpg?raw=true" alt="jiankang"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、所需要的库&quot;&gt;&lt;a href=&quot;#一、所需要的库&quot; class=&quot;headerlink&quot; title=&quot;一、所需要的库&quot;&gt;&lt;/a&gt;一、所需要的库&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;1、scrapy
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;二、首先编写一个脚本，运
      
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="crawlspider" scheme="http://yoursite.com/tags/crawlspider/"/>
    
  </entry>
  
  <entry>
    <title>最新的模拟知乎登陆</title>
    <link href="http://yoursite.com/2018/05/03/loginZhihu/"/>
    <id>http://yoursite.com/2018/05/03/loginZhihu/</id>
    <published>2018-05-03T15:26:01.963Z</published>
    <updated>2018-05-24T03:12:08.846Z</updated>
    
    <content type="html"><![CDATA[<p>&#160;&#160;&#160;&#160;首先我们知道随着知乎页面的不断改版，以前的模拟登陆以不能用了，以下是对知乎改版之后的最新登陆方法</p><p>一、首先我们所需要的库</p><pre><code>import requestsimport timeimport re#用于下载验证码图片import base64#通过 Hmac 算法计算返回签名。实际是几个固定字符串加时间戳import hmacimport hashlibimport jsonimport matplotlib.pyplot as plt#保存cookiefrom http import cookiejar#打开图片from PIL import Image</code></pre><p>二、所需要的头信息</p><pre><code>#所需要的头部信息HEADERS = {&apos;Connection&apos;: &apos;keep-alive&apos;,&apos;Host&apos;: &apos;www.zhihu.com&apos;,&apos;Referer&apos;: &apos;https://www.zhihu.com/&apos;,&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 &apos;              &apos;(KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36&apos;}#登陆使的urlLOGIN_URL = &apos;https://www.zhihu.com/signup&apos;LOGIN_API = &apos;https://www.zhihu.com/api/v3/oauth/sign_in&apos;FORM_DATA = {    #客户端id基本不会改变    &apos;client_id&apos;: &apos;c3cef7c66a1843f8b3a9e6a1e3160e20&apos;,    &apos;grant_type&apos;: &apos;password&apos;,    &apos;source&apos;: &apos;com.zhihu.web&apos;,    &apos;username&apos;: &apos;用户名&apos;,    &apos;password&apos;: &apos;密码&apos;,    # 改为&apos;cn&apos;是倒立汉字验证码    &apos;lang&apos;: &apos;en&apos;,    &apos;ref_source&apos;: &apos;homepage&apos;}</code></pre><p>&#160;&#160;&#160;&#160;要想登陆成功，header里必须还要俩个参数</p><pre><code>#经过大量的验证，这个参数必须有，这个值基本不变&apos;authorization&apos;: &apos;oauth c3cef7c66a1843f8b3a9e6a1e3160e20&apos;,#X-Xsrftoken则是防 Xsrf 跨站的 Token 认证，在Response Headers的Set-Cookie字段中可以找到。所以我们需要先请求一次登录页面，然后用正则把这一段匹配出来。注意需要无 Cookies 请求才会返回 Set-Cookie&apos;X-Xsrftoken&apos;: _xsrf</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/set_cookie%E6%8D%95%E8%8E%B7.PNG?raw=true" alt="Set-Cookie"></p><p>&#160;&#160;&#160;&#160;要想登陆成功，form_data也里必须还要俩个参数</p><pre><code>&apos;captcha&apos;: 验证码,&apos;timestamp&apos;: 时间戳,&apos;signature&apos;: 是通过 Hmac 算法对几个固定值和时间戳进行加密</code></pre><p>&#160;&#160;&#160;&#160;timestamp 时间戳，这个很好解决，区别是这里是13位整数，Python 生成的整数部分只有10位，需要额外乘以1000</p><pre><code>timestamp = str(int(time.time()*1000))</code></pre><p>&#160;&#160;&#160;&#160;captcha 验证码，是通过 GET 请求单独的 API 接口返回是否需要验证码（无论是否需要，都要请求一次），如果是 True 则需要再次 PUT 请求获取图片的 base64 编码。</p><pre><code>def _get_captcha(self, headers):   &quot;&quot;&quot;   请求验证码的 API 接口，无论是否需要验证码都需要请求一次   如果需要验证码会返回图片的 base64 编码   根据头部 lang 字段匹配验证码，需要人工输入   :param headers: 带授权信息的请求头部   :return: 验证码的 POST 参数   &quot;&quot;&quot;   lang = headers.get(&apos;lang&apos;, &apos;en&apos;)   if lang == &apos;cn&apos;:       api = &apos;https://www.zhihu.com/api/v3/oauth/captcha?lang=cn&apos;   else:       api = &apos;https://www.zhihu.com/api/v3/oauth/captcha?lang=en&apos;   resp = self.session.get(api, headers=headers)   show_captcha = re.search(r&apos;true&apos;, resp.text)   if show_captcha:       put_resp = self.session.put(api, headers=headers)       img_base64 = re.findall(           r&apos;&quot;img_base64&quot;:&quot;(.+)&quot;&apos;, put_resp.text, re.S)[0].replace(r&apos;\n&apos;, &apos;&apos;)       with open(&apos;./captcha.jpg&apos;, &apos;wb&apos;) as f:           f.write(base64.b64decode(img_base64))       img = Image.open(&apos;./captcha.jpg&apos;)       if lang == &apos;cn&apos;:           plt.imshow(img)           print(&apos;点击所有倒立的汉字，按回车提交&apos;)           points = plt.ginput(7)           capt = json.dumps({&apos;img_size&apos;: [200, 44],                              &apos;input_points&apos;: [[i[0]/2, i[1]/2] for i in points]})       else:           img.show()           capt = input(&apos;请输入图片里的验证码：&apos;)       # 这里必须先把参数 POST 验证码接口       self.session.post(api, data={&apos;input_text&apos;: capt}, headers=headers)       return capt   return &apos;&apos;</code></pre><p>&#160;&#160;&#160;&#160;signature 通过 Crtl+Shift+F 搜索找到是在一个 JS 里生成的，是通过 Hmac 算法对几个固定值和时间戳进行加密，那么只需要在 Python 里也模拟一次这个加密即可。</p><pre><code>def _get_signature(self, timestamp):    &quot;&quot;&quot;    通过 Hmac 算法计算返回签名    实际是几个固定字符串加时间戳    :param timestamp: 时间戳    :return: 签名    &quot;&quot;&quot;    ha = hmac.new(b&apos;d1b964811afb40118a12068ff74a12f4&apos;, digestmod=hashlib.sha1)    grant_type = self.login_data[&apos;grant_type&apos;]    client_id = self.login_data[&apos;client_id&apos;]    source = self.login_data[&apos;source&apos;]    ha.update(bytes((grant_type + client_id + source + timestamp), &apos;utf-8&apos;))    return ha.hexdigest()</code></pre><p>文章出自   <a href="https://zhuanlan.zhihu.com/p/34073256" target="_blank" rel="external">知乎</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;#160;&amp;#160;&amp;#160;&amp;#160;首先我们知道随着知乎页面的不断改版，以前的模拟登陆以不能用了，以下是对知乎改版之后的最新登陆方法&lt;/p&gt;
&lt;p&gt;一、首先我们所需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import requests
import time
im
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="知乎" scheme="http://yoursite.com/tags/%E7%9F%A5%E4%B9%8E/"/>
    
      <category term="模拟登陆" scheme="http://yoursite.com/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86/"/>
    
  </entry>
  
  <entry>
    <title>利用scrapy爬取伯乐在线的所有文章</title>
    <link href="http://yoursite.com/2018/04/24/scrapy_python/"/>
    <id>http://yoursite.com/2018/04/24/scrapy_python/</id>
    <published>2018-04-24T01:11:05.239Z</published>
    <updated>2018-04-25T09:39:31.442Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先我们所需要的库</p><pre><code>1、resquests2、re</code></pre><p>二、创建一个scrapy</p><pre><code>scrapy startproject ArticleSpiderscrapy genspider jobbole www.jobbole.com</code></pre><p>三、爬取所有文章列表的url以及爬取下一页</p><pre><code>#所要爬取链接的起始urlstart_urls = [&apos;http://blog.jobbole.com/all-posts/&apos;]def parse(self, response):    #获取第一页下的所有a标签    url_list=response.css(&apos;#archive .floated-thumb .post-thumb a&apos;)    for url in url_list:        #获取img标签下的src属性，图片的链接dizhi        url_img=url.css(&apos;img::attr(src)&apos;).extract_first()        #获取每一个url_list的详情页面的url        urls=url.xpath(&apos;@href&apos;)[0].extract()        #通过parse.urljoin传递一个绝对地址        #通过meta属性向item中添加font_image_url属性        yield scrapy.Request(parse.urljoin(response.url,urls),meta={&apos;font_image_url&apos;:url_img},callback=self.get_parse)    #获取下一页的链接地址、    next=response.css(&apos;.next.page-numbers::attr(href)&apos;)[0].extract()    #一直循环，知道没有下一页为止    if next:        #回调parse函数        yield scrapy.Request(parse.urljoin(response.url,next),callback=self.parse)    else:        return None</code></pre><p>四、对详情<a href="http://blog.jobbole.com/all-posts/" target="_blank" rel="external">文章页面</a>进行分析</p><p>&#160;&#160;&#160;&#160;我们要的数据是文章的 </p><pre><code>[标题，日期，标签，文章，点赞数，收藏数，评论数]#标题title= response.css(&apos;.grid-8 .entry-header h1::text&apos;)[0].extract()#日期data=response.css(&apos;.grid-8 .entry-meta p::text&apos;)[0].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()#标签tag_list = response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()tag_list = [element for element in tag_list if not element.strip().endswith(&quot;评论&quot;)]#删除标签内的评论数tags = &quot;,&quot;.join(tag_list)#把标签数组以&apos;,&apos;链接生成字符串#文章article= response.css(&apos;.grid-8 .entry&apos;)[0].extract()#点赞数votetotal=response.css(&apos;.post-adds h10::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, votetotal)#用正则筛选出我们所需要的数字if match_re:    votetotal=int(match_re.group(1))#返回第一个else:    votetotal=0#如国没有则默认为0#收藏数bookmark=response.css(&apos;.post-adds .bookmark-btn::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, bookmark)  if match_re:    bookmark=int(match_re.group(1))else:    bookmark=0#评论数comments=response.css(&apos;.post-adds a .hide-on-480::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, comments)if match_re: comments=int(match_re.group(1))else: comments=0</code></pre><p>&#160;&#160;&#160;&#160;get_parse方法来获取详情页的数据</p><pre><code>def get_parse(self,response):    #接收传过来的 font_iamgge_url    image_url=response.meta.get(&apos;font_image_url&apos;,&apos;&apos;)    title= response.css(&apos;.grid-8 .entry-header h1::text&apos;)[0].extract()    data=response.css(&apos;.grid-8 .entry-meta p::text&apos;)[0].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()    category=response.css(&apos;.grid-8 .entry-meta p a::text&apos;)[0].extract()    tag=response.css(&apos;.grid-8 .entry-meta p a::text&apos;)[-1].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()    article= response.css(&apos;.grid-8 .entry&apos;)[0].extract()    votetotal=response.css(&apos;.post-adds h10::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, votetotal)    if match_re:        votetotal=int(match_re.group(1))    else:        votetotal=0    bookmark=response.css(&apos;.post-adds .bookmark-btn::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, bookmark)    if match_re:        bookmark=int(match_re.group(1))    else:        bookmark=0    comments=response.css(&apos;.post-adds a .hide-on-480::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, comments)    if match_re:        comments=int(match_re.group(1))    else:        comments=0    #对item对象实例化    item=ArticlespiderItem()    item[&apos;url&apos;]=response.url    #调用md5把url压缩为固定的哈希值    item[&apos;url_object_id&apos;] = get_md5(response.url)    item[&apos;image_url&apos;]=[image_url]    #调用dtaetime库把字符串转化为date属性    try:        data=datetime.datetime.strftime(data,&quot;%Y/%m/%d&quot;).date()    except Exception as e:        #如果有异常就获取当前系统的时间        data=datetime.datetime.now().date()    item[&apos;data&apos;]=data    item[&apos;title&apos;] = title    item[&apos;category&apos;] = category    item[&apos;tag&apos;] = tag    item[&apos;article&apos;] = article    item[&apos;votetotal&apos;] = votetotal    item[&apos;bookmark&apos;]=bookmark    item[&apos;comments&apos;] = comments</code></pre><p>五、下载每一篇文章的图片</p><pre><code>在settings.py文件中加入#获取item中iamge_url的图片链接IMAGES_URLS_FIELD=&apos;image_url&apos;#获取当前的文件路径object_url=os.path.abspath(os.path.dirname(__file__))#创建image文件夹来存储图片IMAGES_STORE=os.path.join(object_url,&apos;image&apos;)</code></pre><p>&#160;&#160;&#160;&#160;在image文件下会自动生成图片的名字，在ImagesPipeline中我们会找到path变量，我们可以找到每个url所对应的图片的名字，把它存到item中</p><pre><code>class ArticleImagePipeline(ImagesPipeline):def item_completed(self, results, item, info):    if &apos;image_url&apos; in item:        for ok,value in results:            image_file_path=value[&apos;path&apos;]        item[&apos;image_url_path&apos;]=image_file_path    return item</code></pre><p>六、随着以后爬取速度加快，存的速度赶不上爬的速度，导致存的堆积影响性能，所以使用twisted将musql变成异步操作</p><pre><code>from twisted.enterprise import adbapiclass MysqlTwistePipline(object):def __init__(self,dbpool):    self.dbpool=dbpool@classmethoddef from_settings(cls,settings):    dbparms=dict(        host=settings[&apos;MYSQL_HOST&apos;],        db=settings[&apos;MYSQL_DB&apos;] ,        user=settings[&apos;MYSQL_USER&apos;] ,        passwd=settings[&apos;MYSQL_PASSWORD&apos;] ,        charset=&apos;utf8&apos;,        cursorclass=MySQLdb.cursors.DictCursor,        use_unicode=True,    )    dbpool=adbapi.ConnectionPool(&apos;MySQLdb&apos;,**dbparms)    return cls(dbpool)def process_item(self , item , spider):    #使用twisted将musql变成异步操作    query=self.dbpool.runInteraction(self.do_insert,item)    query.addErrback(self.hand_erro)def hand_erro(self,failure):    print(failure)def do_insert(self,cursor,item):    url = item[&apos;url&apos;]    url_object_id = item[&apos;url_object_id&apos;]    image_urls = item[&apos;image_url&apos;]    image_url = image_urls[0]    image_url_path = item[&apos;image_url_path&apos;]    title = item[&apos;title&apos;]    data = item[&apos;data&apos;]    category = item[&apos;category&apos;]    tag = item[&apos;tag&apos;]    article = item[&apos;article&apos;]    votetotal = item[&apos;votetotal&apos;]    bookmark = item[&apos;bookmark&apos;]    comments = item[&apos;comments&apos;]    cursor.execute(        &apos;insert into jobole(title,data,url,url_object_id,image_url,image_url_path,tag,category,article,votetotal,bookmark,comments) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)&apos; ,        (title , data , url , url_object_id , image_url , image_url_path , tag , category , article , votetotal ,         bookmark , comments))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先我们所需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、resquests
2、re
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;二、创建一个scrapy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy startproject ArticleSpider

scrapy genspid
      
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>爬取房天下的租房信息，并对数据进行可视化展示</title>
    <link href="http://yoursite.com/2018/04/16/ScrapyZufang/"/>
    <id>http://yoursite.com/2018/04/16/ScrapyZufang/</id>
    <published>2018-04-16T12:28:34.098Z</published>
    <updated>2018-04-16T13:46:20.008Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先准备需要的库</p><pre><code>1、pandas//是python的一个数据分析包2、matplotlib//是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。3、jieba//jieba(结巴)是一个强大的分词库,完美支持中文分词4、wordcloud//基于Python的词云生成类库5、numpy//NumPy系统是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix））</code></pre><p>二、用scrapy创建一个项目</p><pre><code>scrapy startproject zufangSpiderscrapy genspider zufang http://zu.sh.fang.com/cities.aspx</code></pre><p>三、实现房天下的的数据爬取</p><pre><code>#初始化urldef start_requests(self):    yield scrapy.Reques(self.city_url,callback=self.get_city)#调用get_city()方法 def get_city(self,response):    url=response.xpath(&apos;/html/body/div[3]/div[1]/a/@href&apos;).extract()    #循环热门城市    for i in url:        product={            &apos;city&apos;:i        }        yield scrapy.Request(product[&apos;city&apos;],callback=self.city_parse, dont_filter=True)        #循环爬取每一页        for j in range(2,10):            next_url=product[&apos;city&apos;]+&apos;house/i3%s&apos;%j            yield scrapy.Request(next_url,callback=self.city_parse,dont_filter=True)#调用city_parse()方法获取每一页的数据def city_parse(self, response):    zufang = response.xpath(&apos;//div[@class=&quot;houseList&quot;]&apos;)    for fangzi in zufang:        title=fangzi.xpath(&apos;//p[@class=&quot;title&quot;]/a/text()&apos;).extract()        area =fangzi.xpath(&apos;//p[@class=&quot;gray6 mt20&quot;]/a[1]/span[1]/text()&apos;).extract()        rent_style = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[1]&apos;).extract()        house_type= fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[2]&apos;).extract()        house_area = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[3]&apos;).extract()        if fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&apos;):            orientation = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&apos;).extract()        else:            orientation=&apos;&apos;        price = fangzi.xpath(&apos;//p[@class=&quot;mt5 alingC&quot;]/span/text()&apos;).extract()        for i in range(len(title)):            item = ZufangScrapyItem()            item[&apos;title&apos;]=title[i]            item[&apos;area&apos;]=area[i]            item[&apos;rent_style&apos;]=rent_style[i].strip()            item[&apos;house_type&apos;]=house_type[i]            item[&apos;house_area&apos;]=house_area[i]            item[&apos;orientation&apos;]=orientation[i].strip()            item[&apos;price&apos;]=price[i]            yield item</code></pre><p>四、对数据进行可视化</p><pre><code>1、首先运行 scrapy crawl zufang -o zufang.csv把数据保存成csv文件2、import pandas as pddata=pd.read_csv(r&apos;H:\Python\zufang_scrapy\zufang.csv&apos;)data.head()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang.jpg?raw=true" alt="zufang.csv"></p><pre><code>3、import matplotlib.pyplot as pltplt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;]data[&apos;orientation&apos;].value_counts().plot(kind=&apos;barh&apos;,rot=0)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang1.jpg?raw=true" alt="image"></p><pre><code>4、final=&apos;&apos;stopword=[&apos;NaN&apos;]for n in range(data.shape[0]):seg_list=list(jieba.cut(data[&apos;area&apos;][n]))for seg in seg_list:    if seg not in stopword:        final=final+seg+&apos; &apos;my_wordcloud=WordCloud(collocations=False,font_path=r&apos;C:\Windows\Fonts\simfang.ttf&apos;,width=2000,height=600,margin=2).generate(final)plt.imshow(my_wordcloud)plt.axis(&apos;off&apos;)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang2.jpg?raw=true" alt="image"></p><p>详细代码：可以访问我的 <a href="https://github.com/TomorrowLi/ScraptZufaang" target="_blank" rel="external">GitHub</a> 地址</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先准备需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、pandas//是python的一个数据分析包
2、matplotlib//是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。
3、jieba//jieba(结巴)是一
      
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>对百度糯米、美团、大众点评的数据进行爬取并保存到 mongodb 数据库和mysql数据库</title>
    <link href="http://yoursite.com/2018/04/09/nuomi-meituan-dazhong/"/>
    <id>http://yoursite.com/2018/04/09/nuomi-meituan-dazhong/</id>
    <published>2018-04-09T11:53:36.595Z</published>
    <updated>2018-04-09T14:31:55.831Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先准备我们需要的库</p><pre><code>1、requests//用来请求网页2、json//用来解析json数据，用于把json数据转换为字典3、re//利用正则对字符串查找3、pyquery//利用css查找4、pymongo//存取pymongo5、MySQLdb//存取mysql</code></pre><p>创建一个配置文件config.py用于存储数据库的密码</p><pre><code>MYSQL_HOST=&apos;localhost&apos;MYSQL_USER=&apos;root&apos;MYSQL_PASSWORD=&apos;&apos;MYSQL_DB=&apos;test&apos;MOGO_URL=&apos;localhost&apos;MOGO_DB=&apos;nuomi&apos;MOGO_TABLE=&apos;product&apos;MOGO_DB_M=&apos;meituan&apos;MOGO_TABLE_M=&apos;product&apos;MOGO_DB_D=&apos;dazhong&apos;MOGO_TABLE_D=&apos;product&apos;</code></pre><p>二、百度糯米</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&apos;https://www.nuomi.com/pcindex/main/changecity&apos;    response=requests.get(url)    response.encoding=&apos;utf-8&apos;    doc=pq(response.text)    items=doc(&apos;.city-list .cities li&apos;).items()    for item in items:        product={            &apos;city&apos;:item.find(&apos;a&apos;).text(),            &apos;url&apos;:&apos;https:&apos;+item.find(&apos;a&apos;).attr(&apos;href&apos;)        }        get_pase(product[&apos;url&apos;],keyword)</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_pase(url,keyword):    head={        &apos;k&apos;:keyword,    }    urls=url+&apos;/search?&apos;+urlencode(head)    response=requests.get(urls)    response.encoding = &apos;utf-8&apos;    req=re.findall(&apos;noresult-tip&apos;,response.text)    if req:        print(&apos;抱歉,没有找到你搜索的内容&apos;)    else:        req=r&apos;&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;.*?&quot; class=&quot;shop-infoo-list-item-img&quot; /&gt;&lt;/a&gt;&apos;        url_req=re.findall(req,response.text)        for i in url_req:            url_pase=&apos;https:&apos;+i            get_pase_url(url_pase)        req=r&apos;&lt;a href=&quot;(.*?)&quot; .*? class=&quot;ui-pager-normal&quot; .*?&lt;/a&gt;&apos;        url_next=re.findall(req,response.text)        for i in url_next:            url_pases=url+i            get_pase_url(url_pases)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_pase_url(url):    response=requests.get(url)    response.encoding = &apos;utf-8&apos;    doc=pq(response.text)    product={        &apos;title&apos;:doc(&apos;.shop-box .shop-title&apos;).text(),        &apos;score&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; p &gt; span.score&apos;).text(),        &apos;price&apos;:doc(&apos;.shop-info .price&apos;).text(),        &apos;location&apos;:doc(&apos;.item .detail-shop-address&apos;).text(),        &apos;phone&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(2) &gt; p&apos;).text(),        &apos;time&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(3) &gt; p&apos;).text(),        &apos;tuijian&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(4) &gt; p&apos;).text()    }    print(product)    save_mysql(product)    #save_mongodb(product)</code></pre><p>4、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor = conn.cursor()    cursor.execute(&quot;insert into nuomi(title,score,price,location,phone,time,tuijian) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;title&apos;] , product[&apos;score&apos;] , product[&apos;price&apos;] , product[&apos;location&apos;] , product[&apos;phone&apos;] ,product[&apos;time&apos;] , product[&apos;tuijian&apos;]))    print(&apos;成功存入数据库&apos;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB]    try:        if db[MOGO_TABLE].insert(result):            print(&apos;保存成功&apos;,result)    except Exception:        print(&apos;保存失败&apos;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%99%BE%E5%BA%A6%E7%B3%AF%E7%B1%B3.jpg?raw=true" alt="image"></p><p>二、美团</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&apos;http://www.meituan.com/changecity/&apos;    response=requests.get(url)    response.encoding=&apos;utf-8&apos;    doc=pq(response.text)    items=doc(&apos;.city-area .cities .city&apos;).items()    for item in items:        product={            &apos;url&apos;:&apos;http:&apos;+item.attr(&apos;href&apos;),            &apos;city&apos;:item.text()        }        get_url_number(product[&apos;url&apos;])</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_number(url):    try:        response=requests.get(url)        req=r&apos;{&quot;currentCity&quot;:{&quot;id&quot;:(.*?),&quot;name&quot;:&quot;.*?&quot;,&quot;pinyin&quot;:&apos;        number_url=re.findall(req,response.text)        for code in range(0,500,32):            url=&apos;http://apimobile.meituan.com/group/v4/poi/pcsearch/{}?limit=32&amp;offset={}&amp;q={}&apos;.format(number_url[0],code,keyword)            response=requests.get(url)            data=json.loads(response.text)            imageUrl=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;imageUrl&apos;]            address=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;address&apos;]            lowestprice=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;lowestprice&apos;]            title=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;title&apos;]            url_id=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;id&apos;]            product={                &apos;url_id&apos;:url_id,                &apos;imageUrl&apos;:imageUrl,                &apos;address&apos;:address,                &apos;lowestprice&apos;:lowestprice,                &apos;title&apos;:title            }            save_mysql(product)    except Exception:        return None</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor = conn.cursor()    cursor.execute(&quot;insert into meituan(url_id,imageUrl,address,lowestprice,title) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;url_id&apos;], product[&apos;imageUrl&apos;], product[&apos;address&apos;], product[&apos;lowestprice&apos;], product[&apos;title&apos;]))    print(&apos;成功存入数据库&apos;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_M]    try:        if db[MOGO_TABLE_M].insert(result):            print(&apos;保存成功&apos;,result)    except Exception:        print(&apos;保存失败&apos;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%BE%8E%E5%9B%A2.jpg?raw=true" alt="image"></p><p>三、大众点评</p><p>1、获取全国的url</p><pre><code>def get_url_city_id():    url = &apos;https://www.dianping.com/ajax/citylist/getAllDomesticCity&apos;    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,    }    response = requests.get(url , headers=headers)    data=json.loads(response.text)    for i in range(1,35):        url_data=data[&apos;cityMap&apos;][str(i)]        for item in url_data:            product={                &apos;cityName&apos;:item[&apos;cityName&apos;],                &apos;cityId&apos;:item[&apos;cityId&apos;],                &apos;cityEnName&apos;:item[&apos;cityEnName&apos;]            }            get_url_keyword(product)            break</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_keyword(product):    urls = &apos;https://www.dianping.com/search/keyword/{}/0_%{}&apos;.format(product[&apos;cityId&apos;], keyword)    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,    }    response = requests.get(urls, headers=headers)    req=r&apos;data-hippo-type=&quot;shop&quot; title=&quot;.*?&quot; target=&quot;_blank&quot; href=&quot;(.*?)&quot;&apos;    data=re.findall(req,response.text)    for url in data:        get_url_data(url)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_url_data(url):    headers= {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,        &apos;Host&apos;: &apos;www.dianping.com&apos;,        &apos;Pragma&apos;: &apos;no - cache&apos;,        &apos;Upgrade - Insecure - Requests&apos;: &apos;1&apos;    }    response = requests.get(url,headers=headers)    doc=pq(response.text)    title=doc(&apos;#basic-info &gt; h1&apos;).text().replace(&apos;\n&apos;,&apos;&apos;).replace(&apos;\xa0&apos;,&apos;&apos;)    avgPriceTitle=doc(&apos;#avgPriceTitle&apos;).text()    taste=doc(&apos;#comment_score &gt; span:nth-of-type(1)&apos;).text()    Environmental=doc(&apos;#comment_score &gt; span:nth-of-type(2)&apos;).text()    service=doc(&apos;#comment_score &gt; span:nth-of-type(3)&apos;).text()    street_address=doc(&apos;#basic-info &gt; div.expand-info.address &gt; span.item&apos;).text()    tel=doc(&apos;#basic-info &gt; p &gt; span.item&apos;).text()    info_name=doc(&apos;#basic-info &gt; div.promosearch-wrapper &gt; p &gt; span&apos;).text()    time=doc(&apos;#basic-info &gt; div.other.J-other &gt; p:nth-of-type(1) &gt; span.item&apos;).text()    product={        &apos;title&apos;:title,        &apos;avgPriceTitle&apos;:avgPriceTitle,        &apos;taste&apos;: taste ,        &apos;Environmental&apos;:Environmental,        &apos;service&apos;: service ,        &apos;street_address&apos;:street_address,        &apos;tel&apos;: tel ,        &apos;info_name&apos;:info_name,        &apos;time&apos;:time    }    save_mysql(product)</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor=conn.cursor()    cursor.execute(&quot;insert into dazhong(title,avgPriceTitle,taste,Environmental,service,street_address,tel,info_name,time) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;title&apos;],product[&apos;avgPriceTitle&apos;],product[&apos;taste&apos;],product[&apos;Environmental&apos;],product[&apos;service&apos;],product[&apos;street_address&apos;],product[&apos;tel&apos;],product[&apos;info_name&apos;],product[&apos;time&apos;]))    print(&apos;成功存入数据库&apos; , product)def save_mogodb(product):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_D]    try:        if db[MOGO_TABLE_D].insert(product):            print(&apos;保存成功&apos;,product)    except Exception:        print(&apos;保存失败&apos;,product)</code></pre><p>详细的描述可以访问我的Github <a href="https://github.com/TomorrowLi/meituan-nuomi-dazhong.git" target="_blank" rel="external">TomorrowLi</a> 里面有我的源码</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先准备我们需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、requests//用来请求网页

2、json//用来解析json数据，用于把json数据转换为字典

3、re//利用正则对字符串查找

3、pyquery//利用css查找

4、pymongo//存取pym
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="全国订餐信息" scheme="http://yoursite.com/tags/%E5%85%A8%E5%9B%BD%E8%AE%A2%E9%A4%90%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>利用scrapy爬取知乎用户信息并存储在mongodb数据库中</title>
    <link href="http://yoursite.com/2018/03/31/scrapyZhihu/"/>
    <id>http://yoursite.com/2018/03/31/scrapyZhihu/</id>
    <published>2018-03-31T06:26:24.348Z</published>
    <updated>2018-03-31T07:13:01.744Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先在setting.py中修改True改为False</p><pre><code># Obey robots.txt rules#允许爬取robots.txt文件内不能爬取的资源ROBOTSTXT_OBEY = True#知乎是由反扒措施的必须添加User-agent以及authorizationDEFAULT_REQUEST_HEADERS = {  &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,  &apos;Accept-Language&apos;: &apos;en&apos;,    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos; ,    &apos;authorization&apos;:&apos;Bearer 2|1:0|10:1521707334|4:z_c0|80:MS4xS1NYS0FnQUFBQUFtQUFBQVlBSlZUVWEzb0Z0ZDl0MGhaa0VJOWM0ODhiejhRenVUd0tURnFnPT0=|98bb6f4900c1b4b52ece0282393ede5e31046c9a90216e69dec1feece8086ee0&apos;}</code></pre><p>二、编写spider的项目文件</p><p>1、zhihu_user.py</p><pre><code># -*- coding: utf-8 -*-import jsonimport scrapyfrom scrapy import Spider,Requestfrom ..items import UserItemclass ZhihuUserSpider(Spider):    name = &apos;zhihu_user&apos;    allowed_domains = [&apos;www.zhihu.com&apos;]    start_urls = [&apos;http://www.zhihu.com/&apos;]    start_user=&apos;excited-vczh&apos;    user_url=&apos;https://www.zhihu.com/api/v4/members/{user}?include={include}&apos;    user_query=&apos;allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&apos;    follows_url=&apos;https://www.zhihu.com/api/v4/members/{user}/followees?include={include}&amp;offset={offset}&amp;limit={limit}&apos;    follows_query=&apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos;    followers_url=&apos;https://www.zhihu.com/api/v4/members/{user}/followers?include={include}&amp;offset={offset}&amp;limit={limit}&apos;    followers_query=&apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos;    def start_requests(self):        yield Request(self.user_url.format(user=self.start_user,include=self.user_query),self.parse_user)        yield Request(self.follows_url.format(user=self.start_user,include=self.follows_query,offset=0,limit=20),callback=self.parse_query)    def parse_user(self, response):        result=json.loads(response.text)        item=UserItem()        for field in item.fields:            if field in result.keys():                item[field]=result.get(field)        yield item        yield Request(self.follows_url.format(user=result.get(&apos;url_token&apos;),include=self.follows_query,offset=0,limit=20),callback=self.parse_query)        yield Request(self.followers_url.format(user=result.get(&apos;url_token&apos;),include=self.followers_query,offset=0,limit=20),callback=self.parse_querys)    def parse_query(self, response):        results=json.loads(response.text)        if &apos;data&apos; in results.keys():            for result in results.get(&apos;data&apos;):                yield Request(self.user_url.format(user=result.get(&apos;url_token&apos;),include=self.user_query),callback=self.parse_user)        if &apos;paging&apos; in results.keys() and results.get(&apos;paging&apos;).get(&apos;is_end&apos;)==False:            next_page=results.get(&apos;paging&apos;).get(&apos;next&apos;)            yield Request(next_page,self.parse_query)    def parse_querys(self, response):        results=json.loads(response.text)        if &apos;data&apos; in results.keys():            for result in results.get(&apos;data&apos;):                yield Request(self.user_url.format(user=result.get(&apos;url_token&apos;),include=self.user_query),callback=self.parse_user)        if &apos;paging&apos; in results.keys() and results.get(&apos;paging&apos;).get(&apos;is_end&apos;)==False:            next_page=results.get(&apos;paging&apos;).get(&apos;next&apos;)            yield Request(next_page,self.parse_querys)</code></pre><p>2、items.py</p><pre><code># -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyfrom scrapy import Item,Fieldclass UserItem(Item):    # define the fields for your item here like:    # name = scrapy.Field()    headline=Field()    avatar_url=Field()    name=Field()    type=Field()    url_token=Field()    user_type=Field()</code></pre><p>3、pipelines.py</p><pre><code># -*- coding: utf-8 -*-# Define your item pipelines here## Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongoclass MongoPipeline(object):    collection_name = &apos;scrapy_items&apos;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[&apos;user&apos;].update({&apos;url_token&apos;:item[&apos;url_token&apos;]},{&apos;$set&apos;:item},True)        #self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、setting.py</p><pre><code>BOT_NAME = &apos;zhihu&apos;SPIDER_MODULES = [&apos;zhihu.spiders&apos;]NEWSPIDER_MODULE = &apos;zhihu.spiders&apos;MONGO_URI=&apos;localhost&apos;MONGO_DATABASE=&apos;zhihu&apos;</code></pre><p>三、最后启动setting.py中的</p><pre><code>ITEM_PIPELINES = {       &apos;zhihu.pipelines.MongoPipeline&apos;: 300,}#去掉注释</code></pre><p>四、结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zhihuMongodb.PNG?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先在setting.py中修改True改为False&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Obey robots.txt rules
#允许爬取robots.txt文件内不能爬取的资源
ROBOTSTXT_OBEY = True
#知乎是由反扒措施的必须添加User-a
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="zhihu爬虫" scheme="http://yoursite.com/tags/zhihu%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>scrapy爬虫框架的简介</title>
    <link href="http://yoursite.com/2018/03/29/scrapy/"/>
    <id>http://yoursite.com/2018/03/29/scrapy/</id>
    <published>2018-03-29T14:43:47.032Z</published>
    <updated>2018-03-30T15:59:38.302Z</updated>
    
    <content type="html"><![CDATA[<p>一、安装scrapy所依赖的库</p><pre><code>1、安装wheel    pip install wheel2、安装lxml    https://pypi.python.org/pypi/lxml/4.1.03、安装pyopenssl    https://pypi.python.org/pypi/pyOpenSSL/17.5.04、安装Twisted    https://www.lfd.uci.edu/~gohlke/pythonlibs/5、安装pywin32    https://sourceforge.net/projects/pywin32/files/6、安装scrapy    pip install scrapy</code></pre><p>二、爬虫举例</p><p>1、创建项目</p><pre><code>scrapy startproject zhihu</code></pre><p>2、创建spider项目程序</p><pre><code>cd zhihuscrapy genspider zhihuspider www.zhihu.com</code></pre><p>3、自动创建目录及文件</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/scrapy.jpg?raw=true" alt="image"></p><p>4、生成的爬虫具有基本的结构，我们可以直接在此基础上编写代码</p><pre><code># -*- coding: utf-8 -*-import scrapyclass ZhihuSpider(scrapy.Spider):name = &quot;zhihuspider&quot;allowed_domains = [&quot;zhihu.com&quot;]start_urls = [&apos;http://www.zhihu.com/&apos;]def parse(self, response):    pass</code></pre><p>5、然后，可以我们按照name来运行爬虫</p><pre><code>scrapy crawl &apos;zhihuspider&apos;</code></pre><p>二、项目文件详细分析</p><p>1、spriders文件夹</p><pre><code>#项目文件,以后的代码都要在这里写入zhihuspider.py</code></pre><p>2、items.py</p><pre><code>from scrapy import Item,Fieldclass UserItem(Item):    #Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。    # define the fields for your item here like:    #爬取所需要的数据的名称在这里定义，很像字典    # name = scrapy.Field()</code></pre><p>3、pipelines.py</p><p>&#160; &#160; &#160; &#160; 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理</p><p>###主要是处理以下4点任务</p><p>1.清理HTML数据</p><p>2.验证爬取的数据(检查item包含某些字段)</p><p>3.查重(并丢弃)</p><p>4.将爬取结果保存到数据库中</p><pre><code>#存到mogodb数据库import pymongoclass MongoPipeline(object):    collection_name = &apos;scrapy_items&apos;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、settings.py</p><p>&#160; &#160; &#160; &#160; 用于设置常量的,例如 mongodb 的 url 和 database</p><pre><code>BOT_NAME = &apos;zhihu&apos;SPIDER_MODULES = [&apos;zhihu.spiders&apos;]NEWSPIDER_MODULE = &apos;zhihu.spiders&apos;MONGO_URI=&apos;localhost&apos;MONGO_DATABASE=&apos;zhihu&apos;#默认是True的这个变量是用来是否爬robot文件的改为False是允许的意思ROBOTSTXT_OBEY = True</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、安装scrapy所依赖的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、安装wheel
    pip install wheel
2、安装lxml
    https://pypi.python.org/pypi/lxml/4.1.0
3、安装pyopenssl
    htt
      
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="爬虫框架" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
</feed>
