<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TomorrowLi&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-05T08:16:58.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>TomorrowLi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>爬取网站的种子链接并下载视频到本地</title>
    <link href="http://yoursite.com/2021/07/18/zhongzi/"/>
    <id>http://yoursite.com/2021/07/18/zhongzi/</id>
    <published>2021-07-18T01:44:26.827Z</published>
    <updated>2018-03-05T08:16:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先读取网站的链接存到txt文件中</p><pre><code>import reimport urllib.requestfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.support.ui import WebDriverWaitfrom multiprocessing import Poolbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def search(url,next_title):    try:        browser.get(url)        html = browser.page_source        reg = r&apos;&lt;span style=&quot;&quot; id=&quot;streamurj&quot;&gt;(.*?)&lt;/span&gt;&apos;        wang = re.findall(reg, html)        for i in wang:            zhongzi=&apos;https://openload.co/stream/&apos; +i            print(zhongzi)            with open(&apos;lianjie.txt&apos;, &apos;a+&apos;) as f:                f.write(str(zhongzi) + &apos;\n&apos;)                f.close()            #load(product[&apos;url&apos;],product[&apos;title&apos;])    except TimeoutException:        return search(url,next_title)def load(url,title):    print(&apos;正在下载&apos;,url)    urllib.request.urlretrieve(url,&apos;mp4/%s.mp4&apos;% title)    print(&apos;下载成功&apos;,title)def index_page(url):    try:        browser.get(url)        html = browser.page_source        reg = r&apos;&lt;td class=&quot;mytd-padding&quot;&gt;&lt;a href=&quot;(.*?)&quot;&gt;&apos;        video = re.findall(reg, html)        for age in video:            next_url = &apos;http://ourjav.com/&apos; + age.split(&apos;[&apos;)[0]            next_title=age.split(&apos;[&apos;)[-1]            next_page(next_url,next_title)    except TimeoutException:        index_page(url)def next_page(url,next_title):    try:        browser.get(url)        html = browser.page_source        age=r&apos;&lt;iframe id=&quot;flash_game_object&quot; name=&quot;flash_game_object&quot; frameborder=&quot;no&quot; scrolling=&quot;no&quot; width=&quot;650&quot; height=&quot;500&quot; src=&quot;(.*?)&quot; allowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;&apos;        video=re.findall(age,html,re.S)        for image in video:            search(image,next_title)    except TimeoutException:        next_page(url, next_title)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        respon=browser.get(line3)        count=count+1        if count==5:            break    print(count)def main(offset):    url = &apos;http://ourjav.com/&apos;+&apos;index.php?&amp;page=%s&apos; % offset    index_page(url)if __name__ == &apos;__main__&apos;:    group = [x for x in range(1, 11)]    pool = Pool()    pool.map(main, group)</code></pre><p>其次从txt文件中读取url进行下载</p><pre><code>from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitbrowser = webdriver.Chrome()wait=WebDriverWait(browser, 10)def read_load():    f2 = open(&quot;lianjie.txt&quot;,&quot;r&quot;)    lines = f2.readlines()    count=0    for line3 in lines:        print(line3)        browser.get(line3)        count=count+1        if count==5:            break    print(count)def main():    read_load()if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      想要密码,请联系我
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="种子" scheme="http://yoursite.com/tags/%E7%A7%8D%E5%AD%90/"/>
    
  </entry>
  
  <entry>
    <title>通过知乎抓取粉丝</title>
    <link href="http://yoursite.com/2021/07/18/zhihu/"/>
    <id>http://yoursite.com/2021/07/18/zhihu/</id>
    <published>2021-07-18T01:44:26.799Z</published>
    <updated>2018-03-14T06:59:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="首先以此-网站-为目标路径"><a href="#首先以此-网站-为目标路径" class="headerlink" title="首先以此 网站 为目标路径"></a>首先以此 <a href="https://zhuanlan.zhihu.com/qinlibo-ml/followers" target="_blank" rel="noopener">网站</a> 为目标路径</h3><h2 id="1、所需要的库"><a href="#1、所需要的库" class="headerlink" title="1、所需要的库"></a>1、所需要的库</h2><p>   requests</p><h2 id="2、对以瀑布流的json数据进行解析"><a href="#2、对以瀑布流的json数据进行解析" class="headerlink" title="2、对以瀑布流的json数据进行解析"></a>2、对以瀑布流的json数据进行解析</h2><pre><code>data={        &apos;limit&apos;:20,        &apos;offset&apos;:20    }</code></pre><h2 id="3、代码演示"><a href="#3、代码演示" class="headerlink" title="3、代码演示"></a>3、代码演示</h2><pre><code>#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2018/3/13 15:40# @Author  : tomorrowliimport requestsclass Zhuhu():#初始化参数def __init__(self):    #赋值网址    self.url = &apos;https://zhuanlan.zhihu.com/api/columns/wajuejiprince/followers&apos;    self.headers={        &apos;user-agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;    }def getMesg(self,param):    self.html=requests.get(self.url,params=param,headers=self.headers)    for m in range(20):        &apos;&apos;&apos;        with open(&apos;zhihu.txt&apos;,&apos;a&apos;) as f:            print(self.html.json()[m][&apos;hash&apos;])            f.write(self.html.json()[m][&apos;hash&apos;]+&apos;\n&apos;)        &apos;&apos;&apos;        data={            &apos;bio&apos;:self.html.json()[m][&apos;bio&apos;],            &apos;name&apos;:self.html.json()[m][&apos;name&apos;],            &apos;profileUrl&apos;:self.html.json()[m][&apos;profileUrl&apos;],            &apos;hash&apos;:self.html.json()[m][&apos;hash&apos;]        }        print(data)def main():    for n in range(0,60,20):        data={            &apos;limit&apos;:20,            &apos;offset&apos;:n        }        zhihu=Zhuhu()        zhihu.getMesg(data)if __name__ == &apos;__main__&apos;:    main()</code></pre><h2 id="4、可以为每个粉丝进行发私信"><a href="#4、可以为每个粉丝进行发私信" class="headerlink" title="4、可以为每个粉丝进行发私信"></a>4、可以为每个粉丝进行发私信</h2><p>  需要获取每个粉丝的hash和自己cookie值</p>]]></content>
    
    <summary type="html">
    
      想要密码,请联系我
    
    </summary>
    
    
      <category term="知乎" scheme="http://yoursite.com/tags/%E7%9F%A5%E4%B9%8E/"/>
    
  </entry>
  
  <entry>
    <title>用正则表达式来爬取某论坛的邮箱地址</title>
    <link href="http://yoursite.com/2021/07/18/youxiang/"/>
    <id>http://yoursite.com/2021/07/18/youxiang/</id>
    <published>2021-07-18T01:44:26.775Z</published>
    <updated>2018-02-27T02:14:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport requestsdef fand_email(url,counts):    data=requests.get(url)    content=data.text    pattern = r&apos;[0-9a-zA-Z._]+@[0-9a-zA-Z._]+\.[0-9a-zA-Z._]+&apos;    p = re.compile(pattern)    m = p.findall(content)    with open(&apos;emal.txt&apos;,&apos;a+&apos;) as f:        for i in m:            f.write(i+&apos;\n&apos;)            print(i)            counts= counts+1    return countsdef main():    counts=0    numbers=0    for i in range(1,32):        url=&apos;http://tieba.baidu.com/p/2314539885?pn=%s&apos;% i        number=fand_email(url,counts)        numbers=numbers+number    print(numbers)if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;代码演示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import re

import requests
def fand_email(url,counts):
    data=requests.get(url)
    content=data.text
    pattern 
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="邮箱地址爬取" scheme="http://yoursite.com/tags/%E9%82%AE%E7%AE%B1%E5%9C%B0%E5%9D%80%E7%88%AC%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>用selenium自动爬取天眼查上的公司信息</title>
    <link href="http://yoursite.com/2021/07/18/tianyancha/"/>
    <id>http://yoursite.com/2021/07/18/tianyancha/</id>
    <published>2021-07-18T01:44:26.763Z</published>
    <updated>2018-03-31T07:41:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quict-Start"><a href="#Quict-Start" class="headerlink" title="Quict Start"></a>Quict Start</h2><h3 id="1、需要的库"><a href="#1、需要的库" class="headerlink" title="1、需要的库"></a>1、需要的库</h3><p>re、selenium、pymysql</p><h3 id="2、程序前的准备"><a href="#2、程序前的准备" class="headerlink" title="2、程序前的准备"></a>2、程序前的准备</h3><p>首先在自己的python路径下放入 <a href="http://npm.taobao.org/mirrors/chromedriver/" target="_blank" rel="noopener">chromedriver.exe</a> 选择自己浏览器合适的版本下载</p><h3 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h3><pre><code>from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport reimport MySQLdbhost=&apos;localhost&apos;#主机名user=&apos;root&apos;#数据库的用户名passwd=&apos;&apos;#数据库的密码db=&apos;test&apos;#数据库的名字conn=MySQLdb.connect(host,user,passwd,db,charset=&apos;utf8&apos;)cursor = conn.cursor()boser=webdriver.Chrome()boser.maximize_window()wait=WebDriverWait(boser , 10)def getlogin():    boser.get(&apos;https://www.tianyancha.com/login&apos;)    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb30.position-rel &gt; input&quot;))    )    password=wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &apos;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.pb40.position-rel &gt; input&apos;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR , &apos;#web-content &gt; div &gt; div &gt; div &gt; div.position-rel.container.company_container &gt; div &gt; div.in-block.vertical-top.float-right.right_content.mt50.mr5.mb5 &gt; div.module.module1.module2.loginmodule.collapse.in &gt; div.modulein.modulein1.mobile_box.pl30.pr30.f14.collapse.in &gt; div.c-white.b-c9.pt8.f18.text-center.login_btn&apos;))    )    user.send_keys(&apos;用户名&apos;)    password.send_keys(&apos;密码&apos;)    submit.click()    getSearch()def getSearch():    user = wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR , &quot;#home-main-search&quot;))    )    submit = wait.until(        EC.element_to_be_clickable((By.CSS_SELECTOR ,                                    &apos;#web-content &gt; div &gt; div.mainV3_tab1_enter.position-rel &gt; div.mainv2_tab1.position-rel &gt; div &gt; div.main-tab-outer &gt; div:nth-child(2) &gt; div &gt; div:nth-child(1) &gt; div.input-group.inputV2 &gt; div &gt; span&apos;))    )    key=&apos;佛山市顺德区&apos;    user.send_keys(key)    submit.click()    html=boser.page_source    geturl(html)    getUrlNext()def getUrlNext():    for i in range(2,5):        boser.get(r&apos;https://www.tianyancha.com/search/p%s?key=佛山市顺德区&apos;% i)        html=boser.page_source        geturl(html)def geturl(html):    try:        reg=r&apos;&lt;a href=&quot;.*?&quot; target=&quot;_blank&quot; tyc-event-click=&quot;&quot; tyc-event-ch=&quot;CompanySearch.Company&quot; style=&quot;word-break: break-all;font-weight: 600;&quot; class=&quot;query_name sv-search-company f18 in-block vertical-middle&quot;&gt;&lt;span&gt;(.*?)&lt;/span&gt;&lt;/a&gt;&apos;        names=re.findall(reg,html,re.S)        reg=r&apos;&lt;a title=&quot;.*?&quot; class=&quot;legalPersonName hover_underline&quot; target=&quot;_blank&quot; href=&quot;.*?&quot;&gt;(.*?)&lt;/a&gt;&apos;        daibiao=re.findall(reg,html,re.S)        reg=r&apos;&lt;div class=&quot;title overflow-width&quot; style=&quot;border-right: none&quot;&gt;.*?&lt;span title=&quot;.*?&quot;&gt;(.*?)&lt;/span&gt;&lt;/div&gt;&apos;        days = re.findall(reg,html,re.S)        reg=r&apos;&lt;span class=&quot;sec-c3&quot;&gt;联系电话：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&apos;        phones = re.findall(reg , html , re.S)        reg=r&apos;&lt;div class=&quot;add&quot;&gt;&lt;span class=&quot;sec-c3&quot;&gt;.*?&lt;/span&gt;&lt;span&gt;：&lt;/span&gt;&lt;span class=&quot;overflow-width over-hide vertical-bottom in-block&quot; style=&quot;max-width:500px;&quot;&gt;(.*?)&lt;/span&gt;&apos;        location=re.findall(reg,html,re.S)        for i in range(20):            data={                &apos;name&apos;:names[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;daibiao&apos;:daibiao[i],                &apos;day&apos;:days[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;phone&apos;:phones[i].replace(&apos;&lt;em&gt;&apos;,&apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;),                &apos;location&apos;: location[i].replace(&apos;&lt;em&gt;&apos;, &apos;&apos;).replace(&apos;&lt;/em&gt;&apos;,&apos;&apos;)            }            print(data)            cursor.execute(&quot;insert into tianyancha(name,daibiao,day,phone,location) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(data[&apos;name&apos;],data[&apos;daibiao&apos;],data[&apos;day&apos;],data[&apos;phone&apos;],data[&apos;location&apos;]))            print(data[&apos;daibiao&apos;],&apos;成功存入数据库&apos;)            conn.commit()    except:        return Nonedef main():    getlogin()if __name__ == &apos;__main__&apos;:    main()</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E5%A4%A9%E7%9C%BC%E6%9F%A5.PNG?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Quict-Start&quot;&gt;&lt;a href=&quot;#Quict-Start&quot; class=&quot;headerlink&quot; title=&quot;Quict Start&quot;&gt;&lt;/a&gt;Quict Start&lt;/h2&gt;&lt;h3 id=&quot;1、需要的库&quot;&gt;&lt;a href=&quot;#1、需要的库&quot; cla
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="信息" scheme="http://yoursite.com/tags/%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>Spring的IOC</title>
    <link href="http://yoursite.com/2021/07/18/springIOC/"/>
    <id>http://yoursite.com/2021/07/18/springIOC/</id>
    <published>2021-07-18T01:44:26.731Z</published>
    <updated>2019-01-09T13:38:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、Spring的IOC"><a href="#1、Spring的IOC" class="headerlink" title="1、Spring的IOC"></a>1、Spring的IOC</h2><h3 id="1-1、实例化-Bean-的三种方式"><a href="#1-1、实例化-Bean-的三种方式" class="headerlink" title="1.1、实例化 Bean 的三种方式"></a>1.1、实例化 Bean 的三种方式</h3><pre><code>第一种方式：使用默认无参构造函数（最为常用）    &lt;!--在默认情况下：    它会根据默认无参构造函数来创建类对象。如果 bean 中没有默认无参构造函数，将会创建失败。    &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;/&gt;</code></pre><a id="more"></a><pre><code>第二种方式：spring 管理静态工厂-使用静态工厂的方法创建对象    /**    * 模拟一个静态工厂，创建业务层实现类    */    public class StaticFactory {    public static IAccountService createAccountService(){    return new AccountServiceImpl();    }    }    &lt;!-- 此种方式是:    使用 StaticFactory 类中的静态方法 createAccountService 创建对象，并存入 spring 容器    id 属性：指定 bean 的 id，用于从容器中获取    class 属性：指定静态工厂的全限定类名    factory-method 属性：指定生产对象的静态方法    --&gt;    &lt;bean id=&quot;accountService&quot;     class=&quot;com.itheima.factory.StaticFactory&quot;     factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt;第三种方式：spring 管理实例工厂-使用实例工厂的方法创建对象        /**        * 模拟一个实例工厂，创建业务层实现类        * 此工厂创建对象，必须现有工厂实例对象，再调用方法        */        public class InstanceFactory {        public IAccountService createAccountService(){        return new AccountServiceImpl();        }        }        &lt;!-- 此种方式是：        先把工厂的创建交给 spring 来管理。        然后在使用工厂的 bean 来调用里面的方法        factory-bean 属性：用于指定实例工厂 bean 的 id。        factory-method 属性：用于指定实例工厂中创建对象的方法。        --&gt;        &lt;bean id=&quot;instancFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt;        &lt;bean id=&quot;accountService&quot;         factory-bean=&quot;instancFactory&quot;         factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt;</code></pre><h3 id="1-2、依赖注入（DI）的三种方式"><a href="#1-2、依赖注入（DI）的三种方式" class="headerlink" title="1.2、依赖注入（DI）的三种方式"></a>1.2、依赖注入（DI）的三种方式</h3><pre><code>第一种方式：构造函数注入    &lt;!-- 使用构造函数的方式，给 service 中的属性传值        要求：            类中需要提供一个对应参数列表的构造函数。        涉及的标签：            constructor-arg        属性：            index:指定参数在构造函数参数列表的索引位置            type:指定参数在构造函数中的数据类型            name:指定参数在构造函数中的名称 用这个找给谁赋值            =======上面三个都是找给谁赋值，下面两个指的是赋什么值的==============            value:它能赋的值是基本数据类型和 String 类型            ref:它能赋的值是其他 bean 类型，也就是说，必须得是在配置文件中配置过的 bean        --&gt;        &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;            &lt;constructor-arg name=&quot;name&quot; value=&quot;张三&quot;&gt;&lt;/constructor-arg&gt;            &lt;constructor-arg name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/constructor-arg&gt;            &lt;constructor-arg name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/constructor-arg&gt;        &lt;/bean&gt;        &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;第二种方式：set 方法注入    &lt;!-- 通过配置文件给 bean 中的属性传值：使用 set 方法的方式        涉及的标签：           property        属性：            name：找的是类中 set 方法后面的部分            ref：给属性赋值是其他 bean 类型的            value：给属性赋值是基本数据类型和 string 类型的            实际开发中，此种方式用的较多。        --&gt;        &lt;bean id=&quot;accountService&quot;  class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;            &lt;property name=&quot;name&quot; value=&quot;test&quot;&gt;&lt;/property&gt;            &lt;property name=&quot;age&quot; value=&quot;21&quot;&gt;&lt;/property&gt;            &lt;property name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/property&gt;        &lt;/bean&gt;        &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、Spring的IOC&quot;&gt;&lt;a href=&quot;#1、Spring的IOC&quot; class=&quot;headerlink&quot; title=&quot;1、Spring的IOC&quot;&gt;&lt;/a&gt;1、Spring的IOC&lt;/h2&gt;&lt;h3 id=&quot;1-1、实例化-Bean-的三种方式&quot;&gt;&lt;a href=&quot;#1-1、实例化-Bean-的三种方式&quot; class=&quot;headerlink&quot; title=&quot;1.1、实例化 Bean 的三种方式&quot;&gt;&lt;/a&gt;1.1、实例化 Bean 的三种方式&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;第一种方式：使用默认无参构造函数（最为常用）

    &amp;lt;!--在默认情况下：
    它会根据默认无参构造函数来创建类对象。如果 bean 中没有默认无参构造函数，将会创建失败。
    &amp;lt;bean id=&amp;quot;accountService&amp;quot; class=&amp;quot;com.itheima.service.impl.AccountServiceImpl&amp;quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spring" scheme="http://yoursite.com/categories/spring/"/>
    
    
      <category term="spring" scheme="http://yoursite.com/tags/spring/"/>
    
  </entry>
  
  <entry>
    <title>Spring的AOP</title>
    <link href="http://yoursite.com/2021/07/18/springAOP/"/>
    <id>http://yoursite.com/2021/07/18/springAOP/</id>
    <published>2021-07-18T01:44:26.711Z</published>
    <updated>2019-01-15T08:37:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、springAOP"><a href="#1、springAOP" class="headerlink" title="1、springAOP"></a>1、springAOP</h2><pre><code>作用：      在程序运行期间，不修改源码对已有方法进行增强。 优势：      减少重复代码         提高开发效率        维护方便 </code></pre><h3 id="1-1-基于接口的动态代理"><a href="#1-1-基于接口的动态代理" class="headerlink" title="1.1 基于接口的动态代理"></a>1.1 基于接口的动态代理</h3><pre><code>提供者：JDK 官方的 Proxy 类。  要求：被代理类最少实现一个接口。 </code></pre><h3 id="1，2-基于子类的动态代理"><a href="#1，2-基于子类的动态代理" class="headerlink" title="1，2 基于子类的动态代理"></a>1，2 基于子类的动态代理</h3><pre><code>提供者：第三方的 CGLib，如果报 asmxxxx 异常，需要导入 asm.jar。要求：被代理类不能用 final 修饰的类（最终类）。 </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、springAOP&quot;&gt;&lt;a href=&quot;#1、springAOP&quot; class=&quot;headerlink&quot; title=&quot;1、springAOP&quot;&gt;&lt;/a&gt;1、springAOP&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;作用：  
    在程序运行期间，不修改源码对已有
      
    
    </summary>
    
      <category term="spring" scheme="http://yoursite.com/categories/spring/"/>
    
    
      <category term="spring" scheme="http://yoursite.com/tags/spring/"/>
    
  </entry>
  
  <entry>
    <title>爬取淘宝美食信息并存储到数据库中</title>
    <link href="http://yoursite.com/2021/07/18/selenium-taobao/"/>
    <id>http://yoursite.com/2021/07/18/selenium-taobao/</id>
    <published>2021-07-18T01:44:26.695Z</published>
    <updated>2018-03-31T07:42:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>代码演示：</p><pre><code>import reimport MySQLdbfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom pyquery import PyQuery as pqbroser=webdriver.Chrome()wait=WebDriverWait(broser, 10)def search(table):    try:        broser.get(&apos;https://www.taobao.com&apos;)        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#q&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;#J_TSearchForm &gt; div.search-button &gt; button&apos;))        )        inputs.send_keys(&apos;美食&apos;)        submit.click()        total = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR,&apos;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total&apos;))        )        get_product(table)        return total.text    except TimeoutException:        return search(table)def next_page(page_number,table):    try:        inputs = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input&quot;))        )        submit = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit&quot;))        )        inputs.clear()        inputs.send_keys(page_number)        submit.click()        wait.until(            EC.text_to_be_present_in_element((By.CSS_SELECTOR, &quot;#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span&quot;),str(page_number))        )        get_product(table)    except TimeoutException:        next_page(page_number,table)def get_product(table):    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,&apos;#mainsrp-itemlist .items .item&apos;)))    html=broser.page_source    doc=pq(html)    items=doc(&apos;#mainsrp-itemlist .items .item&apos;).items()    for item in items:        product={            &apos;image&apos;:item.find(&apos;.pic .img&apos;).attr(&apos;src&apos;),            &apos;price&apos;:item.find(&apos;.price&apos;).text().replace(&apos;\n&apos;,&apos;&apos;),            &apos;deal&apos;:item.find(&apos;.deal-cnt&apos;).text()[:-3],            &apos;title&apos;:item.find(&apos;.title&apos;).text().replace(&apos;\n&apos;,&apos;&apos;),            &apos;shop&apos;:item.find(&apos;.shop&apos;).text(),            &apos;location&apos;:item.find(&apos;.location&apos;).text()        }        print(product)        inserttable(table, product[&apos;image&apos;], product[&apos;price&apos;], product[&apos;deal&apos;], product[&apos;title&apos;],product[&apos;shop&apos;],product[&apos;location&apos;])#连接数据库 mysqldef connectDB():        host=&quot;localhost&quot;        dbName=&quot;test&quot;        user=&quot;root&quot;        password=&quot;&quot;        #此处添加charset=&apos;utf8&apos;是为了在数据库中显示中文，此编码必须与数据库的编码一致        db=MySQLdb.connect(host,user,password,dbName,charset=&apos;utf8&apos;)        return db        cursorDB=db.cursor()        return cursorDB#创建表，SQL语言。CREATE TABLE IF NOT EXISTS 表示：表createTableName不存在时就创建def creatTable(createTableName):    createTableSql=&quot;CREATE TABLE IF NOT EXISTS &quot;+ createTableName+&quot;(image VARCHAR(255),price VARCHAR(255),deal  VARCHAR(255),title VARCHAR(255),shop VARCHAR(255),location VARCHAR(255))&quot;    DB_create=connectDB()    print(&apos;链接数据库成功&apos;)    cursor_create=DB_create.cursor()    cursor_create.execute(createTableSql)    DB_create.close()    print(&apos;creat table &apos;+createTableName+&apos; successfully&apos;)    return createTableName#数据插入表中def inserttable(insertTable,insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction):    insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(image,price,deal,title,shop,location)VALUES(%s,%s,%s,%s,%s,%s)&quot;#         insertContentSql=&quot;INSERT INTO &quot;+insertTable+&quot;(time,title,text,clicks)VALUES(&quot;+insertTime+&quot; , &quot;+insertTitle+&quot; , &quot;+insertText+&quot; , &quot;+insertClicks+&quot;)&quot;    DB_insert=connectDB()    cursor_insert=DB_insert.cursor()    cursor_insert.execute(insertContentSql,(insertimages,insertprice,insertdeal,inserttitle,insertshop,insertloaction))    DB_insert.commit()    DB_insert.close()    print (&apos;inert contents to  &apos;+insertTable+&apos; successfully&apos;)def main():    table = creatTable(&apos;yh1&apos;)    print(&apos;创建表成功&apos;)    total=search(table)    total=int(re.compile(&apos;(\d+)&apos;).search(total).group(1))    for i in range(2,total+1):        next_page(i,table)if __name__==&apos;__main__&apos;:    main()</code></pre><p>结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E6%B7%98%E5%AE%9D%E7%BE%8E%E9%A3%9F.PNG?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;代码演示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import re

import MySQLdb
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
fro
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="美食信息" scheme="http://yoursite.com/tags/%E7%BE%8E%E9%A3%9F%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>利用scrapy爬取伯乐在线的所有文章</title>
    <link href="http://yoursite.com/2021/07/18/scrapy_python/"/>
    <id>http://yoursite.com/2021/07/18/scrapy_python/</id>
    <published>2021-07-18T01:44:26.679Z</published>
    <updated>2018-04-25T09:39:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先我们所需要的库</p><pre><code>1、resquests2、re</code></pre><p>二、创建一个scrapy</p><pre><code>scrapy startproject ArticleSpiderscrapy genspider jobbole www.jobbole.com</code></pre><p>三、爬取所有文章列表的url以及爬取下一页</p><pre><code>#所要爬取链接的起始urlstart_urls = [&apos;http://blog.jobbole.com/all-posts/&apos;]def parse(self, response):    #获取第一页下的所有a标签    url_list=response.css(&apos;#archive .floated-thumb .post-thumb a&apos;)    for url in url_list:        #获取img标签下的src属性，图片的链接dizhi        url_img=url.css(&apos;img::attr(src)&apos;).extract_first()        #获取每一个url_list的详情页面的url        urls=url.xpath(&apos;@href&apos;)[0].extract()        #通过parse.urljoin传递一个绝对地址        #通过meta属性向item中添加font_image_url属性        yield scrapy.Request(parse.urljoin(response.url,urls),meta={&apos;font_image_url&apos;:url_img},callback=self.get_parse)    #获取下一页的链接地址、    next=response.css(&apos;.next.page-numbers::attr(href)&apos;)[0].extract()    #一直循环，知道没有下一页为止    if next:        #回调parse函数        yield scrapy.Request(parse.urljoin(response.url,next),callback=self.parse)    else:        return None</code></pre><p>四、对详情<a href="http://blog.jobbole.com/all-posts/" target="_blank" rel="noopener">文章页面</a>进行分析</p><p>&#160;&#160;&#160;&#160;我们要的数据是文章的 </p><pre><code>[标题，日期，标签，文章，点赞数，收藏数，评论数]#标题title= response.css(&apos;.grid-8 .entry-header h1::text&apos;)[0].extract()#日期data=response.css(&apos;.grid-8 .entry-meta p::text&apos;)[0].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()#标签tag_list = response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()tag_list = [element for element in tag_list if not element.strip().endswith(&quot;评论&quot;)]#删除标签内的评论数tags = &quot;,&quot;.join(tag_list)#把标签数组以&apos;,&apos;链接生成字符串#文章article= response.css(&apos;.grid-8 .entry&apos;)[0].extract()#点赞数votetotal=response.css(&apos;.post-adds h10::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, votetotal)#用正则筛选出我们所需要的数字if match_re:    votetotal=int(match_re.group(1))#返回第一个else:    votetotal=0#如国没有则默认为0#收藏数bookmark=response.css(&apos;.post-adds .bookmark-btn::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, bookmark)  if match_re:    bookmark=int(match_re.group(1))else:    bookmark=0#评论数comments=response.css(&apos;.post-adds a .hide-on-480::text&apos;)[0].extract()match_re = re.match(&apos;.*(\d+).*&apos;, comments)if match_re: comments=int(match_re.group(1))else: comments=0</code></pre><p>&#160;&#160;&#160;&#160;get_parse方法来获取详情页的数据</p><pre><code>def get_parse(self,response):    #接收传过来的 font_iamgge_url    image_url=response.meta.get(&apos;font_image_url&apos;,&apos;&apos;)    title= response.css(&apos;.grid-8 .entry-header h1::text&apos;)[0].extract()    data=response.css(&apos;.grid-8 .entry-meta p::text&apos;)[0].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()    category=response.css(&apos;.grid-8 .entry-meta p a::text&apos;)[0].extract()    tag=response.css(&apos;.grid-8 .entry-meta p a::text&apos;)[-1].extract().strip().replace(&apos;·&apos;,&apos;&apos;).strip()    article= response.css(&apos;.grid-8 .entry&apos;)[0].extract()    votetotal=response.css(&apos;.post-adds h10::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, votetotal)    if match_re:        votetotal=int(match_re.group(1))    else:        votetotal=0    bookmark=response.css(&apos;.post-adds .bookmark-btn::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, bookmark)    if match_re:        bookmark=int(match_re.group(1))    else:        bookmark=0    comments=response.css(&apos;.post-adds a .hide-on-480::text&apos;)[0].extract()    match_re = re.match(&apos;.*(\d+).*&apos;, comments)    if match_re:        comments=int(match_re.group(1))    else:        comments=0    #对item对象实例化    item=ArticlespiderItem()    item[&apos;url&apos;]=response.url    #调用md5把url压缩为固定的哈希值    item[&apos;url_object_id&apos;] = get_md5(response.url)    item[&apos;image_url&apos;]=[image_url]    #调用dtaetime库把字符串转化为date属性    try:        data=datetime.datetime.strftime(data,&quot;%Y/%m/%d&quot;).date()    except Exception as e:        #如果有异常就获取当前系统的时间        data=datetime.datetime.now().date()    item[&apos;data&apos;]=data    item[&apos;title&apos;] = title    item[&apos;category&apos;] = category    item[&apos;tag&apos;] = tag    item[&apos;article&apos;] = article    item[&apos;votetotal&apos;] = votetotal    item[&apos;bookmark&apos;]=bookmark    item[&apos;comments&apos;] = comments</code></pre><p>五、下载每一篇文章的图片</p><pre><code>在settings.py文件中加入#获取item中iamge_url的图片链接IMAGES_URLS_FIELD=&apos;image_url&apos;#获取当前的文件路径object_url=os.path.abspath(os.path.dirname(__file__))#创建image文件夹来存储图片IMAGES_STORE=os.path.join(object_url,&apos;image&apos;)</code></pre><p>&#160;&#160;&#160;&#160;在image文件下会自动生成图片的名字，在ImagesPipeline中我们会找到path变量，我们可以找到每个url所对应的图片的名字，把它存到item中</p><pre><code>class ArticleImagePipeline(ImagesPipeline):def item_completed(self, results, item, info):    if &apos;image_url&apos; in item:        for ok,value in results:            image_file_path=value[&apos;path&apos;]        item[&apos;image_url_path&apos;]=image_file_path    return item</code></pre><p>六、随着以后爬取速度加快，存的速度赶不上爬的速度，导致存的堆积影响性能，所以使用twisted将musql变成异步操作</p><pre><code>from twisted.enterprise import adbapiclass MysqlTwistePipline(object):def __init__(self,dbpool):    self.dbpool=dbpool@classmethoddef from_settings(cls,settings):    dbparms=dict(        host=settings[&apos;MYSQL_HOST&apos;],        db=settings[&apos;MYSQL_DB&apos;] ,        user=settings[&apos;MYSQL_USER&apos;] ,        passwd=settings[&apos;MYSQL_PASSWORD&apos;] ,        charset=&apos;utf8&apos;,        cursorclass=MySQLdb.cursors.DictCursor,        use_unicode=True,    )    dbpool=adbapi.ConnectionPool(&apos;MySQLdb&apos;,**dbparms)    return cls(dbpool)def process_item(self , item , spider):    #使用twisted将musql变成异步操作    query=self.dbpool.runInteraction(self.do_insert,item)    query.addErrback(self.hand_erro)def hand_erro(self,failure):    print(failure)def do_insert(self,cursor,item):    url = item[&apos;url&apos;]    url_object_id = item[&apos;url_object_id&apos;]    image_urls = item[&apos;image_url&apos;]    image_url = image_urls[0]    image_url_path = item[&apos;image_url_path&apos;]    title = item[&apos;title&apos;]    data = item[&apos;data&apos;]    category = item[&apos;category&apos;]    tag = item[&apos;tag&apos;]    article = item[&apos;article&apos;]    votetotal = item[&apos;votetotal&apos;]    bookmark = item[&apos;bookmark&apos;]    comments = item[&apos;comments&apos;]    cursor.execute(        &apos;insert into jobole(title,data,url,url_object_id,image_url,image_url_path,tag,category,article,votetotal,bookmark,comments) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)&apos; ,        (title , data , url , url_object_id , image_url , image_url_path , tag , category , article , votetotal ,         bookmark , comments))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先我们所需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、resquests
2、re
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;二、创建一个scrapy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy startproject ArticleSpider

scrapy genspid
      
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>爬取房天下的租房信息，并对数据进行可视化展示</title>
    <link href="http://yoursite.com/2021/07/18/ScrapyZufang/"/>
    <id>http://yoursite.com/2021/07/18/ScrapyZufang/</id>
    <published>2021-07-18T01:44:26.663Z</published>
    <updated>2018-04-16T13:46:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先准备需要的库</p><pre><code>1、pandas//是python的一个数据分析包2、matplotlib//是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。3、jieba//jieba(结巴)是一个强大的分词库,完美支持中文分词4、wordcloud//基于Python的词云生成类库5、numpy//NumPy系统是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多（该结构也可以用来表示矩阵（matrix））</code></pre><p>二、用scrapy创建一个项目</p><pre><code>scrapy startproject zufangSpiderscrapy genspider zufang http://zu.sh.fang.com/cities.aspx</code></pre><p>三、实现房天下的的数据爬取</p><pre><code>#初始化urldef start_requests(self):    yield scrapy.Reques(self.city_url,callback=self.get_city)#调用get_city()方法 def get_city(self,response):    url=response.xpath(&apos;/html/body/div[3]/div[1]/a/@href&apos;).extract()    #循环热门城市    for i in url:        product={            &apos;city&apos;:i        }        yield scrapy.Request(product[&apos;city&apos;],callback=self.city_parse, dont_filter=True)        #循环爬取每一页        for j in range(2,10):            next_url=product[&apos;city&apos;]+&apos;house/i3%s&apos;%j            yield scrapy.Request(next_url,callback=self.city_parse,dont_filter=True)#调用city_parse()方法获取每一页的数据def city_parse(self, response):    zufang = response.xpath(&apos;//div[@class=&quot;houseList&quot;]&apos;)    for fangzi in zufang:        title=fangzi.xpath(&apos;//p[@class=&quot;title&quot;]/a/text()&apos;).extract()        area =fangzi.xpath(&apos;//p[@class=&quot;gray6 mt20&quot;]/a[1]/span[1]/text()&apos;).extract()        rent_style = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[1]&apos;).extract()        house_type= fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[2]&apos;).extract()        house_area = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[3]&apos;).extract()        if fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&apos;):            orientation = fangzi.xpath(&apos;//p[@class=&quot;font16 mt20 bold&quot;]/text()[4]&apos;).extract()        else:            orientation=&apos;&apos;        price = fangzi.xpath(&apos;//p[@class=&quot;mt5 alingC&quot;]/span/text()&apos;).extract()        for i in range(len(title)):            item = ZufangScrapyItem()            item[&apos;title&apos;]=title[i]            item[&apos;area&apos;]=area[i]            item[&apos;rent_style&apos;]=rent_style[i].strip()            item[&apos;house_type&apos;]=house_type[i]            item[&apos;house_area&apos;]=house_area[i]            item[&apos;orientation&apos;]=orientation[i].strip()            item[&apos;price&apos;]=price[i]            yield item</code></pre><p>四、对数据进行可视化</p><pre><code>1、首先运行 scrapy crawl zufang -o zufang.csv把数据保存成csv文件2、import pandas as pddata=pd.read_csv(r&apos;H:\Python\zufang_scrapy\zufang.csv&apos;)data.head()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang.jpg?raw=true" alt="zufang.csv"></p><pre><code>3、import matplotlib.pyplot as pltplt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;]data[&apos;orientation&apos;].value_counts().plot(kind=&apos;barh&apos;,rot=0)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang1.jpg?raw=true" alt="image"></p><pre><code>4、final=&apos;&apos;stopword=[&apos;NaN&apos;]for n in range(data.shape[0]):seg_list=list(jieba.cut(data[&apos;area&apos;][n]))for seg in seg_list:    if seg not in stopword:        final=final+seg+&apos; &apos;my_wordcloud=WordCloud(collocations=False,font_path=r&apos;C:\Windows\Fonts\simfang.ttf&apos;,width=2000,height=600,margin=2).generate(final)plt.imshow(my_wordcloud)plt.axis(&apos;off&apos;)plt.show()</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zufang2.jpg?raw=true" alt="image"></p><p>详细代码：可以访问我的 <a href="https://github.com/TomorrowLi/ScraptZufaang" target="_blank" rel="noopener">GitHub</a> 地址</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先准备需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、pandas//是python的一个数据分析包
2、matplotlib//是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。
3、jieba//jieba(结巴)是一
      
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>利用scrapy爬取知乎用户信息并存储在mongodb数据库中</title>
    <link href="http://yoursite.com/2021/07/18/scrapyZhihu/"/>
    <id>http://yoursite.com/2021/07/18/scrapyZhihu/</id>
    <published>2021-07-18T01:44:26.639Z</published>
    <updated>2018-03-31T07:13:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先在setting.py中修改True改为False</p><pre><code># Obey robots.txt rules#允许爬取robots.txt文件内不能爬取的资源ROBOTSTXT_OBEY = True#知乎是由反扒措施的必须添加User-agent以及authorizationDEFAULT_REQUEST_HEADERS = {  &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,  &apos;Accept-Language&apos;: &apos;en&apos;,    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos; ,    &apos;authorization&apos;:&apos;Bearer 2|1:0|10:1521707334|4:z_c0|80:MS4xS1NYS0FnQUFBQUFtQUFBQVlBSlZUVWEzb0Z0ZDl0MGhaa0VJOWM0ODhiejhRenVUd0tURnFnPT0=|98bb6f4900c1b4b52ece0282393ede5e31046c9a90216e69dec1feece8086ee0&apos;}</code></pre><p>二、编写spider的项目文件</p><p>1、zhihu_user.py</p><pre><code># -*- coding: utf-8 -*-import jsonimport scrapyfrom scrapy import Spider,Requestfrom ..items import UserItemclass ZhihuUserSpider(Spider):    name = &apos;zhihu_user&apos;    allowed_domains = [&apos;www.zhihu.com&apos;]    start_urls = [&apos;http://www.zhihu.com/&apos;]    start_user=&apos;excited-vczh&apos;    user_url=&apos;https://www.zhihu.com/api/v4/members/{user}?include={include}&apos;    user_query=&apos;allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics&apos;    follows_url=&apos;https://www.zhihu.com/api/v4/members/{user}/followees?include={include}&amp;offset={offset}&amp;limit={limit}&apos;    follows_query=&apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos;    followers_url=&apos;https://www.zhihu.com/api/v4/members/{user}/followers?include={include}&amp;offset={offset}&amp;limit={limit}&apos;    followers_query=&apos;data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics&apos;    def start_requests(self):        yield Request(self.user_url.format(user=self.start_user,include=self.user_query),self.parse_user)        yield Request(self.follows_url.format(user=self.start_user,include=self.follows_query,offset=0,limit=20),callback=self.parse_query)    def parse_user(self, response):        result=json.loads(response.text)        item=UserItem()        for field in item.fields:            if field in result.keys():                item[field]=result.get(field)        yield item        yield Request(self.follows_url.format(user=result.get(&apos;url_token&apos;),include=self.follows_query,offset=0,limit=20),callback=self.parse_query)        yield Request(self.followers_url.format(user=result.get(&apos;url_token&apos;),include=self.followers_query,offset=0,limit=20),callback=self.parse_querys)    def parse_query(self, response):        results=json.loads(response.text)        if &apos;data&apos; in results.keys():            for result in results.get(&apos;data&apos;):                yield Request(self.user_url.format(user=result.get(&apos;url_token&apos;),include=self.user_query),callback=self.parse_user)        if &apos;paging&apos; in results.keys() and results.get(&apos;paging&apos;).get(&apos;is_end&apos;)==False:            next_page=results.get(&apos;paging&apos;).get(&apos;next&apos;)            yield Request(next_page,self.parse_query)    def parse_querys(self, response):        results=json.loads(response.text)        if &apos;data&apos; in results.keys():            for result in results.get(&apos;data&apos;):                yield Request(self.user_url.format(user=result.get(&apos;url_token&apos;),include=self.user_query),callback=self.parse_user)        if &apos;paging&apos; in results.keys() and results.get(&apos;paging&apos;).get(&apos;is_end&apos;)==False:            next_page=results.get(&apos;paging&apos;).get(&apos;next&apos;)            yield Request(next_page,self.parse_querys)</code></pre><p>2、items.py</p><pre><code># -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyfrom scrapy import Item,Fieldclass UserItem(Item):    # define the fields for your item here like:    # name = scrapy.Field()    headline=Field()    avatar_url=Field()    name=Field()    type=Field()    url_token=Field()    user_type=Field()</code></pre><p>3、pipelines.py</p><pre><code># -*- coding: utf-8 -*-# Define your item pipelines here## Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongoclass MongoPipeline(object):    collection_name = &apos;scrapy_items&apos;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[&apos;user&apos;].update({&apos;url_token&apos;:item[&apos;url_token&apos;]},{&apos;$set&apos;:item},True)        #self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、setting.py</p><pre><code>BOT_NAME = &apos;zhihu&apos;SPIDER_MODULES = [&apos;zhihu.spiders&apos;]NEWSPIDER_MODULE = &apos;zhihu.spiders&apos;MONGO_URI=&apos;localhost&apos;MONGO_DATABASE=&apos;zhihu&apos;</code></pre><p>三、最后启动setting.py中的</p><pre><code>ITEM_PIPELINES = {       &apos;zhihu.pipelines.MongoPipeline&apos;: 300,}#去掉注释</code></pre><p>四、结果展示</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/zhihuMongodb.PNG?raw=true" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先在setting.py中修改True改为False&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Obey robots.txt rules
#允许爬取robots.txt文件内不能爬取的资源
ROBOTSTXT_OBEY = True
#知乎是由反扒措施的必须添加User-a
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="zhihu爬虫" scheme="http://yoursite.com/tags/zhihu%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>scrapy爬虫框架的简介</title>
    <link href="http://yoursite.com/2021/07/18/scrapy/"/>
    <id>http://yoursite.com/2021/07/18/scrapy/</id>
    <published>2021-07-18T01:44:26.623Z</published>
    <updated>2018-03-30T15:59:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>一、安装scrapy所依赖的库</p><pre><code>1、安装wheel    pip install wheel2、安装lxml    https://pypi.python.org/pypi/lxml/4.1.03、安装pyopenssl    https://pypi.python.org/pypi/pyOpenSSL/17.5.04、安装Twisted    https://www.lfd.uci.edu/~gohlke/pythonlibs/5、安装pywin32    https://sourceforge.net/projects/pywin32/files/6、安装scrapy    pip install scrapy</code></pre><p>二、爬虫举例</p><p>1、创建项目</p><pre><code>scrapy startproject zhihu</code></pre><p>2、创建spider项目程序</p><pre><code>cd zhihuscrapy genspider zhihuspider www.zhihu.com</code></pre><p>3、自动创建目录及文件</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/scrapy.jpg?raw=true" alt="image"></p><p>4、生成的爬虫具有基本的结构，我们可以直接在此基础上编写代码</p><pre><code># -*- coding: utf-8 -*-import scrapyclass ZhihuSpider(scrapy.Spider):name = &quot;zhihuspider&quot;allowed_domains = [&quot;zhihu.com&quot;]start_urls = [&apos;http://www.zhihu.com/&apos;]def parse(self, response):    pass</code></pre><p>5、然后，可以我们按照name来运行爬虫</p><pre><code>scrapy crawl &apos;zhihuspider&apos;</code></pre><p>二、项目文件详细分析</p><p>1、spriders文件夹</p><pre><code>#项目文件,以后的代码都要在这里写入zhihuspider.py</code></pre><p>2、items.py</p><pre><code>from scrapy import Item,Fieldclass UserItem(Item):    #Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。    # define the fields for your item here like:    #爬取所需要的数据的名称在这里定义，很像字典    # name = scrapy.Field()</code></pre><p>3、pipelines.py</p><p>&#160; &#160; &#160; &#160; 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理</p><p>###主要是处理以下4点任务</p><p>1.清理HTML数据</p><p>2.验证爬取的数据(检查item包含某些字段)</p><p>3.查重(并丢弃)</p><p>4.将爬取结果保存到数据库中</p><pre><code>#存到mogodb数据库import pymongoclass MongoPipeline(object):    collection_name = &apos;scrapy_items&apos;    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri = mongo_uri        self.mongo_db = mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)        )    def open_spider(self, spider):        self.client = pymongo.MongoClient(self.mongo_uri)        self.db = self.client[self.mongo_db]    def close_spider(self, spider):        self.client.close()    def process_item(self, item, spider):        self.db[self.collection_name].insert_one(dict(item))        return item</code></pre><p>4、settings.py</p><p>&#160; &#160; &#160; &#160; 用于设置常量的,例如 mongodb 的 url 和 database</p><pre><code>BOT_NAME = &apos;zhihu&apos;SPIDER_MODULES = [&apos;zhihu.spiders&apos;]NEWSPIDER_MODULE = &apos;zhihu.spiders&apos;MONGO_URI=&apos;localhost&apos;MONGO_DATABASE=&apos;zhihu&apos;#默认是True的这个变量是用来是否爬robot文件的改为False是允许的意思ROBOTSTXT_OBEY = True</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、安装scrapy所依赖的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、安装wheel
    pip install wheel
2、安装lxml
    https://pypi.python.org/pypi/lxml/4.1.0
3、安装pyopenssl
    htt
      
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="爬虫框架" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫的精华篇</title>
    <link href="http://yoursite.com/2021/07/18/python/"/>
    <id>http://yoursite.com/2021/07/18/python/</id>
    <published>2021-07-18T01:44:26.606Z</published>
    <updated>2018-03-22T10:21:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最简单的反爬手段"><a href="#最简单的反爬手段" class="headerlink" title="最简单的反爬手段"></a>最简单的反爬手段</h2><p>&#160; &#160; &#160; &#160; 有一部分网站（甚至包括部分商业网站）其实并没有真正认真做过反爬虫这件事情，你不需要做任何伪装，即可分分钟抓光它的网站。面对这样的网站，你只需要开大马力抓抓抓就好了，如果处于情怀考虑，你可以抓的慢一点，让它的服务器别太累。</p><p>&#160; &#160; &#160; &#160; 除去这样的情况，最简单的反爬虫机制，应该算是U-A校验了。浏览器在发送请求的时候，会附带一部分浏览器及当前系统环境的参数给服务器，这部分数据放在HTTP请求的header部分。</p><h2 id="通过访问频度反爬"><a href="#通过访问频度反爬" class="headerlink" title="通过访问频度反爬"></a>通过访问频度反爬</h2><p>&#160; &#160; &#160; &#160; 这种反爬虫机制的原理是，真人通过浏览器访问网站的速度（相对程序来讲）是很慢的，所以如果你一秒访问了200个页面——这里不代表实际数值，只是表达在较短时间内发送了较多请求，具体阈值需要具体考虑——服务器则认为你是一个爬虫。</p><p>适用情况：限制频率情况。</p><p>Requests，Urllib2都可以使用time库的sleep()函数：</p><pre><code>import timetime.sleep(1)</code></pre><h2 id="通过验证码限制"><a href="#通过验证码限制" class="headerlink" title="通过验证码限制"></a>通过验证码限制</h2><p>&#160; &#160; &#160; &#160; 这样的方式与上面的方式相比更加难处理的是，不管你的访问频次怎么样，你都需要输入验证码才行。比如12306，不管你是登录还是购票，也不论你是第一次还是第一万次购买，你都需要输入一个验证码之后才能继续。这时候，在绝大部分情况下，你必须要想办法识别验证码了。之所以说是大多数情况下，是因为在极少数极少数情况下（尤其是政府网站），棒槌程序员通过客户端的JavaScript来校验验证码，这时候对你的爬虫来讲，其实是没有验证码的（比如中国商标网）。除开你遇到这种几乎可以忽略不计的棒槌开发的网站，其他时候你只有通过识别验证码来继续后面的操作了。</p><p>1、首先你可以找到验证吗图片的url<br>2、把图片保存到本地,然后手动输入验证码达到验证的效果，可以参考我前面的文章，有关豆瓣验证码的处理</p><h2 id="通过经常变换网页结构"><a href="#通过经常变换网页结构" class="headerlink" title="通过经常变换网页结构"></a>通过经常变换网页结构</h2><p>&#160; &#160; &#160; &#160; 常见于一些社交网站，他们会经常更换网页的结构，如果你是通过依赖网页结构来解析需要的数据（不幸的是大部分情况下我们都需要通过网页结构来解析需要的数据），则在原本的网页位置找不到原本需要的内容。这时候，要分不同情况具体处理了。如果你只打算一次性抓取特定数据，那么赶快行动，它以后结构变了无所谓，反正你也不打算在抓它一次。如果是需要持续性抓取的网站，就要仔细思考下应对方案了。一个简单粗暴但是比较费力的办法是，写一个校验脚本，定期校验网页结构，如果与预期不符，那么赶快通知自己（或者特定开发者）修改解析部分，以应对网页结构变化。</p><h2 id="对于Ajax请求的处理"><a href="#对于Ajax请求的处理" class="headerlink" title="对于Ajax请求的处理"></a>对于Ajax请求的处理</h2><p>&#160; &#160; &#160; &#160; 对于“加载更多”情况，使用Ajax来传输很多数据。它的工作原理是：从网页的url加载网页的源代码之后，会在浏览器里执行JavaScript程序。这些程序会加载更多的内容，“填充”到网页里。这就是为什么如果你直接去爬网页本身的url，你会找不到页面的实际内容。这里，若使用Google Chrome分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。</p><h2 id="自动化测试工具Selenium"><a href="#自动化测试工具Selenium" class="headerlink" title="自动化测试工具Selenium"></a>自动化测试工具Selenium</h2><p>&#160; &#160; &#160; &#160; Selenium是一款自动化测试工具。它能实现操纵浏览器，包括字符填充、鼠标点击、获取元素、页面切换等一系列操作。总之，凡是浏览器能做的事，Selenium都能够做到。</p><h2 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h2><p>&#160; &#160; &#160; &#160; 适用情况：限制IP地址情况，也可解决由于“频繁点击”而需要输入验证码登陆的情况。<br>这种情况最好的办法就是维护一个代理IP池，网上有很多免费的代理IP，良莠不齐，可以通过筛选找到能用的。对于“频繁点击”的情况，我们还可以通过限制爬虫访问网站的频率来避免被网站禁掉。</p><pre><code>proxies = {&apos;http&apos;:&apos;http://XX.XX.XX.XX:XXXX&apos;}Requests：//强烈推荐使用import requestsresponse = requests.get(url=url, proxies=proxies)Urllib2：import urllib2proxy_support = urllib2.ProxyHandler(proxies)opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)urllib2.install_opener(opener) # 安装opener，此后调用urlopen()时都会使用安装过的opener对象response = urllib2.urlopen(url)</code></pre><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>&#160; &#160; &#160; &#160; 由于Python的是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。<br>multiprocessing提供模块一个了Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：<br>    from multiprocessing import Process<br>    import os</p><pre><code>#子进程要执行的代码def run_proc(name):    print &apos;Run child process %s (%s)...&apos; % (name, os.getpid())if __name__==&apos;__main__&apos;:    print &apos;Parent process %s.&apos; % os.getpid()    p = Process(target=run_proc, args=(&apos;test&apos;,))    print &apos;Process will start.&apos;    p.start()    p.join()    print &apos;Process end.&apos;</code></pre><p>执行结果如下：</p><pre><code>Parent process 928.Process will start.Run child process test (929)...Process end.</code></pre><p>参考 <a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/00143192823818768cd506abbc94eb5916192364506fa5d000" target="_blank" rel="noopener">廖雪峰</a> 的python教程</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;最简单的反爬手段&quot;&gt;&lt;a href=&quot;#最简单的反爬手段&quot; class=&quot;headerlink&quot; title=&quot;最简单的反爬手段&quot;&gt;&lt;/a&gt;最简单的反爬手段&lt;/h2&gt;&lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160; 有一部分网站（甚至包括部分商业网站）
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python学习" scheme="http://yoursite.com/tags/python%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>今日头条上的美女图片进行爬取</title>
    <link href="http://yoursite.com/2021/07/18/python-student/"/>
    <id>http://yoursite.com/2021/07/18/python-student/</id>
    <published>2021-07-18T01:44:26.587Z</published>
    <updated>2018-02-28T03:43:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎来到<a href="https://www.python.org/" target="_blank" rel="noopener">python</a>!学习。 这是我人生旅途的第一课，非常重要。 我从这个案例中学到的许多关于python爬虫的知识，这是我python爬虫的启蒙。</p><p>#所需要的库<br>requests：用于网页请求</p><p>BeautifulSoup：选择所要的元素</p><p>json：用来解析json数据的库</p><p>re：用于正则表达式的筛选</p><p>代码演示：</p><pre><code>import jsonimport refrom hashlib import md5import osimport requestsfrom urllib.parse import urlencodefrom bs4 import BeautifulSoupfrom config import *from multiprocessing import Pool#对要爬取页面的解析def get_page_index(offset, keyword):    data = {        &apos;offset&apos;: offset,        &apos;format&apos;: &apos;json&apos;,        &apos;keyword&apos;: keyword,        &apos;autoload&apos;: &apos;true&apos;,        &apos;count&apos;: &apos;20&apos;,        &apos;cur_tab&apos;: 3,        &apos;from&apos;:&apos;gallery&apos;    }    url = &apos;https://www.toutiao.com/search_content/?&apos; + urlencode(data)    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&apos;请求索引失败&apos;)        return None#获取所要的数据在json中def prase_get_index(html):    data = json.loads(html)    if data and &apos;data&apos; in data.keys():        for item in data.get(&apos;data&apos;):            yield item.get(&apos;article_url&apos;)#用正则对json数据进行解析def prase_page_urlli(html):    soup = BeautifulSoup(html, &apos;lxml&apos;)    title = soup.select(&apos;title&apos;)[0].get_text()    print(title)    images_pattern= re.compile(&apos;gallery: JSON.parse\((.*?)\)&apos;,re.S)    result=re.search(images_pattern,html)    if result:        data=json.loads(result.group(1))        data=eval(data)        if data and &apos;sub_images&apos;in data.keys():            sub_images=data.get(&apos;sub_images&apos;)            image=[item.get(&apos;url&apos;).replace(&apos;\\&apos;,&apos;&apos;) for item in sub_images]            for images_page in image:                dowload_image(images_page)            return{                &apos;title&apos;:title,                &apos;image&apos;:image            }#对所要下载的图片链接进行访问def get_page_urlli(url):    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            return response.text        return None    except:        print(&apos;请求失败&apos;,url)        return None#下载图片def dowload_image(url):    print(&quot;正在下载&quot;,url)    try:        header = {            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36&apos;        }        response = requests.get(url,headers=header)        if response.status_code == 200:            save_image(response.content)        return None    except:        print(&apos;请求图片出错&apos;,url)        return None#把图片保存到本地路径下def save_image(content):    file_path=&apos;{0}/{1}.{2}&apos;.format(os.getcwd(),md5(content).hexdigest(),&apos;jpg&apos;)    if not os.path.exists(file_path):        with open(file_path,&apos;wb&apos;) as f:            f.write(content)            f.close()def main(offset):    html = get_page_index(offset, keyword)    for url in prase_get_index(html):        html=get_page_urlli(url)        if html:            result=prase_page_urlli(html)            print(result)if __name__ == &apos;__main__&apos;:    group=[x*20 for x in range(stat,end+1)]    pool=Pool()    pool.map(main,group)</code></pre><p>详细的解释。</p>]]></content>
    
    <summary type="html">
    
      想要密码,请联系我
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="美女图片" scheme="http://yoursite.com/tags/%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>百度文库下载和百度云加速下载</title>
    <link href="http://yoursite.com/2021/07/18/pojie/"/>
    <id>http://yoursite.com/2021/07/18/pojie/</id>
    <published>2021-07-18T01:44:26.563Z</published>
    <updated>2018-07-12T03:03:06.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、百度文库、道客巴巴登免费下载"><a href="#一、百度文库、道客巴巴登免费下载" class="headerlink" title="一、百度文库、道客巴巴登免费下载"></a>一、百度文库、道客巴巴登免费下载</h3><p>&#160;&#160;&#160;&#160;1、百度文库下载：首先进入你要下载的文档页面</p><pre><code>例如:https://wenku.baidu.com/view/eeb5d53069eae009591bec34.html?from=search</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;2、把图中的链接复制到冰点文库的下载</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baidu1.PNG?raw=true" alt="image"></p><p>&#160;&#160;&#160;&#160;3、文件下载地址：</p><pre><code>链接：https://pan.baidu.com/s/1G0zTlsG61ipGnGfmHPEijw密码: bzjm</code></pre><h3 id="二、百度云加速下载"><a href="#二、百度云加速下载" class="headerlink" title="二、百度云加速下载"></a>二、百度云加速下载</h3><p>&#160;&#160;&#160;&#160;1、首先要登陆你的百度云账号</p><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/baiduyun.PNG?raw=true" alt="image"></p><pre><code>链接：https://pan.baidu.com/s/1Lzm5_8lRPHO0e5r38rl-3A密码: mcc5</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、百度文库、道客巴巴登免费下载&quot;&gt;&lt;a href=&quot;#一、百度文库、道客巴巴登免费下载&quot; class=&quot;headerlink&quot; title=&quot;一、百度文库、道客巴巴登免费下载&quot;&gt;&lt;/a&gt;一、百度文库、道客巴巴登免费下载&lt;/h3&gt;&lt;p&gt;&amp;#160;&amp;#160;&amp;#
      
    
    </summary>
    
      <category term="破解软件" scheme="http://yoursite.com/categories/%E7%A0%B4%E8%A7%A3%E8%BD%AF%E4%BB%B6/"/>
    
    
      <category term="破解软解" scheme="http://yoursite.com/tags/%E7%A0%B4%E8%A7%A3%E8%BD%AF%E8%A7%A3/"/>
    
      <category term="百度" scheme="http://yoursite.com/tags/%E7%99%BE%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>对百度糯米、美团、大众点评的数据进行爬取并保存到 mongodb 数据库和mysql数据库</title>
    <link href="http://yoursite.com/2021/07/18/nuomi-meituan-dazhong/"/>
    <id>http://yoursite.com/2021/07/18/nuomi-meituan-dazhong/</id>
    <published>2021-07-18T01:44:26.549Z</published>
    <updated>2018-04-09T14:31:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>一、首先准备我们需要的库</p><pre><code>1、requests//用来请求网页2、json//用来解析json数据，用于把json数据转换为字典3、re//利用正则对字符串查找3、pyquery//利用css查找4、pymongo//存取pymongo5、MySQLdb//存取mysql</code></pre><p>创建一个配置文件config.py用于存储数据库的密码</p><pre><code>MYSQL_HOST=&apos;localhost&apos;MYSQL_USER=&apos;root&apos;MYSQL_PASSWORD=&apos;&apos;MYSQL_DB=&apos;test&apos;MOGO_URL=&apos;localhost&apos;MOGO_DB=&apos;nuomi&apos;MOGO_TABLE=&apos;product&apos;MOGO_DB_M=&apos;meituan&apos;MOGO_TABLE_M=&apos;product&apos;MOGO_DB_D=&apos;dazhong&apos;MOGO_TABLE_D=&apos;product&apos;</code></pre><p>二、百度糯米</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&apos;https://www.nuomi.com/pcindex/main/changecity&apos;    response=requests.get(url)    response.encoding=&apos;utf-8&apos;    doc=pq(response.text)    items=doc(&apos;.city-list .cities li&apos;).items()    for item in items:        product={            &apos;city&apos;:item.find(&apos;a&apos;).text(),            &apos;url&apos;:&apos;https:&apos;+item.find(&apos;a&apos;).attr(&apos;href&apos;)        }        get_pase(product[&apos;url&apos;],keyword)</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_pase(url,keyword):    head={        &apos;k&apos;:keyword,    }    urls=url+&apos;/search?&apos;+urlencode(head)    response=requests.get(urls)    response.encoding = &apos;utf-8&apos;    req=re.findall(&apos;noresult-tip&apos;,response.text)    if req:        print(&apos;抱歉,没有找到你搜索的内容&apos;)    else:        req=r&apos;&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;.*?&quot; class=&quot;shop-infoo-list-item-img&quot; /&gt;&lt;/a&gt;&apos;        url_req=re.findall(req,response.text)        for i in url_req:            url_pase=&apos;https:&apos;+i            get_pase_url(url_pase)        req=r&apos;&lt;a href=&quot;(.*?)&quot; .*? class=&quot;ui-pager-normal&quot; .*?&lt;/a&gt;&apos;        url_next=re.findall(req,response.text)        for i in url_next:            url_pases=url+i            get_pase_url(url_pases)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_pase_url(url):    response=requests.get(url)    response.encoding = &apos;utf-8&apos;    doc=pq(response.text)    product={        &apos;title&apos;:doc(&apos;.shop-box .shop-title&apos;).text(),        &apos;score&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; p &gt; span.score&apos;).text(),        &apos;price&apos;:doc(&apos;.shop-info .price&apos;).text(),        &apos;location&apos;:doc(&apos;.item .detail-shop-address&apos;).text(),        &apos;phone&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(2) &gt; p&apos;).text(),        &apos;time&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(3) &gt; p&apos;).text(),        &apos;tuijian&apos;:doc(&apos;body &gt; div.main-container &gt; div.shop-box &gt; ul &gt; li:nth-child(4) &gt; p&apos;).text()    }    print(product)    save_mysql(product)    #save_mongodb(product)</code></pre><p>4、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor = conn.cursor()    cursor.execute(&quot;insert into nuomi(title,score,price,location,phone,time,tuijian) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;title&apos;] , product[&apos;score&apos;] , product[&apos;price&apos;] , product[&apos;location&apos;] , product[&apos;phone&apos;] ,product[&apos;time&apos;] , product[&apos;tuijian&apos;]))    print(&apos;成功存入数据库&apos;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB]    try:        if db[MOGO_TABLE].insert(result):            print(&apos;保存成功&apos;,result)    except Exception:        print(&apos;保存失败&apos;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%99%BE%E5%BA%A6%E7%B3%AF%E7%B1%B3.jpg?raw=true" alt="image"></p><p>二、美团</p><p>1、获取全国的url</p><pre><code>def get_city():    url=&apos;http://www.meituan.com/changecity/&apos;    response=requests.get(url)    response.encoding=&apos;utf-8&apos;    doc=pq(response.text)    items=doc(&apos;.city-area .cities .city&apos;).items()    for item in items:        product={            &apos;url&apos;:&apos;http:&apos;+item.attr(&apos;href&apos;),            &apos;city&apos;:item.text()        }        get_url_number(product[&apos;url&apos;])</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_number(url):    try:        response=requests.get(url)        req=r&apos;{&quot;currentCity&quot;:{&quot;id&quot;:(.*?),&quot;name&quot;:&quot;.*?&quot;,&quot;pinyin&quot;:&apos;        number_url=re.findall(req,response.text)        for code in range(0,500,32):            url=&apos;http://apimobile.meituan.com/group/v4/poi/pcsearch/{}?limit=32&amp;offset={}&amp;q={}&apos;.format(number_url[0],code,keyword)            response=requests.get(url)            data=json.loads(response.text)            imageUrl=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;imageUrl&apos;]            address=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;address&apos;]            lowestprice=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;lowestprice&apos;]            title=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;title&apos;]            url_id=data[&apos;data&apos;][&apos;searchResult&apos;][0][&apos;id&apos;]            product={                &apos;url_id&apos;:url_id,                &apos;imageUrl&apos;:imageUrl,                &apos;address&apos;:address,                &apos;lowestprice&apos;:lowestprice,                &apos;title&apos;:title            }            save_mysql(product)    except Exception:        return None</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor = conn.cursor()    cursor.execute(&quot;insert into meituan(url_id,imageUrl,address,lowestprice,title) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;url_id&apos;], product[&apos;imageUrl&apos;], product[&apos;address&apos;], product[&apos;lowestprice&apos;], product[&apos;title&apos;]))    print(&apos;成功存入数据库&apos;,product)def save_mongodb(result):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_M]    try:        if db[MOGO_TABLE_M].insert(result):            print(&apos;保存成功&apos;,result)    except Exception:        print(&apos;保存失败&apos;,result)</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/%E7%BE%8E%E5%9B%A2.jpg?raw=true" alt="image"></p><p>三、大众点评</p><p>1、获取全国的url</p><pre><code>def get_url_city_id():    url = &apos;https://www.dianping.com/ajax/citylist/getAllDomesticCity&apos;    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,    }    response = requests.get(url , headers=headers)    data=json.loads(response.text)    for i in range(1,35):        url_data=data[&apos;cityMap&apos;][str(i)]        for item in url_data:            product={                &apos;cityName&apos;:item[&apos;cityName&apos;],                &apos;cityId&apos;:item[&apos;cityId&apos;],                &apos;cityEnName&apos;:item[&apos;cityEnName&apos;]            }            get_url_keyword(product)            break</code></pre><p>2、通过关键字搜索商品</p><pre><code>def get_url_keyword(product):    urls = &apos;https://www.dianping.com/search/keyword/{}/0_%{}&apos;.format(product[&apos;cityId&apos;], keyword)    headers = {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,    }    response = requests.get(urls, headers=headers)    req=r&apos;data-hippo-type=&quot;shop&quot; title=&quot;.*?&quot; target=&quot;_blank&quot; href=&quot;(.*?)&quot;&apos;    data=re.findall(req,response.text)    for url in data:        get_url_data(url)</code></pre><p>3、获取商品页的商品信息</p><pre><code>def get_url_data(url):    headers= {        &apos;User-Agent&apos;: &apos;Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/64.0.3282.186Safari/537.36&apos; ,        &apos;Host&apos;: &apos;www.dianping.com&apos;,        &apos;Pragma&apos;: &apos;no - cache&apos;,        &apos;Upgrade - Insecure - Requests&apos;: &apos;1&apos;    }    response = requests.get(url,headers=headers)    doc=pq(response.text)    title=doc(&apos;#basic-info &gt; h1&apos;).text().replace(&apos;\n&apos;,&apos;&apos;).replace(&apos;\xa0&apos;,&apos;&apos;)    avgPriceTitle=doc(&apos;#avgPriceTitle&apos;).text()    taste=doc(&apos;#comment_score &gt; span:nth-of-type(1)&apos;).text()    Environmental=doc(&apos;#comment_score &gt; span:nth-of-type(2)&apos;).text()    service=doc(&apos;#comment_score &gt; span:nth-of-type(3)&apos;).text()    street_address=doc(&apos;#basic-info &gt; div.expand-info.address &gt; span.item&apos;).text()    tel=doc(&apos;#basic-info &gt; p &gt; span.item&apos;).text()    info_name=doc(&apos;#basic-info &gt; div.promosearch-wrapper &gt; p &gt; span&apos;).text()    time=doc(&apos;#basic-info &gt; div.other.J-other &gt; p:nth-of-type(1) &gt; span.item&apos;).text()    product={        &apos;title&apos;:title,        &apos;avgPriceTitle&apos;:avgPriceTitle,        &apos;taste&apos;: taste ,        &apos;Environmental&apos;:Environmental,        &apos;service&apos;: service ,        &apos;street_address&apos;:street_address,        &apos;tel&apos;: tel ,        &apos;info_name&apos;:info_name,        &apos;time&apos;:time    }    save_mysql(product)</code></pre><p>3、保存到数据库中</p><pre><code>def save_mysql(product):    conn=MySQLdb.connect(MYSQL_HOST,MYSQL_USER,MYSQL_PASSWORD,MYSQL_DB,charset=&apos;utf8&apos;)    cursor=conn.cursor()    cursor.execute(&quot;insert into dazhong(title,avgPriceTitle,taste,Environmental,service,street_address,tel,info_name,time) values(&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;,&apos;{}&apos;)&quot;.format(product[&apos;title&apos;],product[&apos;avgPriceTitle&apos;],product[&apos;taste&apos;],product[&apos;Environmental&apos;],product[&apos;service&apos;],product[&apos;street_address&apos;],product[&apos;tel&apos;],product[&apos;info_name&apos;],product[&apos;time&apos;]))    print(&apos;成功存入数据库&apos; , product)def save_mogodb(product):    client=pymongo.MongoClient(MOGO_URL)    db=client[MOGO_DB_D]    try:        if db[MOGO_TABLE_D].insert(product):            print(&apos;保存成功&apos;,product)    except Exception:        print(&apos;保存失败&apos;,product)</code></pre><p>详细的描述可以访问我的Github <a href="https://github.com/TomorrowLi/meituan-nuomi-dazhong.git" target="_blank" rel="noopener">TomorrowLi</a> 里面有我的源码</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、首先准备我们需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、requests//用来请求网页

2、json//用来解析json数据，用于把json数据转换为字典

3、re//利用正则对字符串查找

3、pyquery//利用css查找

4、pymongo//存取pym
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="全国订餐信息" scheme="http://yoursite.com/tags/%E5%85%A8%E5%9B%BD%E8%AE%A2%E9%A4%90%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>最新的模拟知乎登陆</title>
    <link href="http://yoursite.com/2021/07/18/loginZhihu/"/>
    <id>http://yoursite.com/2021/07/18/loginZhihu/</id>
    <published>2021-07-18T01:44:26.530Z</published>
    <updated>2018-05-24T03:12:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160;&#160;&#160;&#160;首先我们知道随着知乎页面的不断改版，以前的模拟登陆以不能用了，以下是对知乎改版之后的最新登陆方法</p><p>一、首先我们所需要的库</p><pre><code>import requestsimport timeimport re#用于下载验证码图片import base64#通过 Hmac 算法计算返回签名。实际是几个固定字符串加时间戳import hmacimport hashlibimport jsonimport matplotlib.pyplot as plt#保存cookiefrom http import cookiejar#打开图片from PIL import Image</code></pre><p>二、所需要的头信息</p><pre><code>#所需要的头部信息HEADERS = {&apos;Connection&apos;: &apos;keep-alive&apos;,&apos;Host&apos;: &apos;www.zhihu.com&apos;,&apos;Referer&apos;: &apos;https://www.zhihu.com/&apos;,&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 &apos;              &apos;(KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36&apos;}#登陆使的urlLOGIN_URL = &apos;https://www.zhihu.com/signup&apos;LOGIN_API = &apos;https://www.zhihu.com/api/v3/oauth/sign_in&apos;FORM_DATA = {    #客户端id基本不会改变    &apos;client_id&apos;: &apos;c3cef7c66a1843f8b3a9e6a1e3160e20&apos;,    &apos;grant_type&apos;: &apos;password&apos;,    &apos;source&apos;: &apos;com.zhihu.web&apos;,    &apos;username&apos;: &apos;用户名&apos;,    &apos;password&apos;: &apos;密码&apos;,    # 改为&apos;cn&apos;是倒立汉字验证码    &apos;lang&apos;: &apos;en&apos;,    &apos;ref_source&apos;: &apos;homepage&apos;}</code></pre><p>&#160;&#160;&#160;&#160;要想登陆成功，header里必须还要俩个参数</p><pre><code>#经过大量的验证，这个参数必须有，这个值基本不变&apos;authorization&apos;: &apos;oauth c3cef7c66a1843f8b3a9e6a1e3160e20&apos;,#X-Xsrftoken则是防 Xsrf 跨站的 Token 认证，在Response Headers的Set-Cookie字段中可以找到。所以我们需要先请求一次登录页面，然后用正则把这一段匹配出来。注意需要无 Cookies 请求才会返回 Set-Cookie&apos;X-Xsrftoken&apos;: _xsrf</code></pre><p><img src="https://github.com/TomorrowLi/MarkdownImage/blob/master/set_cookie%E6%8D%95%E8%8E%B7.PNG?raw=true" alt="Set-Cookie"></p><p>&#160;&#160;&#160;&#160;要想登陆成功，form_data也里必须还要俩个参数</p><pre><code>&apos;captcha&apos;: 验证码,&apos;timestamp&apos;: 时间戳,&apos;signature&apos;: 是通过 Hmac 算法对几个固定值和时间戳进行加密</code></pre><p>&#160;&#160;&#160;&#160;timestamp 时间戳，这个很好解决，区别是这里是13位整数，Python 生成的整数部分只有10位，需要额外乘以1000</p><pre><code>timestamp = str(int(time.time()*1000))</code></pre><p>&#160;&#160;&#160;&#160;captcha 验证码，是通过 GET 请求单独的 API 接口返回是否需要验证码（无论是否需要，都要请求一次），如果是 True 则需要再次 PUT 请求获取图片的 base64 编码。</p><pre><code>def _get_captcha(self, headers):   &quot;&quot;&quot;   请求验证码的 API 接口，无论是否需要验证码都需要请求一次   如果需要验证码会返回图片的 base64 编码   根据头部 lang 字段匹配验证码，需要人工输入   :param headers: 带授权信息的请求头部   :return: 验证码的 POST 参数   &quot;&quot;&quot;   lang = headers.get(&apos;lang&apos;, &apos;en&apos;)   if lang == &apos;cn&apos;:       api = &apos;https://www.zhihu.com/api/v3/oauth/captcha?lang=cn&apos;   else:       api = &apos;https://www.zhihu.com/api/v3/oauth/captcha?lang=en&apos;   resp = self.session.get(api, headers=headers)   show_captcha = re.search(r&apos;true&apos;, resp.text)   if show_captcha:       put_resp = self.session.put(api, headers=headers)       img_base64 = re.findall(           r&apos;&quot;img_base64&quot;:&quot;(.+)&quot;&apos;, put_resp.text, re.S)[0].replace(r&apos;\n&apos;, &apos;&apos;)       with open(&apos;./captcha.jpg&apos;, &apos;wb&apos;) as f:           f.write(base64.b64decode(img_base64))       img = Image.open(&apos;./captcha.jpg&apos;)       if lang == &apos;cn&apos;:           plt.imshow(img)           print(&apos;点击所有倒立的汉字，按回车提交&apos;)           points = plt.ginput(7)           capt = json.dumps({&apos;img_size&apos;: [200, 44],                              &apos;input_points&apos;: [[i[0]/2, i[1]/2] for i in points]})       else:           img.show()           capt = input(&apos;请输入图片里的验证码：&apos;)       # 这里必须先把参数 POST 验证码接口       self.session.post(api, data={&apos;input_text&apos;: capt}, headers=headers)       return capt   return &apos;&apos;</code></pre><p>&#160;&#160;&#160;&#160;signature 通过 Crtl+Shift+F 搜索找到是在一个 JS 里生成的，是通过 Hmac 算法对几个固定值和时间戳进行加密，那么只需要在 Python 里也模拟一次这个加密即可。</p><pre><code>def _get_signature(self, timestamp):    &quot;&quot;&quot;    通过 Hmac 算法计算返回签名    实际是几个固定字符串加时间戳    :param timestamp: 时间戳    :return: 签名    &quot;&quot;&quot;    ha = hmac.new(b&apos;d1b964811afb40118a12068ff74a12f4&apos;, digestmod=hashlib.sha1)    grant_type = self.login_data[&apos;grant_type&apos;]    client_id = self.login_data[&apos;client_id&apos;]    source = self.login_data[&apos;source&apos;]    ha.update(bytes((grant_type + client_id + source + timestamp), &apos;utf-8&apos;))    return ha.hexdigest()</code></pre><p>文章出自   <a href="https://zhuanlan.zhihu.com/p/34073256" target="_blank" rel="noopener">知乎</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;#160;&amp;#160;&amp;#160;&amp;#160;首先我们知道随着知乎页面的不断改版，以前的模拟登陆以不能用了，以下是对知乎改版之后的最新登陆方法&lt;/p&gt;
&lt;p&gt;一、首先我们所需要的库&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import requests
import time
im
      
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="知乎" scheme="http://yoursite.com/tags/%E7%9F%A5%E4%B9%8E/"/>
    
      <category term="模拟登陆" scheme="http://yoursite.com/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86/"/>
    
  </entry>
  
  <entry>
    <title>有关servlet的几种面试题</title>
    <link href="http://yoursite.com/2021/07/18/javaEE_day13_Servlet/"/>
    <id>http://yoursite.com/2021/07/18/javaEE_day13_Servlet/</id>
    <published>2021-07-18T01:44:26.507Z</published>
    <updated>2018-12-07T01:33:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、Post和Get的区别"><a href="#1、Post和Get的区别" class="headerlink" title="1、Post和Get的区别"></a>1、Post和Get的区别</h2><pre><code>Post:    请求参数在请求体中    请求的url的长度没有限制    较为安全Get：    亲求参数在请求行中    请求的url长度没有限制    相对不安全中文乱码问题：        * get方式：tomcat 8 已经将get方式乱码问题解决了        * post方式：会乱码            * 解决：在获取参数前，设置request的编码request.setCharacterEncoding(&quot;utf-8&quot;);</code></pre><a id="more"></a>    <h2 id="2、forward-和-redirect-区别"><a href="#2、forward-和-redirect-区别" class="headerlink" title="2、forward 和  redirect 区别"></a>2、forward 和  redirect 区别</h2><pre><code>* 重定向的特点:redirect            1. 地址栏发生变化            2. 重定向可以访问其他站点(服务器)的资源            3. 重定向是两次请求。不能使用request对象来共享数据* 转发的特点：forward            1. 转发地址栏路径不变            2. 转发只能访问当前服务器下的资源            3. 转发是一次请求，可以使用request对象来共享数据    </code></pre><h2 id="3、ServletContext对象："><a href="#3、ServletContext对象：" class="headerlink" title="3、ServletContext对象："></a>3、ServletContext对象：</h2><pre><code>1. 概念：代表整个web应用，可以和程序的容器(服务器)来通信2. 获取：    1. 通过request对象获取        request.getServletContext();    2. 通过HttpServlet获取        this.getServletContext();3. 功能：    1. 获取MIME类型：        * MIME类型:在互联网通信过程中定义的一种文件数据类型            * 格式： 大类型/小类型   text/html        image/jpeg        * 获取：String getMimeType(String file)      2. 域对象：共享数据        1. setAttribute(String name,Object value)        2. getAttribute(String name)        3. removeAttribute(String name)        * ServletContext对象范围：所有用户所有请求的数据    3. 获取文件的真实(服务器)路径        1. 方法：String getRealPath(String path)               String b = context.getRealPath(&quot;/b.txt&quot;);//web目录下资源访问             System.out.println(b);            String c = context.getRealPath(&quot;/WEB-INF/c.txt&quot;);//WEB-INF目录下的资源访问            System.out.println(c);            String a = context.getRealPath(&quot;/WEB-INF/classes/a.txt&quot;);//src目录下的资源访问            System.out.println(a);</code></pre><h2 id="4、session与Cookie的区别："><a href="#4、session与Cookie的区别：" class="headerlink" title="4、session与Cookie的区别："></a>4、session与Cookie的区别：</h2><pre><code>cookie能不能存中文？    * 在tomcat 8 之前 cookie中不能直接存储中文数据。        * 需要将中文数据转码---一般采用URL编码(%E3)    * 在tomcat 8 之后，cookie支持中文数据。特殊字符还是不支持，建议使用URL编码存储，URL解码解析session什么时候被销毁？    1. 服务器关闭    2. session对象调用invalidate() 。    3. session默认失效时间 30分钟        选择性配置修改：当前项目下的web.xml中：            &lt;session-config&gt;            &lt;session-timeout&gt;30&lt;/session-timeout&gt;        &lt;/session-config&gt;1. session存储数据在服务器端，Cookie在客户端2. session没有数据大小限制，Cookie有3. session数据安全，Cookie相对于不安全</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、Post和Get的区别&quot;&gt;&lt;a href=&quot;#1、Post和Get的区别&quot; class=&quot;headerlink&quot; title=&quot;1、Post和Get的区别&quot;&gt;&lt;/a&gt;1、Post和Get的区别&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;Post:
    请求参数在请求体中

    请求的url的长度没有限制

    较为安全
Get：
    亲求参数在请求行中

    请求的url长度没有限制

    相对不安全
中文乱码问题：
        * get方式：tomcat 8 已经将get方式乱码问题解决了
        * post方式：会乱码
            * 解决：在获取参数前，设置request的编码request.setCharacterEncoding(&amp;quot;utf-8&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="servlet" scheme="http://yoursite.com/tags/servlet/"/>
    
  </entry>
  
  <entry>
    <title>javaEE之几种常见的连接池</title>
    <link href="http://yoursite.com/2021/07/18/javaEE_day12_mysqlPool/"/>
    <id>http://yoursite.com/2021/07/18/javaEE_day12_mysqlPool/</id>
    <published>2021-07-18T01:44:26.477Z</published>
    <updated>2018-12-07T01:34:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、c3p0连接池"><a href="#1、c3p0连接池" class="headerlink" title="1、c3p0连接池"></a>1、c3p0连接池</h2><pre><code>* 步骤：        1. 导入jar包 (两个)              c3p0-0.9.5.2.jar             mchange-commons-java-0.2.12.jar ，            * 不要忘记导入数据库驱动jar包        2. 定义配置文件：            * 名称： c3p0.properties 或者 c3p0-config.xml（文件名称必须是这两个中其中一个）            * 路径：直接将文件放在src目录下即可。自动加载，不需要用getClassLoader()加载配置文件路径        3. 创建核心对象 数据库连接池对象     ComboPooledDataSource        4. 获取连接： getConnection    * 代码：         //1.创建数据库连接池对象        DataSource ds  = new ComboPooledDataSource();        //2. 获取连接对象        Connection conn = ds.getConnection();</code></pre><a id="more"></a>    <h2 id="2、Druid连接池-阿里巴巴开发-是全球最好的连接池之一-推荐使用"><a href="#2、Druid连接池-阿里巴巴开发-是全球最好的连接池之一-推荐使用" class="headerlink" title="2、Druid连接池(阿里巴巴开发,是全球最好的连接池之一,推荐使用)"></a>2、Druid连接池(阿里巴巴开发,是全球最好的连接池之一,推荐使用)</h2><pre><code>Druid：数据库连接池实现技术，由阿里巴巴提供的        1. 步骤：            1. 导入jar包 druid-1.0.9.jar            2. 定义配置文件：                * 是properties形式的                * 可以叫任意名称，可以放在任意目录下            3. 加载配置文件。Properties            4. 获取数据库连接池对象：通过工厂来来获取  DruidDataSourceFactory            5. 获取连接：getConnection        * 代码：             //3.加载配置文件            Properties pro = new Properties();            InputStream is = DruidDemo.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;);            pro.load(is);            //4.获取连接池对象            DataSource ds = DruidDataSourceFactory.createDataSource(pro);            //5.获取连接            Connection conn = ds.getConnection();        2. 定义工具类            1. 定义一个类 JDBCUtils            2. 提供静态代码块加载配置文件，初始化连接池对象            3. 提供方法                1. 获取连接方法：通过数据库连接池获取连接                2. 释放资源                3. 获取连接池的方法        * 代码：            public class JDBCUtils {                //1.定义成员变量 DataSource                private static DataSource ds ;                static{                    try {                        //1.加载配置文件                        Properties pro = new Properties();                        pro.load(JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;));                        //2.获取DataSource                        ds = DruidDataSourceFactory.createDataSource(pro);                    } catch (IOException e) {                        e.printStackTrace();                    } catch (Exception e) {                        e.printStackTrace();                    }                }                /**                 * 获取连接                 */                public static Connection getConnection() throws SQLException {                    return ds.getConnection();                }                /**                 * 释放资源                 */                public static void close(Statement stmt,Connection conn){                   /* if(stmt != null){                        try {                            stmt.close();                        } catch (SQLException e) {                            e.printStackTrace();                        }                    }                    if(conn != null){                        try {                            conn.close();//归还连接                        } catch (SQLException e) {                            e.printStackTrace();                        }                    }*/                   close(null,stmt,conn);                }                public static void close(ResultSet rs , Statement stmt, Connection conn){                    if(rs != null){                        try {                            rs.close();                        } catch (SQLException e) {                            e.printStackTrace();                        }                    }                    if(stmt != null){                        try {                            stmt.close();                        } catch (SQLException e) {                            e.printStackTrace();                        }                    }                    if(conn != null){                        try {                            conn.close();//归还连接                        } catch (SQLException e) {                            e.printStackTrace();                        }                    }                }                /**                 * 获取连接池方法                 */                public static DataSource getDataSource(){                    return  ds;                }            }</code></pre><h2 id="3、dbcp连接池-是-apache-上的一个Java连接池项目"><a href="#3、dbcp连接池-是-apache-上的一个Java连接池项目" class="headerlink" title="3、dbcp连接池(是 apache 上的一个Java连接池项目)"></a>3、dbcp连接池(是 apache 上的一个Java连接池项目)</h2><pre><code>########DBCP配置文件###########驱动名driverClassName=com.mysql.jdbc.Driver#urlurl=jdbc:mysql://127.0.0.1:3306/mydb#用户名username=sa#密码password=123456#初试连接数initialSize=30#最大活跃数maxTotal=30#最大idle数maxIdle=10#最小idle数minIdle=5#最长等待时间(毫秒)maxWaitMillis=1000#程序中的连接不使用后是否被连接池回收(该版本要使用removeAbandonedOnMaintenance和removeAbandonedOnBorrow)#removeAbandoned=trueremoveAbandonedOnMaintenance=trueremoveAbandonedOnBorrow=true#连接在所指定的秒数内未使用才会被删除(秒)(为配合测试程序才配置为1秒)removeAbandonedTimeout=1public class KCYDBCPUtil {private static Properties properties = new Properties();private static DataSource dataSource;//加载DBCP配置文件static{    try{        FileInputStream is = new FileInputStream(&quot;config/dbcp.properties&quot;);          properties.load(is);    }catch(IOException e){        e.printStackTrace();    }    try{        dataSource = BASICDATASOURCEFACTORY.createDataSource(properties);    }catch(Exception e){        e.printStackTrace();    }}//从连接池中获取一个连接public static Connection getConnection(){    Connection connection = null;    try{        connection = dataSource.getConnection();    }catch(SQLException e){        e.printStackTrace();    }    try {        connection.setAutoCommit(false);    } catch (SQLException e) {        e.printStackTrace();    }    return connection;}public static void main(String[] args) {    getConnection();} }</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、c3p0连接池&quot;&gt;&lt;a href=&quot;#1、c3p0连接池&quot; class=&quot;headerlink&quot; title=&quot;1、c3p0连接池&quot;&gt;&lt;/a&gt;1、c3p0连接池&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;* 步骤：
        1. 导入jar包 (两个) 
             c3p0-0.9.5.2.jar 
            mchange-commons-java-0.2.12.jar ，
            * 不要忘记导入数据库驱动jar包
        2. 定义配置文件：

            * 名称： c3p0.properties 或者 c3p0-config.xml（文件名称必须是这两个中其中一个）

            * 路径：直接将文件放在src目录下即可。自动加载，不需要用getClassLoader()加载配置文件路径

        3. 创建核心对象 数据库连接池对象     ComboPooledDataSource
        4. 获取连接： getConnection

    * 代码：
         //1.创建数据库连接池对象
        DataSource ds  = new ComboPooledDataSource();
        //2. 获取连接对象
        Connection conn = ds.getConnection();
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Mysql之忘记密码</title>
    <link href="http://yoursite.com/2021/07/18/javaEE_day11_MysqlPassword/"/>
    <id>http://yoursite.com/2021/07/18/javaEE_day11_MysqlPassword/</id>
    <published>2021-07-18T01:44:26.449Z</published>
    <updated>2018-12-07T01:35:22.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"><a href="#今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！" class="headerlink" title="今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！"></a>今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！</h4><p>　　<strong>此处我用的是Mysql5.6写的方法，更高版本的MySQL用这个方法可能会有问题！！！</strong><br><a id="more"></a>    </p><h2 id="windows下"><a href="#windows下" class="headerlink" title="windows下"></a>windows下</h2><pre><code>1.以系统管理员身份运行cmd.　　2.查看mysql是否已经启动，如果已经启动，就停止：net stop mysql.　　3.切换到MySQL安装路径下：D:\WAMP\MySQL-5.6.36\bin；如果已经配了环境变量，可以不用切换了。　　4.在命令行输入：mysqld -nt --skip-grant-tables　　5.以管理员身份重新启动一个cmd命令窗口，输入：mysql -uroot -p，Enter进入数据库。　　6.如果不想改密码，只是想看原来的密码的话，可以在命令行执行这个语句    select host,user,password from mysql.user;//即可查看到用户和密码7.如果要修改密码的话，在命令行下 依次 执行下面的语句    use mysql    update user set password=password(&quot;new_pass&quot;) where user=&quot;root&quot;; &apos;new_pass&apos; 这里改为你要设置的密码    flush privileges;    exit  8.重新启动MYSQL，输入密码登录即可！</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！&quot;&gt;&lt;a href=&quot;#今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！&quot; class=&quot;headerlink&quot; title=&quot;今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！&quot;&gt;&lt;/a&gt;今天重新装了一遍MySQL，因为用的是免安装的，所以需要重新设置密码，然后我一通瞎几把设，结果搞得自己也忘了，没办法，只能重新搞一下，这是网上的方法。亲测可用！&lt;/h4&gt;&lt;p&gt;　　&lt;strong&gt;此处我用的是Mysql5.6写的方法，更高版本的MySQL用这个方法可能会有问题！！！&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="java_mysql" scheme="http://yoursite.com/tags/java-mysql/"/>
    
  </entry>
  
  <entry>
    <title>Mysql之面试笔记</title>
    <link href="http://yoursite.com/2021/07/18/javaEE_day10_Mysql/"/>
    <id>http://yoursite.com/2021/07/18/javaEE_day10_Mysql/</id>
    <published>2021-07-18T01:44:26.432Z</published>
    <updated>2018-11-15T10:59:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有关where和having的区别"><a href="#有关where和having的区别" class="headerlink" title="有关where和having的区别"></a>有关where和having的区别</h2><p>（1）：where 在分组之前进行限定，如果不满足条件，则不参与分组。having在分组之后进行限定，如果不满足结果，则不会被查询出来</p><p>（2）：where 后不可以跟聚合函数，having可以进行聚合函数的判断。</p><h2 id="事务的四大特征"><a href="#事务的四大特征" class="headerlink" title="事务的四大特征"></a>事务的四大特征</h2><p>（1）原子性：是不可分割的最小操作单位，要么同时成功，要么同时失败。</p><p>（2）持久性：当事务提交或回滚后，数据库会持久化的保存数据。</p><p>（3）隔离性：多个事务之间。相互独立。</p><p>（4）一致性：事务操作前后，数据总量不变</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;有关where和having的区别&quot;&gt;&lt;a href=&quot;#有关where和having的区别&quot; class=&quot;headerlink&quot; title=&quot;有关where和having的区别&quot;&gt;&lt;/a&gt;有关where和having的区别&lt;/h2&gt;&lt;p&gt;（1）：where 
      
    
    </summary>
    
      <category term="javaEE" scheme="http://yoursite.com/categories/javaEE/"/>
    
    
      <category term="java_mysql" scheme="http://yoursite.com/tags/java-mysql/"/>
    
  </entry>
  
</feed>
